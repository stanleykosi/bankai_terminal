Directory structure:
└── devs/
    ├── _meta.json
    ├── consumers.mdx
    ├── reputers.mdx
    ├── validators.mdx
    ├── workers.mdx
    ├── consumers/
    │   ├── _meta.json
    │   ├── allora-api-endpoint.mdx
    │   ├── existing-consumers.mdx
    │   ├── rpc-data-access.mdx
    │   ├── walkthrough-use-topic-inference.mdx
    │   └── consumer-contracts/
    │       ├── _meta.json
    │       ├── deploy-consumer.mdx
    │       └── dev-consumers.mdx
    ├── get-started/
    │   ├── _meta.json
    │   ├── basic-usage.mdx
    │   ├── cli.mdx
    │   ├── existing-topics.mdx
    │   ├── managing-gas.mdx
    │   ├── model-forge.mdx
    │   ├── overview.mdx
    │   ├── query-network-data.mdx
    │   └── setup-wallet.mdx
    ├── reference/
    │   ├── _meta.json
    │   ├── allorad.mdx
    │   ├── module-accounts.mdx
    │   └── params/
    │       ├── _meta.json
    │       ├── chain.mdx
    │       ├── consensus.mdx
    │       ├── mint.mdx
    │       └── stake.mdx
    ├── reputers/
    │   ├── _meta.json
    │   ├── coin-prediction-reputer.mdx
    │   ├── query-ema-score.mdx
    │   ├── query-reputer-data.mdx
    │   ├── reputers.mdx
    │   └── set-and-adjust-stake.mdx
    ├── sdk/
    │   ├── _meta.json
    │   ├── allora-sdk-py.mdx
    │   ├── allora-sdk-ts.mdx
    │   └── overview.mdx
    ├── topic-creators/
    │   ├── _meta.json
    │   ├── how-to-create-topic.mdx
    │   ├── query-topic-data.mdx
    │   └── topic-life-cycle.mdx
    ├── validators/
    │   ├── _meta.json
    │   ├── deploy-chain.mdx
    │   ├── nop-requirements.mdx
    │   ├── run-full-node.mdx
    │   ├── software-upgrades.mdx
    │   ├── stake-a-validator.mdx
    │   └── validator-operations.mdx
    └── workers/
        ├── _meta.json
        ├── deploy-forecaster.mdx
        ├── query-ema-score.mdx
        ├── query-worker-data.mdx
        ├── requirements.mdx
        ├── deploy-worker/
        │   ├── _meta.json
        │   ├── allora-mdk.mdx
        │   ├── build-and-deploy-worker-with-alibaba-cloud.mdx
        │   ├── build-and-deploy-worker-with-node-runners.mdx
        │   └── using-docker.mdx
        └── walkthroughs/
            ├── _meta.json
            ├── walkthrough-hugging-face-worker.mdx
            ├── walkthrough-price-prediction-worker.mdx
            └── walkthrough-price-prediction-worker/
                ├── _meta.json
                └── modelpy.mdx


Files Content:

================================================
FILE: pages/devs/_meta.json
================================================
{
  "get-started": "Get Started",
  "topic-creators": "Topic Creators",
  "consumers": "Consumers",
  "sdk": "SDKs",
  "workers": "Workers",
  "reputers": "Reputers",
  "validators": "Validators",
  "reference": "Reference"
}



================================================
FILE: pages/devs/consumers.mdx
================================================
# Consumers
Consumers are entities that utilize the inferences generated by the network. These consumers can take various forms, from individuals or organizations making use of inference data to automated contracts that interact with the blockchain.

## Distinction between Consumers and Consumer Contracts
### Consumers
Consumers are all-encompassing actors that consume inferences on the Allora Network. These could be businesses, developers, data scientists, or any entity interested in the intelligence generated by the network.

### Consumer Contracts
Consumer contracts are blockchain-deployed contracts that consume inferences. These smart contracts automatically interact with the network to retrieve inference data and use it within their logic. For example, a decentralized finance (DeFi) application might use consumer contracts to obtain and act upon real-time price predictions for cryptocurrencies.



================================================
FILE: pages/devs/reputers.mdx
================================================
# Reputers

Reputers ensure the accuracy and reliability of worker inferences and the overall integrity of topics. 

## What do Reputers do?

### Source Ground Truth

Reputers source the ground truth as specified by the [topic metadata](/devs/topic-creators/how-to-create-topic#creating-your-first-topic). 
For example, they might retrieve the actual price of ETH at a specific moment in time. 

This ground truth is essential for evaluating the accuracy of inferences made by workers.

### Calculate Loss

Reputers calculate the loss of worker inferences and forecast-implied inferences relative to the ground truth. 

For instance, if a topic's [loss function](/devs/topic-creators/create-deploy-loss-calculation-function) is an L1-norm, reputers apply this norm to each worker's inference and the actual price of ETH in 10 days. 
They then respond with a [`ValueBundle` of losses](https://github.com/allora-network/allora-chain/blob/1d56c50c8d0f43446d770cf387dbd43bb3613e8c/x/emissions/proto/emissions/v1/reputer.proto#L28), detailing the calculated losses for each inference.

### Secure Topics with Stake

Reputers secure topics with their [stake](/devs/reputers/set-and-adjust-stake). The more a reputer stakes in a topic, the greater their influence on the consensus of losses. 

Additionally, reputers can be [delegated to](/devs/reference/allorad#delegate-stake-to-a-reputer-for-a-topic), increasing their ability to secure the topic further. This delegated stake enhances the extent to which reputers secure the topic 
as opposed to the broader chain security.

### Receive Rewards

Reputers [receive rewards](/home/layers/consensus/reputers) based on how close their reported losses are to the consensus. A stake-weighted average of each reported loss is taken among reputers per topic per epoch. The closer a reputer's values are to this average, the more they are rewarded. 

This system incentivizes reputers to provide accurate and reliable loss calculations, contributing to the network's overall integrity and reliability.


================================================
FILE: pages/devs/validators.mdx
================================================
# Validators

Validators maintain the security and integrity of the Allora appchain.

## What do Validators do?

### Secure Chain with Stake

Validators secure the Allora appchain by staking tokens in a delegated proof of stake system through CometBFT. The more a validator stakes, the greater their influence on the overall security and consensus of the blockchain. 

Similarly, stakeholders can delegate their tokens to validators, further enhancing the security and reliability of the chain.

#### Topic Security vs Chain Security

- *Topic security* is a subset of *chain security*.
- If the underlying state is corrupted, topic security is compromised.
- One can have chain security without topic security if:
    - Validators are generally honest (weighted by stake).
    - Reputers of a specific topic are generally dishonest (weighted by stake).


### Validate Transactions

Validators validate transactions and blocks, ensuring that all transactions are legitimate and conform to the rules of the blockchain

### Participate in Consensus

Validators participate in the consensus mechanism of the appchain, running CometBFT. By participating in the consensus, validators collectively agree on the state of the blockchain.

### Receive Rewards

Validators [receive rewards]() based on the amount of stake they hold or have delegated to them.

## Learn More

Test run a validator of the Allora appchain by following the instructions [here](/devs/validators/run-full-node).

CometBFT can be explored in the following two articles, among many other places:

- [Staking and Delegation in Cosmos](https://medium.com/@notional-ventures/staking-and-delegation-in-cosmos-db660154bcf9)
- [CometBFT: Security and Consensus in Cosmos](https://medium.com/@notional-ventures/cometbft-security-and-consensus-in-cosmos-part-1-a7be84f0bf25)


================================================
FILE: pages/devs/workers.mdx
================================================
import { Callout } from 'nextra/components'

# Workers

## For Data Scientists

As a data scientist, your expertise in AI and ML is invaluable to this network, enabling you to contribute predictive models and insights that power a wide range of applications. The Allora Network, powered by Allora's unique consensus mechanism, crowdsources financial predictions produced by machine learning models. The network incentivizes the contribution of machine intelligence to optimize various financial objectives.




================================================
FILE: pages/devs/consumers/_meta.json
================================================
{
    "allora-api-endpoint": "Allora API Endpoint",
    "rpc-data-access": "RPC Data Access",
    "consumer-contracts": "Consumer Contracts",
    "existing-consumers": "Existing Consumers",
    "walkthrough-use-topic-inference": "Walkthrough: Using a Topic Inference on-chain"
}



================================================
FILE: pages/devs/consumers/allora-api-endpoint.mdx
================================================
import { Callout } from 'nextra/components'

# Allora API: How to Query Data of Existing Topics

The **Allora API** provides an interface to query real-time on-chain data of the latest inferences made by workers. Here's an explanation of how it works using the example endpoint:

## API Authentication

To access the Allora API, you need to authenticate your requests using an API key.

### Obtaining an API Key

You can obtain an API key through the Allora API key management system. Contact the Allora team on [Discord](https://discord.com/invite/allora) for access to API keys.

### Using an API Key

Once you have an API key, you can include it in your API requests using the `x-api-key` header:

```bash
curl -X 'GET' \
  --url 'https://api.allora.network/v2/allora/consumer/<chainId>?allora_topic_id=<topicId>' \
  -H 'accept: application/json' \
  -H 'x-api-key: <apiKey>'
```

Replace `<apiKey>` with your actual API key, `<chainId>` with the chain ID (e.g., `ethereum-11155111` for Sepolia), and `<topicId>` with the topic ID you want to query.

### API Key Security

Your API key is a sensitive credential that should be kept secure. Do not share your API key or commit it to version control systems. Instead, use environment variables or secure credential storage mechanisms to manage your API key.

```javascript
// Example of using an environment variable for API key
const apiKey = process.env.ALLORA_API_KEY;
```

### Rate Limiting

API requests are subject to rate limiting. If you exceed the rate limit, you will receive a 429 Too Many Requests response. To avoid rate limiting issues, consider implementing retry logic with exponential backoff in your applications.

## API Endpoints

**Generic**: `https://allora-api.testnet.allora.network/emissions/{version_number}/latest_network_inferences/{topic_id}`

**Example**: `https://allora-api.testnet.allora.network/emissions/v7/latest_network_inferences/1`

Where:
- "v7" represents the latest network version number
- "1" represents the topic ID

Sample Response:

```json
{
  "network_inferences": {
    "topic_id": "1",
    "reputer_request_nonce": null,
    "reputer": "",
    "extra_data": null,
    "combined_value": "2605.533879185080648394998043723508",
    "inferer_values": [
      {
        "worker": "allo102ksu3kx57w0mrhkg37kvymmk2lgxqcan6u7yn",
        "value": "2611.01109296"
      },
      {
        "worker": "allo10q6hm2yae8slpvvgmxqrcasa30gu5qfysp4wkz",
        "value": "2661.505295679922"
      }
    ],
    "forecaster_values": [
        {
            "worker": "allo1za8r9v0st4ntfyeka23qs5wvd7mvsnzhztupk0",
            "value": "2610.160000000000000000000000000000"
        }
    ],
    "naive_value": "2605.533879185080648394998043723508",
    "one_out_inferer_values": [
      {
        "worker": "allo102ksu3kx57w0mrhkg37kvymmk2lgxqcan6u7yn",
        "value": "2570.859434973857748387096774193548"
      },
      {
        "worker": "allo10q6hm2yae8slpvvgmxqrcasa30gu5qfysp4wkz",
        "value": "2569.230589724828006451612903225806"
      }
    ],
    "one_out_forecaster_values": [],
    "one_in_forecaster_values": [],
    "one_out_inferer_forecaster_values": []
  },
  "inferer_weights": [
    {
      "worker": "allo102ksu3kx57w0mrhkg37kvymmk2lgxqcan6u7yn",
      "weight": "0.0002191899319465528034563075461505151"
    },
    {
      "worker": "allo10q6hm2yae8slpvvgmxqrcasa30gu5qfysp4wkz",
      "weight": "0.0002191899319465528034563075461505151"
    }
  ],
  "forecaster_weights": [
    {
      "worker": "allo1za8r9v0st4ntfyeka23qs5wvd7mvsnzhztupk0",
      "weight": "0.1444137067859501612197657742201029"
    }
  ],
  "forecast_implied_inferences": [
    {
      "worker": "allo1za8r9v0st4ntfyeka23qs5wvd7mvsnzhztupk0",
      "value": "2610.160000000000000000000000000000"
    }
  ],
  "inference_block_height": "1349577",
  "loss_block_height": "0",
  "confidence_interval_raw_percentiles": [
    "2.28",
    "15.87",
    "50",
    "84.13",
    "97.72"
  ],
  "confidence_interval_values": [
    "2492.1675618299669694181830608795809",
    "2543.9249467952655499150756965734158",
    "2611.033130351115229549044053766836",
    "2662.29523395638446190095015123294396",
    "2682.827040221238"
  ]
}
```

<Callout type="warning">
Please be aware that there may be some expected volatility in predictions due to the nascency of the network and the more forgiving testnet configurations currently in place. We are actively working on implementing an outlier protection mechanism, which will be applied at the consumer layer and tailored to individual use cases in the near future.
</Callout>

## Breaking Down the Response

Below is an explanation of important sub-objects displayed in the JSON output:

### `topic_id`
In this case, "1" represents the topic being queried. [Topics](/devs/topic-creators/how-to-create-topic) define the context and rules for a particular inference.

### `naive_value`
The **naive value** omits all forecast-implied inferences from the weighted average by setting their weights to zero. The naive network inference is used to quantify the contribution of the
forecasting task to the network accuracy, which in turn sets the reward distribution between the inference and forecasting tasks.

### `combined_value`
The **combined value** is an optimized inference that represents a collective intelligence approach, taking both naive submissions and forecast data into account.

> If you are looking to just get one value or number from Allora for a data oracle, this would be the one to take.

### `inferer_values`
Workers in the network submit their inferences, each represented by an `allo` address. For example:

```json
{
    "worker": "allo102ksu3kx57w0mrhkg37kvymmk2lgxqcan6u7yn",
    "value": "2611.01109296"
}
```

Each worker submits a value based on their own models. These individual submissions contribute to both the naive and combined values. The combined value gives higher weighting to more reliable workers, based on performance or other criteria.

### `one_out_inferer_values`
These values simulate removing a single worker from the pool to see how the overall inference changes. This is a technique used to evaluate the impact of individual inferences on the combined result. 

### `forecast_implied_inferences`
The [Forecast-Implied Inference](/home/layers/forecast-synthesis/synthesis#forecast-implied-inferences) uses forecasted losses and worker inferences to produce a predicted value where each prediction is weighted based on how accurately the forecasters predicted losses in previous time steps, or epochs.

### `inference_block_height`
The specific chain block that the inference data was generated

### `confidence_interval_raw_percentiles`
Fixed percentiles that are used to generate [confidence intervals](/home/confidence-intervals)

### `confidence_interval_values`
[Confidence intervals](/home/confidence-intervals) show the predicted range of outcomes based on worker inferences.




================================================
FILE: pages/devs/consumers/existing-consumers.mdx
================================================
# Existing Consumer Deployments

> Where deployments on supported chains can be found

## Consumer Contract Deployments

You can find the deployed consumer contracts by looking at the latest saved deployments in the repository:
- [Sepolia](https://sepolia.etherscan.io/address/0x8E45fbef38DaC54e32AfB27AC8cBab30E6818ce6#code)
- [Arbitrum One](https://arbiscan.io/address/0xd75A47C0e5Eb0CeDF57072268F48ba971d2cD7F3#code)

## Deploying to additional chains

If you would like to deploy to an additional chain not listed above, you can learn how to do so [here](./deploy-consumer).

## Existing Allora Appchain Topics

Existing Allora Appchain Topics can be found [here](/devs/get-started/existing-topics).



================================================
FILE: pages/devs/consumers/rpc-data-access.mdx
================================================
import { Callout } from 'nextra/components'

# Accessing Allora Data Through RPC

In addition to the [Allora API](/devs/consumers/allora-api-endpoint), you can also access Allora network data directly through RPC (Remote Procedure Call) endpoints. This provides an alternative method for consuming outputs from the network, especially useful for applications that need to interact directly with the blockchain.

## Prerequisites

- [`allorad` CLI](/devs/get-started/cli) installed
- Access to an Allora RPC node

For a complete list of available RPC endpoints and commands, see the [allorad reference section](/devs/reference/allorad).

## RPC URL and Chain ID

Each network uses a different RPC URL and Chain ID which are needed to specify which network to run commands on when using specific commands on allorad.

### Testnet
- **RPC URLs**:
  - `https://rpc.ankr.com/allora_testnet`
  - `https://allora-rpc.testnet.allora.network/`
- **Chain ID**: `allora-testnet-1`

## RPC Endpoints for Consumers

The following RPC methods are particularly useful for consumers looking to access inference data from the Allora network:

### Get Latest Available Network Inferences

This is the primary method for consumers to retrieve the latest network inference for a specific topic.

```bash
allorad q emissions latest-available-network-inferences [topic_id] --node <RPC_URL>
```

**Parameters:**
- `topic_id`: The identifier of the topic for which you want to retrieve the latest available network inference.
- `RPC_URL`: The URL of the RPC node you're connecting to.

**Example:**
```bash
allorad q emissions latest-available-network-inferences 1 --node https://allora-rpc.testnet.allora.network/
```

**Response:**
The response includes the network inference data, including the combined value, individual worker values, confidence intervals, and more. Here's a simplified example:

```json
{
  "network_inferences": {
    "topic_id": "1",
    "combined_value": "2605.533879185080648394998043723508",
    "inferer_values": [
      {
        "worker": "allo102ksu3kx57w0mrhkg37kvymmk2lgxqcan6u7yn",
        "value": "2611.01109296"
      },
      {
        "worker": "allo10q6hm2yae8slpvvgmxqrcasa30gu5qfysp4wkz",
        "value": "2661.505295679922"
      }
    ],
    "naive_value": "2605.533879185080648394998043723508"
  },
  "confidence_interval_values": [
    "2492.1675618299669694181830608795809",
    "2543.9249467952655499150756965734158",
    "2611.033130351115229549044053766836",
    "2662.29523395638446190095015123294396",
    "2682.827040221238"
  ]
}
```

<Callout type="info">
The `combined_value` field represents the optimized inference that takes both naive submissions and forecast data into account. This is typically the value you want to use for most consumer applications.
</Callout>

## Using RPC in Your Applications

### JavaScript/TypeScript Example

Here's an example of how to query the Allora network using RPC in a JavaScript/TypeScript application:

```typescript
import axios from 'axios';

async function getLatestInference(topicId: number, rpcUrl: string) {
  try {
    const response = await axios.post(rpcUrl, {
      jsonrpc: '2.0',
      id: 1,
      method: 'abci_query',
      params: {
        path: `/allora.emissions.v1.Query/GetLatestAvailableNetworkInferences`,
        data: Buffer.from(JSON.stringify({ topic_id: topicId })).toString('hex'),
        prove: false
      }
    });

    // Parse the response
    const result = response.data.result;
    if (result.response.code !== 0) {
      throw new Error(`Query failed with code ${result.response.code}`);
    }

    // Decode the response value
    const decodedValue = Buffer.from(result.response.value, 'base64').toString();
    const parsedValue = JSON.parse(decodedValue);

    return parsedValue;
  } catch (error) {
    console.error('Error querying Allora RPC:', error);
    throw error;
  }
}

// Example usage
getLatestInference(1, 'https://allora-rpc.testnet.allora.network/')
  .then(data => {
    console.log('Latest inference:', data.network_inferences.combined_value);
    console.log('Confidence intervals:', data.confidence_interval_values);
  })
  .catch(error => {
    console.error('Failed to get inference:', error);
  });
```

### Python Example

Here's an example of how to query the Allora network using RPC in a Python application:

```python
import requests
import json
import base64

def get_latest_inference(topic_id, rpc_url):
    try:
        payload = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "abci_query",
            "params": {
                "path": "/allora.emissions.v1.Query/GetLatestAvailableNetworkInferences",
                "data": bytes(json.dumps({"topic_id": topic_id}), 'utf-8').hex(),
                "prove": False
            }
        }
        
        response = requests.post(rpc_url, json=payload)
        response.raise_for_status()
        
        result = response.json()["result"]
        if result["response"]["code"] != 0:
            raise Exception(f"Query failed with code {result['response']['code']}")
        
        # Decode the response value
        decoded_value = base64.b64decode(result["response"]["value"]).decode('utf-8')
        parsed_value = json.loads(decoded_value)
        
        return parsed_value
    except Exception as e:
        print(f"Error querying Allora RPC: {e}")
        raise

# Example usage
try:
    data = get_latest_inference(1, "https://allora-rpc.testnet.allora.network/")
    print(f"Latest inference: {data['network_inferences']['combined_value']}")
    print(f"Confidence intervals: {data['confidence_interval_values']}")
except Exception as e:
    print(f"Failed to get inference: {e}")
```

## RPC vs API: When to Use Each

### Use RPC When:

- You need direct blockchain access without intermediaries
- You want to query historical data that might not be available through the API
- You're building applications that need to interact with multiple aspects of the Allora network
- You want to avoid potential rate limiting on the API

### Use the API When:

- You need a simpler interface with standardized authentication
- You want to avoid the complexity of RPC calls
- You're primarily interested in the latest inference data
- You need additional features provided by the API that aren't available through RPC

<Callout type="warning">
RPC nodes may have their own rate limiting or access restrictions. Make sure to implement proper error handling and retry logic in your applications.
</Callout>



================================================
FILE: pages/devs/consumers/walkthrough-use-topic-inference.mdx
================================================
# Walkthrough: Using a Topic Inference on-chain

Follow these instructions to bring the most recent inference data on-chain for a given topic. 

## Complete Example:

```solidity
    /**
     * @notice Example for calling a protocol function with topic inference data
     * 
     * @param protocolFunctionArgument An argument for the protocol function
     * @param alloraNetworkInferenceData The signed data from the Allora Consumer
     */
    function callProtocolFunctionWithAlloraTopicInference(
        uint256 protocolFunctionArgument,
        AlloraConsumerNetworkInferenceData calldata alloraNetworkInferenceData
    ) external payable {
        (
            uint256 value,
            uint256[] memory confidenceIntervalPercentiles,
            uint256[] memory confidenceIntervalValues,
        ) = IAlloraConsumer(<Consumer Contract Address>).verifyNetworkInference(alloraNetworkInferenceData);

        _protocolFunctionRequiringPredictionValue(
            protocolFunctionArgument, 
            value,
            confidenceIntervalPercentiles,
            confidenceIntervalValues
        );
    }
```

## Step by Step Guide:

Call the Consumer Inference API using the `asset` and `timeframe` you want to query.

- `asset` is the asset you want to query, e.g. `BTC`, `ETH`
- `timeframe` is the timeframe you want to query, e.g. `5m`, `8h`

```shell

curl -X 'GET' --url 'https://api.allora.network/v2/allora/consumer/price/ethereum-111551111/ETH/5m' -H 'x-api-key: <apiKey>'
```

Here is an example response: 
```json
{
	"request_id": "b52b7c20-57ae-4852-bdbb-8f39cf317974",
	"status": true,
	"data": {
		"signature": "0x99b8b75f875a9ecc09fc499073656407458d464edeceb384686dba990ed785d841e6510b578d253a6e19a20503d1ec1e3c38b4c60980ff3b4df9ce3335ebd3851b",
		"inference_data": {
			"network_inference": "3365485208027959000000",
			"confidence_interval_percentiles": ["2280000000000000000", "15870000000000000000", "50000000000000000000", "84130000000000000000", "97720000000000000000"],
			"confidence_interval_values": ["2280000000000000000", "15870000000000000000", "50000000000000000000", "84130000000000000000", "97720000000000000000"],
			"topic_id": "9",
			"timestamp": "1719866777",
			"extra_data": "0x"
		}
	}
}
```

3. Construct a call to the Allora Consumer contract on the chain of your choice (options listed under [deployments](./existing-consumers)) using the returned `signature` and `network-inference` as follows:

## Creating the Transaction:

Note you be doing something more like `callProtocolFunctionWithAlloraTopicInference` in the example above, so you would want to construct your call to that contract in a similar way to the following. You can find the complete example [here](https://github.com/allora-network/allora-consumer/blob/main/script/verifyDataExampleSimple.ts). 

```typescript
const alloraConsumer = 
  (new AlloraConsumer__factory())
    .attach(ALLORA_CONSUMER_ADDRESS)
    .connect(senderWallet) as AlloraConsumer

const tx = await alloraConsumer.verifyNetworkInference({
  signature: '0x99b8b75f875a9ecc09fc499073656407458d464edeceb384686dba990ed785d841e6510b578d253a6e19a20503d1ec1e3c38b4c60980ff3b4df9ce3335ebd3851b',
    networkInference: {
    topicId: 9,
    timestamp: 1719866777,
    extraData: ethers.toUtf8Bytes(''),
    networkInference: '3365485208027959000000',
    confidenceIntervalPercentiles:['2280000000000000000', '15870000000000000000', '50000000000000000000', '84130000000000000000', '97720000000000000000' ],
    confidenceIntervalValues:[ '3016256807053656000000', '3029849059956295000000', '3049738780726754000000', '3148682039955208400000', '3278333171848616500000' ],
  }, 
  extraData: ethers.toUtf8Bytes(''),
})

console.info('tx hash:', tx.hash) 
console.info('Awaiting tx confirmation...')

const result = await tx.wait()

console.info('tx receipt:', result)
```


## Notes

- The API endpoint uses `snake_case`, while the smart contract uses `camelCase` for attribute names.
- Ethers.js does not accept `''` for `extraData`. Empty `extraData` should be denoted with `'0x'`.

## Code Links

- [Open source consumer code](https://github.com/allora-network/allora-consumer/blob/main/src/)
- [IAlloraConsumer](https://github.com/allora-network/allora-consumer/blob/main/src/interface/IAlloraConsumer.sol), including the structs used for Solidity code.



================================================
FILE: pages/devs/consumers/consumer-contracts/_meta.json
================================================
{
    "dev-consumers": "Getting Started",
    "deploy-consumer": "Deployment"
}


================================================
FILE: pages/devs/consumers/consumer-contracts/deploy-consumer.mdx
================================================
import { Callout } from 'nextra/components'

# Deploying a Consumer to an EVM Target Chain

Consumer contracts are required to bring Allora Network prices to a chain. Follow the instructions below to deploy the Consumer to a new EVM chain. 

## 1. Set up the repo

- Git clone and `cd` into the [Allora Consumer repository](https://github.com/allora-network/allora-consumer).
```bash
git clone https://github.com/allora-network/allora-consumer.git
cd allora-consumer
```

- run `yarn`
{/* 3. run `forge build` If forge is not already installed, follow [these instructions](https://book.getfoundry.sh/getting-started/installation) to install `foundryup` and `forge`. */}

## 2. Set up the deploy script

The deploy script can be found under `deploy/deploy.ts`

Replace the `ADMIN` address with the desired admin for the new consumer. 

## 3. Set up the .env file

Create a `.env` file in the root of the project with the following structure:

```Text .env
deploymentName=<chain name string>
chainId=<chain id unt>
rpcUrl=<rpc url>
privateKey=<private key hex without '0x'>
etherscanApiKey=<etherscan api key string>
```

## 4. Run the deployment

```bash
yarn
yarn deploy
```

<Callout>
You should see output indicating that the `AlloraConsumer` is being deployed, and then verified on etherscan. Deployed addresses will be saved in `/deploy/deployments/<deploymentName>.json`. If a given contract is already deployed with that deployment name, it will be skipped. Delete the deployment record `/deploy/deployments/<deploymentName>.json` to deploy new contracts.
</Callout>


================================================
FILE: pages/devs/consumers/consumer-contracts/dev-consumers.mdx
================================================
# Getting Started with Consumers Contracts

> Sample code snippets to help you get started using inferences from Allora topics

Consumer contracts are essential for bringing Allora Network prices on-chain. You can find the code repository containing example consumer contracts [here](https://github.com/allora-network/allora-consumer). Consumer contracts verify that the data is correctly formatted, and signed by a valid signer.

## Consuming Allora Inferences

Below is a complete example of a contract that brings inference data on-chain for use in a protocol, and verifies the data against an Allora Consumer contract. This example code can be found [here](https://github.com/allora-network/allora-consumer/blob/main/src/examples/AlloraConsumerBringPredictionOnChainExample.sol).

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.13;

import { 
  IAlloraConsumer, 
  TopicValue, 
  AlloraConsumerNetworkInferenceData
} from '../interface/IAlloraConsumer.sol';
import { Ownable2Step } from "../../lib/openzeppelin-contracts/contracts/access/Ownable2Step.sol";
import { EnumerableSet } from "../../lib/openzeppelin-contracts/contracts/utils/structs/EnumerableSet.sol";


/**
 * @title AlloraConsumerBringPredictionOnChainExample
 * @notice Example contract for using the Allora consumer by bringing predictions on-chain
 */
contract AlloraConsumerBringPredictionOnChainExample is Ownable2Step {

    // Sepolia consumer Address
    IAlloraConsumer public alloraConsumer = IAlloraConsumer(0x4341a3F0a350C2428184a727BAb86e16D4ba7018);

    // ***************************************************************
    // * ================== USER INTERFACE ========================= *
    // ***************************************************************

    /**
     * @notice Example for calling a protocol function with using an inference and confidence 
     *   intervals already stored on the Allora Consumer, only if the value is not stale.
     * 
     * @param protocolFunctionArgument An argument for the protocol function
     * @param topicId The id of the topic to use the most recent stored value for
     */
    function callProtocolFunctionWithExistingValue(
        uint256 protocolFunctionArgument,
        uint256 topicId
    ) external payable {
        TopicValue memory topicValue = alloraConsumer.getTopicValue(topicId, '');

        if (topicValue.recentValueTime + 1 hours < block.timestamp) {
            revert('AlloraConsumerBringPredictionOnChainExample: stale value');
        }

        _protocolFunctionRequiringPredictionValue(
            protocolFunctionArgument, 
            topicValue.recentValue,
            topicValue.confidenceIntervalPercentiles,
            topicValue.confidenceIntervalValues
        );
    }

    /**
     * @notice Example for calling a protocol function with an inference value from the Allora Consumer
     * 
     * @param protocolFunctionArgument An argument for the protocol function
     * @param alloraNetworkInferenceData The signed data from the Allora Consumer
     */
    function callProtocolFunctionWithAlloraTopicInference(
        uint256 protocolFunctionArgument,
        AlloraConsumerNetworkInferenceData calldata alloraNetworkInferenceData
    ) external payable {
        (
            uint256 value,
            uint256[] memory confidenceIntervalPercentiles,
            uint256[] memory confidenceIntervalValues,
        ) = alloraConsumer.verifyNetworkInference(alloraNetworkInferenceData);

        _protocolFunctionRequiringPredictionValue(
            protocolFunctionArgument, 
            value,
            confidenceIntervalPercentiles,
            confidenceIntervalValues
        );
    }

    function _protocolFunctionRequiringPredictionValue(
        uint256 protocolFunctionArgument, 
        uint256 value,
        uint256[] memory confidenceIntervalPercentiles,
        uint256[] memory confidenceIntervalValues
    ) internal {
        // use arguments and value 
    }

    // ***************************************************************
    // * ========================= ADMIN =========================== *
    // ***************************************************************

    /**
     * @notice Set the AlloraConsumer contract address
     * 
     * @param alloraConsumer_ The AlloraConsumer contract address
     */
    function setAlloraConsumerContract(IAlloraConsumer alloraConsumer_) external onlyOwner {
        alloraConsumer = alloraConsumer_;
    }
}

```



================================================
FILE: pages/devs/get-started/_meta.json
================================================
{
    "overview": "Overview",
    "setup-wallet": "Setup Wallet",
    "cli": "Installation",
    "basic-usage": "Basic Usage",
    "managing-gas": "Managing Gas",
    "existing-topics": "Existing Topics",
    "query-network-data": "How to Query Network Data using allorad",
    "model-forge": "Model Forge Quickstart"
}



================================================
FILE: pages/devs/get-started/basic-usage.mdx
================================================
# Basic Usage

The Allora Network is a sophisticated ecosystem designed to facilitate various participants, including inference workers, forecasters, reputers, and validators, each playing a crucial role in the network's functionality and integrity. Spinning up these different network participants involves a deep understanding of the network's architecture and protocols.

Despite the complexities involved in the setup and operation of different participants, interacting with the Allora Network on a basic level is straightforward. Here are some ways to get started:

## Querying an Inference On-chain

Interacting with the Allora Network also involves querying data of existing topics on-chain. This can be efficiently done using the Allorad CLI tool. The CLI tool provides a command-line interface to interact with the network, enabling users to retrieve on-chain data seamlessly.

Follow the tutorial [here](/devs/get-started/query-network-data#get-latest-available-network-inferences) to learn how to query an inference on-chain using the `allorad` CLI tool.

## Delegating Stake to a Reputer

Users can delegate their stake to a reputer, contributing to the network's overall health and performance. This involves a basic understanding of staking mechanisms and can be done through the `allorad` CLI tool.

Follow the tutorial [here](/devs/reference/allorad#delegate-stake-to-a-reputer-for-a-topic) to learn how to delegate your stake to a reputer.



================================================
FILE: pages/devs/get-started/cli.mdx
================================================
import { Callout } from 'nextra/components'

# Allora CLI Spec

Allora provides a CLI tools that allows network participants to perform different functions on the Allora Network:

- `allorad` -  Used to read and write data to the chain, e.g. to create a wallet, create new topics or add/delegate stake to a reputer
  - Refer to the [Allorad Reference](/devs/reference/allorad) section for a full list of `allorad` commands with their explanations

## Installing `allorad`

### Prerequisites

You will need to install `go` to download and use `allorad` successfully.

To install Go, follow one of the recommended methods below or consult the [official Go documentation](https://go.dev/doc/install) for the correct download for your operating system. The command-line instructions are based on standard installation locations, but you may customize them as needed.

### Installation

```Text bash
curl -sSL https://raw.githubusercontent.com/allora-network/allora-chain/dev/install.sh | bash -s -- v0.12.1

```

A **successful** installation should output the following line:

```bash
YYYY-MM-DD hh:mm:ss (N MB/s) - ‘/tmp/allorad’ saved [116706514/116706514]
```

### Verifying Installation

After installation, verify that `allorad` is correctly installed and ready to interact with the Allora Network by running:

```
allorad version
```

`allorad` supports general Cosmos SDK and Tendermint commands. You can run the tool to see a list of commands with explanations of what they do:

```text
$ allorad
allorad - the Allora chain

Usage:
  allorad [command]

Available Commands:
  comet       CometBFT subcommands
  completion  Generate the autocompletion script for the specified shell
  config      Utilities for managing application configuration
  debug       Tool for helping with debugging your application
  export      Export state to JSON
  genesis     Application's genesis-related subcommands
  help        Help about any command
  init        Initialize private validator, p2p, genesis, and application configuration files
  keys        Manage your application's keys
  prune       Prune app history states by keeping the recent heights and deleting old heights
  query       Querying subcommands
  rollback    rollback Cosmos SDK and CometBFT state by one height
  snapshots   Manage local snapshots
  start       Run the full node
  status      Query remote node for status
  tx          Transactions subcommands
  version     Print the application binary version information

Flags:
  -h, --help                help for allorad
      --home string         directory for config and data (default "/Users/<USER>/.allorad")
      --log_format string   The logging format (json|plain) (default "plain")
      --log_level string    The logging level (trace|debug|info|warn|error|fatal|panic|disabled or '*:<level>,<key>:<level>') (default "info")
      --log_no_color        Disable colored logs
      --trace               print out full stack trace on errors

Use "allorad [command] --help" for more information about a command.
```



================================================
FILE: pages/devs/get-started/existing-topics.mdx
================================================
import { Callout } from 'nextra/components'

# Existing Allora Appchain Topics

> Some useful topics have already been created

The Allora Appchain already contains the following topics on Testnet. Below, you'll find the topic ID, name, and a brief description for each.


| Topic ID | Metadata                                | Default Arg          |
|----------|-----------------------------------------|----------------------|
| 1        | ETH 10min Prediction                    | ETH                  |
| 2        | ETH 24h Prediction                      | ETH                  |
| 3        | BTC 10min Prediction                    | BTC                  |
| 4        | BTC 24h Prediction                      | BTC                  |
| 5        | SOL 10min Prediction                    | SOL                  |
| 6        | SOL 24h Prediction                      | SOL                  |
| 7        | ETH 20min Prediction                    | ETH                  |
| 8        | BNB 20min Prediction                    | BNB                  |
| 9        | ARB 20min Prediction                    | ARB                  |
| 10       | Memecoin 1h Prediction                  | TOKEN_FROM_API       |
| 11       | US Presidential Election 2024 - Winning Party | R                |
| 13       | ETH 5min Prediction                     | ETH                  |
| 14       | BTC 5min Prediction                     | BTC                  |
| 15       | ETH 5min Volatility Prediction          | ETH                  |
| 16       | BTC 5min Volatility Prediction          | BTC                  |



<Callout type="warning">
**Warning**: Topic ordering is never guaranteed to be consistent between separate chains/deployments.
</Callout>

[Install `allorad`](/devs/get-started/cli#installing-allorad) and [create your first topic](/devs/topic-creators/how-to-create-topic) by following the instructions in the hyperlinks provided.



================================================
FILE: pages/devs/get-started/managing-gas.mdx
================================================
import { Callout } from 'nextra/components'

# Managing Gas with `allorad`

Invoking transactions causes network validators to do computations on your behalf and update the chain's state. These actions are compensated via _gas_. Gas is paid by wallets who send transactions to the Allora chain.

In its [v0.7.0 release](https://github.com/allora-network/allora-chain/releases/tag/v0.7.0), Allora incorporated the [x/feemarket module](https://github.com/skip-mev/feemarket), which means that gas calculations follow an [EIP-1559-like schedule](https://help.coinbase.com/en/coinbase/getting-started/crypto-education/eip-1559) (see: the [original EIP-1559](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-1559.md)). This requires transactions to be structured differently from other cosmos chains.

Prior to `allora-chain` `v0.7.0` and in many other Cosmos chains, transactions are typically structured like so:

```
allorad tx emissions ... --from ACCOUNT_NAME --node RPC --chain-id allora-testnet-1 --keyring-backend test --keyring-dir ~/.allorad/ --gas auto --gas-adjustment 1.2 --fees 2024700uallo ...
```

Since `v0.7.0`, transactions should instead abide by the structure:
```
allorad tx emissions ... --from ACCOUNT_NAME --node RPC --chain-id allora-testnet-1 --keyring-backend test --keyring-dir ~/.allorad/ --gas 130206 --gas-adjustment 1.2 --gas-prices 10uallo ...
```

To emphasize: This^^ is the way transactions should be structured today using `allorad`.

The specific differences are:

__`fees Xuallo` becomes `gas-prices 10uallo`__
- This is a config set in the network validators, so `10uallo` is the universally recommended value

__`gas auto` becomes `gas 130206`__
- This value can change per the use case

Other clients such as [CosmJS](https://github.com/cosmos/cosmjs) and [Ignite](https://docs.ignite.com/clients/go-client) would similarly need to include these flags when building transactions.



================================================
FILE: pages/devs/get-started/model-forge.mdx
================================================
import { Callout } from "nextra/components";

# Model Forge Quickstart

The [Allora Model Forge Competition](https://forge.allora.network) is an open-source hackathon where participants attempt to create the best model for a given [topic](/devs/topic-creators/topic-life-cycle) on the Allora Network.

- Inferences are submitted to the Allora Network.
- Participants are scored based on the accuracy of their model compared to the ground truth, and rewarded accordingly.

## Getting Started

### Creating a Wallet

To get started, you'll need to create a wallet on the Allora Network.

#### Download the `allorad` CLI Tool

Open your terminal and run the following command to install the `allorad` CLI tool.

```bash
curl -sSL https://raw.githubusercontent.com/allora-network/allora-chain/dev/install.sh | bash -s -- v0.12.1
```

#### Create a Wallet

Run the following command to create a wallet.

```bash
allorad keys add <key-name>
```

Save your key name, wallet address, and mnemonic in a secure location. You will need this to submit inferences to the Allora Network.

### Register for the Competition

To register for the competition, go to the [Sign Up](https://vk4z45e3hne.typeform.com/to/ypA2Yl1J?utm_source=landing-page&typeform-source=forge.allora.network) page.

Fill out the form and submit your application. 

<Callout>
Input your wallet address that you created in the previous step in the `Allora Wallet Address` field.
</Callout>

<Callout type="warning">
Although a discord account is not required to register, participants with a discord account will be able to join exclusive channels in the Allora Discord server to get priority access to help from the Allora Labs team and other participants.

Join the Allora Discord server [here](https://discord.gg/allora).
</Callout>

### Successful Registration

Once you've been notified that you've been accepted into the competition:

1. Download the [Keplr Wallet Chrome Extension](https://chromewebstore.google.com/detail/keplr/dmkamcknogkgcdfhhbddcghachkejeap?hl=en&pli=1) from the Chrome Web Store.
2. Connect your wallet to the [Allora Forge](forge.allora.network) site.

![connect-wallet](/forge-connect.png)

3. You should see an `eligible` status in the top left corner of the screen once connected.

![eligible](/eligible-forge.png)

Congratulations! You're now registered for the Allora Model Forge Competition.

## Topics

The Allora Network is categorized into distinct [topics](/devs/topic-creators/how-to-create-topic) that serve as Schelling points for model-makers to submit inferences on.

Topics are assigned an ID and categorized by a specific Target Variable, Epoch, Loss Method, and Metadata. Let's quickly explain each in the context of the Forge Competition:

- **Topic ID**: A unique identifier for the topic that participants will need to submit inferences on.
- **Target Variable**: The asset or asset pair that models are trying to predict for
  - E.g. `ETH`, `USDC/ETH`.
- **Epoch**: A discrete period during which inferences and forecasts are submitted, and rewards are distributed. 
  - Epochs are defined in blocks on the Network
  - Each epoch provides a timeframe for evaluating and scoring the performance of workers and reputers.
  - In the Forge Competition, epochs are abstracted into universal timeframes, e.g. `5min`, `1hr`, `1d`.
- **Loss Function**: Measures how far off your model's predictions are from the actual values
  - For all topics in the Forge Competition, the loss function is [`Mean Squared Error`](https://en.wikipedia.org/wiki/Mean_squared_error).
- **Metadata**: Additional information about the topic, including the financial indicator, prediction period, and target variable.
  - E.g. ETH 12hr Volatility Prediction [Epoch length: 5min]
    - Indicates that the topic is predicting the volatility of the ETH asset 12hr into the future.
    - Epoch length indicates that models are submitting the 12hr predicted volatility of the ETH asset every 5min.


<Callout>
[Additional fields that define a given topic](/devs/topic-creators/how-to-create-topic) can be pulled using the following [`allorad`](/devs/get-started/model-forge#download-the-allorad-cli-tool) command:

```bash
allorad query emissions topic <topic-id> --node <node-url>
```

- `node-rpc` is the RPC URL of the Allora Network node you are querying.

Example Usage:

```bash
allorad query emissions topic 13 --node https://allora-rpc.testnet.allora.network
```
</Callout>

### Competition Topics

1. [5min ETH Price Prediction](https://forge.allora.network/competitions/1)
2. [12hr USDC/ETH Volume Prediction](https://forge.allora.network/competitions/2)
3. [12hr ETH Volatility Prediction](https://forge.allora.network/competitions/3)

| Topic ID | Topic | Target Variable | Epoch | Loss Function | Metadata |
|----------|-------|----------------|--------|---------------|-----------|
| 30 | ETH Price Prediction | `ETH` | `5min` | Mean Squared Error | ETH 5min Price Prediction |
| 29 | USDC/ETH Volume Prediction | `USDC/ETH` | `5min` | Mean Squared Error | USDC/ETH 12hr Volume Prediction |
| 28 | ETH Volatility Prediction | `ETH` | `5min` | Mean Squared Error | ETH 12hr Volatility Prediction |

## Model Creation

If your registration is successful, start building your model and preparing it for inference submission to the network.

We've built out the **Allora Model Development Kit** (MDK) framework to help you get started with model creation. These models are optimized for price prediction, but can be used for other topics as well.

[**Allora MDK Repository**](https://github.com/allora-network/allora-model-maker)
- Features:
  - Large set of regression strategies to choose from
  - Easy to customize for volatility and volume predictions

We offer a comprehensive set of [documentation](/devs/workers/deploy-worker/allora-mdk) for the Allora MDK for you to dive into.

## Leaderboard

The leaderboard is a live-updating list of the top performing models in the competition. Go to the competition page for the specific topic you've entered to view the leaderboard.

1. [5min ETH Price Prediction](https://forge.allora.network/competitions/1)
2. [5min USDC/ETH Volume Prediction](https://forge.allora.network/competitions/2)
3. [5min ETH Volatility Prediction](https://forge.allora.network/competitions/3)

<Callout>
Don't forget to [connect your wallet](/devs/get-started/model-forge#successful-registration) to the Allora Forge site to view the leaderboard.
</Callout>

## Detailed Performance Metrics

We've built out an [Allora Explorer](explorer.allora.network) that displays performance metrics to help you evaluate the success of your model. These metrics are used to score your model in the competition.

Steps:

1. Connect your wallet to the Allora Explorer

![connect-wallet](/explorer-connect.png)
2. Click `Your Topics`

![your-topics](/your-topic.png)
3. Click on a topic to view your performance metrics

![topic-metrics](/topic-stats.png)
- Your `Score` and `ALLO Earned` indicate your overall performance in the topic.



================================================
FILE: pages/devs/get-started/overview.mdx
================================================
import { Callout } from 'nextra/components'

# Getting Started with Allora

Welcome to the Allora developer's guide! This page will help you get up to speed with the Allora repositories that will be used throughout our documentation and show you how to start contributing to our ecosystem. 

## Overview

Allora is a decentralized network that leverages the power of collective participation in tasks such as data inference, forecasting, and verification. [Contribute](/community/contribute) to any of the repositories below to help enhance and grow the Allora ecosystem.

## Allora Chain

The [Allora chain](https://github.com/allora-network/allora-chain) is a Cosmos Hub chain that forms the core of the Allora network. This repository contains the blockchain's codebase and is useful for validators.

You can use the [`allorad` CLI tool](/devs/get-started/cli#installing-allorad) to query the chain and make transactions.

<Callout>
Some subsections in the Table of Contents below require downloading and installing [`allorad`](/devs/get-started/cli#installing-allorad).

As a general rule, it is suggested to download and install `allorad` for any developer looking to interact with the network.
</Callout>

## [Setup Wallet](/devs/get-started/setup-wallet.mdx)
Instructions for setting up your wallet to interact with the Allora network.

### Subsections
#### [Basic Usage](/devs/get-started/basic-usage.mdx)
#### [Existing Topics](/devs/get-started/existing-topics.mdx)
#### [How to Query Network Data using `allorad`](/devs/get-started/query-network-data.mdx)
  
---

## [Workers](/devs/workers/workers.mdx)
Explore how workers function in Allora, including setup and data querying.

### Subsections
#### [System Requirements](/devs/workers/nop-requirements.mdx)
#### [Build/Deploy an Inference Worker](/devs/workers/deploy-worker.mdx)
#### [Walkthroughs](/devs/workers/walkthroughs/index.mdx)
#### [Build and Deploy a Forecaster](/devs/workers/deploy-forecaster.mdx)
#### [How To Query Worker Data using `allorad`](/devs/workers/query-worker-data.mdx)
#### [Query EMA Score of a Worker using `allorad`](/devs/workers/query-ema-score.mdx)

---

## [Reputers](/devs/reputers/reputers.mdx)
Understand how reputers contribute to reputation management and query operations.

### Subsections
#### [Coin Prediction Reputer](/devs/reputers/coin-prediction-reputer.mdx)
#### [Query EMA Score for a Reputer using `allorad`](/devs/reputers/query-ema-score.mdx)
#### [Query Reputer Data using `allorad`](/devs/reputers/query-reputer-data.mdx)
#### [Set and Adjust Stake](/devs/reputers/set-and-adjust-stake.mdx)

---

## [Validators](/devs/validators/validators.mdx)
Details on how to set up and run a validator in the Allora network.

### Subsections
#### [System Requirements](/devs/validators/nop-requirements.mdx)
#### [Deploy Allora Chain](/devs/validators/deploy-chain.mdx)
#### [Run a Full Node](/devs/validators/run-full-node.mdx)
#### [Stake a Validator](/devs/validators/stake-a-validator.mdx)
#### [Validator Operations](/devs/validators/validator-operations.mdx)
#### [Software Upgrades](/devs/validators/software-upgrades.mdx)

---

## [Consumers](/devs/consumers/consumers.mdx)
Resources for interacting with Allora as a consumer, including querying data and contracts.

### Subsections
#### [Allora API Endpoint](/devs/consumers/allora-api-endpoint.mdx)
#### [Consumer Contracts](/devs/consumers/consumer-contracts/deploy-consumer.mdx)
##### [Deploy Consumer Contracts](/devs/consumers/consumer-contracts/dev-consumers.mdx)
#### [Existing Consumers](/devs/consumers/existing-consumers.mdx)
#### [Walkthrough: Using a Topic Inference on-chain](/devs/consumers/walkthrough-use-topic-inference.mdx)


================================================
FILE: pages/devs/get-started/query-network-data.mdx
================================================
# How to Query Network Data using `allorad`


To query network-level data on the Allora chain using the `allorad` CLI, you need to interact with various RPC methods designed to return aggregate or holistic information about the
network. These methods enable you to pull data that is crucial for understanding the overall state and performance of the network.

## Prerequisites

- [`allorad` CLI](/devs/get-started/cli)

## Query Functions

These functions read from the appchain only and do not write. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad q emissions [Command] --node <RPC_URL>
```

### Get Latest Available Network Inferences

- **RPC Method:** `GetLatestAvailableNetworkInferences`
- **Command:** `latest-available-network-inferences [topic_id]`
- **Description:** Returns the latest network inference for a given topic, but only if all necessary information to compute the inference is present. The result is only provided when complete data from the network is available to ensure accuracy.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic for which you want to retrieve the latest available network inference.

#### Use Case:
**Why use it?**  
- This command is useful when you need to retrieve the most recent network-wide inference for a topic, ensuring that all necessary data has been collected and processed. It is ideal for situations where decision-making relies on the completeness of the data and where partial data may lead to inaccurate conclusions.

**Example Scenario:**  
- If you want to make a decision based on network predictions, but only when the inference is fully computed, use this command. For example, you might want the latest ETH price prediction, but only when all worker and forecaster data is available to provide an accurate result.

---

### Get Total Rewards to Distribute

- **RPC Method:** `GetTotalRewardToDistribute`
- **Command:** `total-rewards`
- **Description:** Returns the total amount of rewards that will be distributed across all rewardable topics in the current block. It provides an aggregate view of the rewards available for distribution.

#### Use Case:
**Why use it?**  
- This command is useful if you want to understand the total reward pool for a given block. It helps participants gauge the potential rewards available and how they may be distributed across topics based on performance.

**Example Scenario:**  
- As a worker or forecaster, you might use this command to estimate the reward pool for the current block. This allows you to understand the potential total rewards before they are distributed across different topics and participants.

---

### Get Current Module Parameters

- **RPC Method:** `GetParams`
- **Command:** `params`
- **Description:** Retrieves the current parameters of the module in the Allora network. It is used to check the configuration and settings that control various aspects of the module's behavior.
- **Positional Arguments:**
    - This command does not require any positional arguments.

#### Use Case:
**Why use it?**  
- This command is useful for querying the configuration settings of the module. It provides transparency into how the module is configured and allows participants to verify whether certain parameters, such as reward distribution rules or other operational settings, are up-to-date.

**Example Scenario:**  
- If you're troubleshooting the behavior of a module or need to verify the configuration before making any changes, this command can give you insight into the current parameters and their values.




================================================
FILE: pages/devs/get-started/setup-wallet.mdx
================================================
import { Callout } from 'nextra/components'

# Setup Wallet

## Create Wallet

Follow the instructions [here](/devs/get-started/cli) to install our CLI tool `allorad`, which is needed to create a wallet.

Prior to executing transactions, a wallet must be created by running:

```shell
allorad keys add testkey
```

Learn more about setting up keys [here](https://docs.cosmos.network/main/user/run-node/keyring).

Make sure you save your mnemomic and account information safely.

<Callout type="info">
Creating a wallet using `allorad` will generate a wallet address for all currently deployed versions of the Allora Chain (e.g. testnet, local, mainnet). 
</Callout>

## Wallet Recovery

To recover a given wallet's keys, run the following command:

```bash
allorad keys add <wallet name> --recover
```

## Add Faucet Funds
Each network has a different URL to access and request funds from. Please see the faucet URLs for the different networks below:

- **Testnet**: https://faucet.testnet.allora.network/

Enter the Allora Wallet address for the account that needs funding. If you don't have a wallet created yet, follow the instructions above to create one.

## Explorer

- **Testnet**: https://explorer.testnet.allora.network/

<Callout>
Check to see that your wallet has been funded after requesting funds from the faucet by clicking the search bar on the top right corner of the explorer UI and entering your account address.
</Callout>

## RPC URL and Chain ID
Each network uses a different RPC URL and Chain ID which are needed to specify which network to run commands on when using specific commands on `allorad`. See a list of all RPC URLs and their respective Chain IDs supported today:

- **Testnet**
    - `RPC_URL`:  
        - https://rpc.ankr.com/allora_testnet
        - https://allora-rpc.testnet.allora.network/
    - `CHAIN_ID`: `allora-testnet-1`


================================================
FILE: pages/devs/reference/_meta.json
================================================
{
    "allorad": "allorad",
    "module-accounts": "Module Accounts",
    "params": "Parameters"
}


================================================
FILE: pages/devs/reference/allorad.mdx
================================================
# `allorad` Reference

`allorad` commands below are broken out into: 
1. [Query functions](#query-functions), or functions that read from the chain
    - e.g. get active topics, get amount of stake in a topic
2. [Tx functions](#tx-functions), or functions that write to the chain
    - e.g. create a topic, add stake to a reputer

## Query Functions

These functions read from the appchain only and do not write. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad q emissions [Command] --node <RPC_URL>
```

### Params

- **RPC Method:** `Params`
- **Command:** `params`
- **Description:** Get the current module parameters.

### Get Next Topic ID

- **RPC Method:** `GetNextTopicId`
- **Command:** `next-topic-id`
- **Description:** Get next topic id. Topic ids are incremented with each newly added topic.

### Get Topic

- **RPC Method:** `GetTopic`
- **Command:** `topic [topic_id]`
- **Description:** Get topic by topic_id.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Topic Exists

- **RPC Method:** `TopicExists`
- **Command:** `topic-exists [topic_id]`
- **Description:** True if topic exists at given id, else false.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Active Topics

- **RPC Method:** `GetActiveTopics`
- **Command:** `active-topics [pagination]`
- **Description:** Get topic by topic_id.
- **Positional Arguments:**
  - `pagination` The json key-value pair of the limit of topics outputted
    - **Example:** `'{"limit":10}'`

### Is Topic Active

- **RPC Method:** `IsTopicActive`
- **Command:** `is-topic-active [topic_id]`
- **Description:** True if the topic is active, else false.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Rewardable Topics

- **RPC Method:** `GetRewardableTopics`
- **Command:** `rewardable-topics`
- **Description:** Get Rewardable Topics.

### Get Delegate Reward Per Share

- **RPC Method:** `GetDelegateRewardPerShare`
- **Command:** `delegate-reward-per-share [topic_id] [reputer_address]`
- **Description:** Get total delegate reward per share stake in a reputer for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `reputer_address` Address of the reputer.

### Get Delegate Stake Placement

- **RPC Method:** `GetDelegateStakePlacement`
- **Command:** `delegate-reward-per-share [topic_id] [delegator] [target]`
- **Description:** Get the amount of token delegated to a target by a delegator in a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `delegator` Address of the delegator.
  - `target` Address of the target.

### Get Delegate Stake Removal

- **RPC Method:** `GetDelegateStakeRemoval`
- **Command:** `delegate-stake-removal [block_height] [topic_id] [delegator] [reputer]`
- **Description:** Get the current state of a pending delegate stake removal.
- **Positional Arguments:**
  - `block_height` Block height to query.
  - `topic_id` Identifier of the topic whose information will be returned.
  - `delegator` Address of the delegator.
  - `reputer` Address of the reputer.

### Get Delegate Stake Upon Reputer

- **RPC Method:** `GetDelegateStakeUponReputer`
- **Command:** `delegate-stake-on-reputer [topic_id] [target]`
- **Description:** Get the total amount of token delegated to a target reputer in a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `target` Address of the target reputer.

### Get Forecast Scores Until Block

- **RPC Method:** `GetForecastScoresUntilBlock`
- **Command:** `forecast-scores-until-block [topic_id] [block_height]`
- **Description:** Get all saved scores for all forecasters for a topic descending until a given past block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Forecaster Network Regret

- **RPC Method:** `GetForecasterNetworkRegret`
- **Command:** `forecaster-regret [topic_id] [worker]`
- **Description:** Get current network regret for a given forecaster.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `worker` Address of the forecaster.

### Get Inference Scores Until Block

- **RPC Method:** `GetInferenceScoresUntilBlock`
- **Command:** `inference-scores-until-block [topic_id] [block_height]`
- **Description:** Get all saved scores for all inferers for a topic descending until a given past block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Inferer Network Regret

- **RPC Method:** `GetInfererNetworkRegret`
- **Command:** `inferer-regret [topic_id] [actor_id]`
- **Description:** Get current network regret for a given inferer.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `actor_id` Address of the inferer.

### Is Reputer Nonce Unfulfilled

- **RPC Method:** `IsReputerNonceUnfulfilled`
- **Command:** `reputer-nonce-unfulfilled [topic_id] [block_height]`
- **Description:** True if reputer nonce is unfulfilled, else false.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Is Worker Nonce Unfulfilled

- **RPC Method:** `IsWorkerNonceUnfulfilled`
- **Command:** `worker-nonce-unfulfilled [topic_id] [block_height]`
- **Description:** True if worker nonce is unfulfilled, else false.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Latest Available Network Inference

- **RPC Method:** `GetLatestAvailableNetworkInference`
- **Command:** `latest-available-network-inference [topic_id]`
- **Description:** Returns network inference only if all available information to compute the inference is present.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Latest Forecaster Score

- **RPC Method:** `GetLatestForecasterScore`
- **Command:** `latest-forecaster-score [topic_id] [forecaster]`
- **Description:** Returns the latest score for a forecaster in a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `forecaster` Address of the forecaster.

### Get Latest Inferer Score

- **RPC Method:** `GetLatestInfererScore`
- **Command:** `latest-inferer-score [topic_id] [inferer]`
- **Description:** Returns the latest score for an inferer in a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `inferer` Address of the inferer.

### Get Latest Reputer Score

- **RPC Method:** `GetLatestReputerScore`
- **Command:** `latest-reputer-score [topic_id] [reputer]`
- **Description:** Returns the latest score for a reputer in a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `reputer` Address of the reputer.

### Get Latest Topic Inferences

- **RPC Method:** `GetLatestTopicInferences`
- **Command:** `latest-topic-raw-inferences [topic_id]`
- **Description:** Returns the latest round of raw inferences from workers for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Listening Coefficient

- **RPC Method:** `GetListeningCoefficient`
- **Command:** `listening-coefficient [topic_id] [reputer]`
- **Description:** Returns the current listening coefficient for a given reputer.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `reputer` Address of the reputer.

### Get One In Forecaster Network Regret

- **RPC Method:** `GetOneInForecasterNetworkRegret`
- **Command:** `one-in-forecaster-regret [topic_id] [forecaster] [inferer]`
- **Description:** Returns regret born from including a forecaster's implied inference in a batch with an inferer.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `forecaster` Address of the forecaster.
  - `inferer` Address of the inferer.

### Get Naive Inferer Network Regret

- **RPC Method:** `GetNaiveInfererNetworkRegret`
- **Command:** `naive-inferer-network-regret [topic_id] [inferer]`
- **Description:** Returns regret born from including an inferer's naive inference in a batch.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `inferer` Address of the inferer.

### Get One Out Inferer Inferer Network Regret

- **RPC Method:** `GetOneOutInfererInfererNetworkRegret`
- **Command:** `one-out-inferer-inferer-network-regret [topic_id] [one_out_inferer] [inferer]`
- **Description:** Returns regret born from including one inferer's implied inference in a batch with another inferer.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `one_out_inferer` Address of the inferer being compared.
  - `inferer` Address of the primary inferer.

### Get One Out Inferer Forecaster Network Regret

- **RPC Method:** `GetOneOutInfererForecasterNetworkRegret`
- **Command:** `one-out-inferer-forecaster-network-regret [topic_id] [one_out_inferer] [forecaster]`
- **Description:** Returns regret born from including one inferer's implied inference in a batch with a forecaster.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `one_out_inferer` Address of the inferer.
  - `forecaster` Address of the forecaster.

### Get One Out Forecaster Inferer Network Regret

- **RPC Method:** `GetOneOutForecasterInfererNetworkRegret`
- **Command:** `one-out-forecaster-inferer-network-regret [topic_id] [one_out_forecaster] [inferer]`
- **Description:** Returns regret born from including one forecaster's implied inference in a batch with an inferer.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `one_out_forecaster` Address of the forecaster.
  - `inferer` Address of the inferer.

### Get One Out Forecaster Forecaster Network Regret

- **RPC Method:** `GetOneOutForecasterForecasterNetworkRegret`
- **Command:** `one-out-forecaster-forecaster-network-regret [topic_id] [one_out_forecaster] [forecaster]`
- **Description:** Returns regret born from including one forecaster's implied inference in a batch with another forecaster.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `one_out_forecaster` Address of the forecaster being compared.
  - `forecaster` Address of the primary forecaster.

### Get Previous Forecast Reward Fraction

- **RPC Method:** `GetPreviousForecastRewardFraction`
- **Command:** `previous-forecaster-reward-fraction [topic_id] [worker]`
- **Description:** Return previous reward fraction for a worker.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `worker` Address of the worker.

### Get Previous Inference Reward Fraction

- **RPC Method:** `GetPreviousInferenceRewardFraction`
- **Command:** `previous-inference-reward-fraction [topic_id] [worker]`
- **Description:** Return previous reward fraction for a worker.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `worker` Address of the worker.

### Get Previous Percentage Reward To Staked Reputers

- **RPC Method:** `GetPreviousPercentageRewardToStakedReputers`
- **Command:** `previous-percentage-reputer-reward`
- **Description:** Return the previous percentage reward paid to staked reputers.

### Get Previous Reputer Reward Fraction

- **RPC Method:** `GetPreviousReputerRewardFraction`
- **Command:** `previous-reputer-reward-fraction [topic_id] [reputer]`
- **Description:** Return the previous reward fraction for a reputer.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `reputer` Address of the reputer.

### Get Previous Topic Weight

- **RPC Method:** `GetPreviousTopicWeight`
- **Command:** `previous-topic-weight [topic_id]`
- **Description:** Return the previous topic weight.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Reputer Loss Bundles At Block

- **RPC Method:** `GetReputerLossBundlesAtBlock`
- **Command:** `reputer-loss-bundle [topic_id] [block_height]`
- **Description:** Return the reputer loss bundle at a block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Reputers Scores At Block

- **RPC Method:** `GetReputersScoresAtBlock`
- **Command:** `reputer-scores [topic_id] [block_height]`
- **Description:** Return reputer scores at a block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Stake Removal For Reputer And Topic Id

- **RPC Method:** `GetStakeRemovalForReputerAndTopicId`
- **Command:** `reputer-scores [reputer] [topic_id]`
- **Description:** Return stake removal information for a reputer in a topic.
- **Positional Arguments:**
  - `reputer` Address of the reputer.
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Stake Reputer Authority

- **RPC Method:** `GetStakeReputerAuthority`
- **Command:** `reputer-authority [topic_id] [reputer]`
- **Description:** Return total stake on reputer in a topic, including delegate stake and their own.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `reputer` Address of the reputer.

### Get Topic Fee Revenue

- **RPC Method:** `GetTopicFeeRevenue`
- **Command:** `topic-fee-revenue [topic_id]`
- **Description:** Return effective fee revenue for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Topic Reward Nonce

- **RPC Method:** `GetTopicRewardNonce`
- **Command:** `topic-reward-nonce [topic_id]`
- **Description:** Return the reward nonce for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Topic Stake

- **RPC Method:** `GetTopicStake`
- **Command:** `topic-stake [topic_id]`
- **Description:** Return total stake in a topic, including delegate stake.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Total Reward To Distribute

- **RPC Method:** `GetTotalRewardToDistribute`
- **Command:** `total-rewards`
- **Description:** Return total rewards to be distributed among all rewardable topics.

### Get Unfulfilled Reputer Nonces

- **RPC Method:** `GetUnfulfilledReputerNonces`
- **Command:** `unfulfilled-reputer-nonces [topic_id]`
- **Description:** Return topic reputer nonces that have yet to be fulfilled.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Unfulfilled Worker Nonces

- **RPC Method:** `GetUnfulfilledWorkerNonces`
- **Command:** `unfulfilled-worker-nonces [topic_id]`
- **Description:** Return topic worker nonces that have yet to be fulfilled.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Worker Forecast Scores At Block

- **RPC Method:** `GetWorkerForecastScoresAtBlock`
- **Command:** `forecast-scores [topic_id] [block_height]`
- **Description:** Return scores for a worker at a block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Worker Inference Scores At Block

- **RPC Method:** `GetWorkerInferenceScoresAtBlock`
- **Command:** `inference-scores [topic_id] [block_height]`
- **Description:** Return scores for a worker at a block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Get Stake From Reputer In Topic In Self

- **RPC Method:** `GetStakeFromReputerInTopicInSelf`
- **Command:** `stake-reputer-in-topic-self [reputer_address] [topic_id]`
- **Description:** Get the stake of a reputer in a topic that they put on themselves.
- **Positional Arguments:**
  - `reputer_address` Address of the reputer.
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Stake Removals Up Until Block

- **RPC Method:** `GetStakeRemovalsUpUntilBlock`
- **Command:** `stake-removals-up-until-block [block_height]`
- **Description:** Get all pending stake removal requests going to happen at a given block height.
- **Positional Arguments:**
  - `block_height` Block height to query.

### Get Delegate Stake Removals Up Until Block

- **RPC Method:** `GetDelegateStakeRemovalsUpUntilBlock`
- **Command:** `delegate-stake-removals-up-until-block [block_height]`
- **Description:** Get all pending delegate stake removal requests going to happen at a given block height.
- **Positional Arguments:**
  - `block_height` Block height to query.

### Get Stake Removal Info

- **RPC Method:** `GetStakeRemovalInfo`
- **Command:** `stake-removal-info [address] [topic_id]`
- **Description:** Get a pending stake removal for a reputer in a topic.
- **Positional Arguments:**
  - `address` Address of the reputer.
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Delegate Stake Removal Info

- **RPC Method:** `GetDelegateStakeRemovalInfo`
- **Command:** `delegate-stake-removal-info [delegator] [reputer] [topic_id]`
- **Description:** Get a pending delegate stake removal for a delegator in a topic.
- **Positional Arguments:**
  - `delegator` Address of the delegator.
  - `reputer` Address of the reputer.
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Topic Last Worker Commit Info

- **RPC Method:** `GetTopicLastWorkerCommitInfo`
- **Command:** `topic-last-worker-commit [topic_id]`
- **Description:** Get the last commit by a worker for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Topic Last Reputer Commit Info

- **RPC Method:** `GetTopicLastReputerCommitInfo`
- **Command:** `topic-last-reputer-commit [topic_id]`
- **Description:** Get the last commit by a reputer for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.

### Get Forecasts for a Topic at Block Height
- **RPC Method:** `GetForecastsAtBlock`
- **Command:** `forecasts-at-block [topic_id] [block_height]`
- **Description:** Get the Forecasts for a topic at block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `block_height` Number of blocks that precede the specific block you are trying to query

### Execute GetMultiReputerStakeInTopic RPC Method
- **RPC Method:** `GetMultiReputerStakeInTopic`
- **Command:** `get-multi-reputer-stake-in-topic`
- **Description:** Execute the GetMultiReputerStakeInTopic RPC method.

### Get All Inferences Produced for a Topic in a Particular Timestamp
- **RPC Method:** `GetInferencesAtBlock`
- **Command:** `inferences-at-block [topic_id] [block_height]`
- **Description:** Get All Inferences produced for a topic in a particular timestamp.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `block_height` Number of blocks that precede the specific block you are trying to query

### Check if Reputer is Registered in the Topic
- **RPC Method:** `IsReputerRegistered`
- **Command:** `is-reputer-registered [topic_id] [address]`
- **Description:** True if reputer is registered in the topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `address` Reputer Address


### Check if an Address is a Whitelist Admin
- **RPC Method:** `IsWhitelistAdmin`
- **Command:** `is-whitelist-admin [address]`
- **Description:** Check if an address is a whitelist admin. True if so, else false.
- **Positional Arguments:**
  - `address` Address to check

### Check if Worker is Registered in the Topic
- **RPC Method:** `IsWorkerRegistered`
- **Command:** `is-worker-registered [topic_id] [address]`
- **Description:** True if worker is registered in the topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `address` Address to check

### Get the Latest Network Inferences and Weights for a Topic
- **RPC Method:** `GetLatestNetworkInference`
- **Command:** `latest-network-inference [topic_id]`
- **Description:** Get the latest Network inferences and weights for a topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned

### Get the Network Inferences for a Topic at a Block Height
- **RPC Method:** `GetNetworkInferencesAtBlock`
- **Command:** `network-inferences-at-block [topic_id] [block_height_last_inference] [block_height_last_reward]`
- **Description:** Get the Network Inferences for a topic at a block height where the last inference was made and the last reward was given.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `block_height_last_inference` Block height where the last inference was made
  - `block_height_last_reward` Block height where the last reward was given

### Get the Network Loss Bundle for a Topic at Given Block Height
- **RPC Method:** `GetNetworkLossBundleAtBlock`
- **Command:** `network-loss-bundle-at-block [topic_id] [block]`
- **Description:** Get the network loss bundle for a topic at given block height.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `block` Block to query on

### Get Amount of Stake in a Topic for a Delegator
- **RPC Method:** `GetStakeDelegatorInTopic`
- **Command:** `stake-delegator-in-topic [delegator_address] [topic_id]`
- **Description:** Get amount of stake in a topic for a delegator.
- **Positional Arguments:**
  - `delegator_address` Address of the delegator
  - `topic_id` Identifier of the topic whose information will be returned

### Get Amount of Stake from Delegator in a Topic for a Reputer
- **RPC Method:** `GetStakeDelegatorInTopicReputer`
- **Command:** `stake-delegator-in-topic-reputer [delegator_address] [reputer_address] [topic_id]`
- **Description:** Get amount of stake from delegator in a topic for a reputer.
- **Positional Arguments:**
  - `delegator_address` Address of the delegator
  - `reputer_address` Address of the reputer
  - `topic_id` Identifier of the topic whose information will be returned

### Get Reputer Stake in a Topic
- **RPC Method:** `GetStakeInTopicReputer`
- **Command:** `stake-in-topic-reputer [address] [topic_id]`
- **Description:** Get reputer stake in a topic, including stake delegated to them in that topic.
- **Positional Arguments:**
  - `address` Address of the reputer
  - `topic_id` Identifier of the topic whose information will be returned

### Get Total Delegate Stake in a Topic and Reputer
- **RPC Method:** `GetTotalDelegatedStakeInTopicReputer`
- **Command:** `stake-total-delegated-in-topic-reputer [reputer_address] [topic_id]`
- **Description:** Get total delegate stake in a topic and reputer.
- **Positional Arguments:**
  - `reputer_address` Address of the reputer
  - `topic_id` Identifier of the topic whose information will be returned

### Get the Total Amount of Staked Tokens by All Participants in the Network
- **RPC Method:** `GetTotalStake`
- **Command:** `total-stake`
- **Description:** Get the total amount of staked tokens by all participants in the network.

### Get the Latest Inference for a Given Worker and Topic
- **RPC Method:** `GetWorkerLatestInference`
- **Command:** `worker-latest-inference [topic_id] [worker_address]`
- **Description:** Get the latest inference for a given worker and topic.
- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `worker_address` Given worker to query on

## Tx Functions

These functions write to the appchain. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad tx emissions [Command]
```

### Create New Topic
- **RPC Method:** `CreateNewTopic`
- **Command:** `create-topic [creator] [metadata] [loss_logic] [loss_method] [inference_logic] [inference_method] [epoch_length] [ground_truth_lag] [default_arg] [p_norm] [alpha_regret] [allow_negative] [tolerance]`
- **Description:** Add a new topic to the network.
- **Positional Arguments:**
  - `creator` The creator is the owner of the topic that is able to update the topic in the future
  - `metadata`
  - `loss_logic`
  - `loss_method`
  - `inference_logic`
  - `inference_method`
  - `epoch_length`
  - `ground_truth_lag`
  - `default_arg`
  - `p_norm`
  - `alpha_regret`
  - `allow_negative`
  - `tolerance`

Detailed instructions on [how to create a topic](/devs/topic-creators/how-to-create-topic) are linked.

### Add an Admin Address to the Whitelist
- **RPC Method:** `AddToWhitelistAdmin`
- **Command:** `add-to-whitelist-admin [sender] [address]`
- **Description:** Add an admin address to the whitelist used for admin functions on-chain.
- **Positional Arguments:**
  - `sender` Address of the sender
  - `address` Address that will be added to the whitelist

### Remove an Admin Address from the Whitelist
- **RPC Method:** `RemoveFromWhitelistAdmin`
- **Command:** `remove-from-whitelist-admin`
- **Description:** Remove an admin address from the whitelist used for admin functions on-chain.
- **Positional Arguments:**
  - `sender` Address of the sender
  - `address` Address that will be removed to the whitelist

### Register Network Actor
- **RPC Method:** `Register`
- **Command:** `register [sender] [lib_p2p_key] [multi_address] [topic_ids] [initial_stake] [owner] [is_reputer]`
- **Description:** Register a new reputer or worker for a topic.
- **Positional Arguments:**
  - `sender`
  - `lib_p2p_key`
  - `multi_address`
  - `topic_ids`
  - `owner`
  - `is_reputer`

### Remove a Reputer or Worker from a Topic
- **RPC Method:** `RemoveRegistration`
- **Command:** `remove-registration [creator] [owner] [is_reputer]`
- **Description:** Remove a reputer or worker from a topic.
- **Positional Arguments:**
  - `creator` Address of the creator
  - `owner` Address of the owner of the reputer/worker
  - `is_reputer` Set to `true` if the network participant to remove is a reputer

### Insert Bulk Reputer Payload
- **RPC Method:** `InsertBulkReputerPayload`
- **Command:** `insert-bulk-reputer-payload [reputer_value_bundles]`
- **Description:** Insert bulk reputer payload.
- **Positional Arguments:**
  - `reputer_value_bundles` Reputer payload to insert 

### Insert Bulk Worker Payload
- **RPC Method:** `InsertBulkWorkerPayload`
- **Command:** `insert-bulk-worker-payload [worker_value_bundles]`
- **Description:** Insert bulk worker payload.
- **Positional Arguments:**
  - `worker_value_bundles` Worker payload to insert 

### Add Stake
- **RPC Method:** `AddStake`
- **Command:** `add-stake [sender] [topic_id] [amount]`
- **Description:** Add stake [amount] to one's self sender [reputer or worker] for a topic.
- **Positional Arguments:**
  - `sender` The staker. This is the address of the transaction sender.
  - `topic_id` Identifier of the topic to add stake to
  - `amount` The stake

### Remove Stake from a Topic
- **RPC Method:** `RemoveStake`
- **Command:** `remove-stake [sender] [topic_id] [amount]`
- **Description:** Modify sender's [reputer] stake position by removing [amount] stake from a topic [topic_id].
- **Positional Arguments:**
  - `sender` The staker. This is the address of the transaction sender.
  - `topic_id` Identifier of the topic to remove stake from
  - `amount` The amount staked

### Delegate Stake to a Reputer for a Topic
- **RPC Method:** `DelegateStake`
- **Command:** `delegate-stake [sender] [topic_id] [reputer] [amount]`
- **Description:** Delegate stake [amount] to a reputer for a topic.
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `topic_id` Identifier of the topic to remove stake from
  - `reputer` Address of the reputer
  - `amount` The amount to add to stake

### Remove Delegate Stake from a Topic
- **RPC Method:** `RemoveDelegateStake`
- **Command:** `remove-delegate-stake [sender] [topic_id] [reputer] [amount]`
- **Description:** Modify sender's [reputer] delegate stake position by removing [amount] stake from a topic [topic_id] from a reputer [reputer].
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `topic_id` Identifier of the topic to remove stake from
  - `reputer` Address of the reputer
  - `amount` The amount to remove from stake

### Cancel Removing Delegate Stake
- **RPC Method:** `CancelRemoveDelegateStake`
- **Command:** `cancel-remove-delegate-stake [sender] [topic_id] [reputer]`
- **Description:** Cancel the removal of delegated stake for a delegator staking on a reputer in a topic
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `topic_id` Identifier of the topic
  - `reputer` Address of the reputer

### Cancel Removing Stake
- **RPC Method:** `CancelRemoveStake`
- **Command:** `cancel-remove-stake [sender] [topic_id]`
- **Description:** Cancel the removal of stake for a reputer in a topic
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `topic_id` Identifier of the topic

### Send Funds to a Topic to Pay for Inferences
- **RPC Method:** `FundTopic`
- **Command:** `fund-topic [sender] [topic_id] [amount] [extra_data]`
- **Description:** Send funds to a topic to pay for inferences.
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `topic_id` Identifier of the topic
  - `amount` The amount to send
  - `extra_data`

### Get Reward for Delegator for a Topic
- **RPC Method:** `RewardDelegateStake`
- **Command:** `reward-delegate-stake [sender] [topic_id] [reputer]`
- **Description:** Get Reward for Delegator [sender] for a topic.
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `topic_id` Identifier of the topic
  - `reputer` Address of the reputer

### Update Network Parameters
- **RPC Method:** `UpdateParams`
- **Command:** `update-params [sender] [params]`
- **Description:** Update parameters of the network.
- **Positional Arguments:**
  - `sender` This is the address of the transaction sender
  - `params` Params to be updated




================================================
FILE: pages/devs/reference/module-accounts.mdx
================================================
# Allora Module Accounts

The Allora Chain uses [Cosmos SDK module accounts](https://docs.cosmos.network/main/build/modules/bank#module-accounts) to hold tokens belonging to various different actors on the network. This page describes the various places that module accounts hold funds are held, and the flow of money through the network.

### Actors that Earn Token Rewards

There are three actors in the Allora network that earn token rewards:

- **Cosmos Validators**: For the service of running the cosmos blockchain powering Allora.
- **Reputers**: For providing ground truth to each topic, and maintaining a reputation system scoring the quality of worker outputs.
- **Workers**: For creating the actual AI/ML Inferences that the system provides for each topic.

### Sources of Token Rewards

There are also three sources of tokens rewards, that pay the three actors who earn them:

- **Cosmos network transaction fees**: Transaction fees on Allora are optional, at least at the time of this writing. However Cosmos SDK does support an optional transaction fee to be paid by the creator of a transaction, paid in units of computational steps taken (like gas for those familiar with the EVM). If the creator of a transaction chooses to add a fee (say, for get a higher priority of being added to a block), that fee will be paid out as token rewards.
- **Inference request fees**: When making an inference request, the requestor (inference data consumer) will bid a price they are willing to pay for that request. In that bid, they must send that amount of tokens to the network. If and when an inference is fulfilled, the upshot network will pay out the fee collected for that request as rewards.
- **Token inflationary rewards**: Allora has an inflationary token emissions schedule that halves on regular intervals, similar to Bitcoin. Newly minted tokens are paid out each block as rewards.

### Module Accounts Used by Allora

The following represents the list of module accounts that are changed or important in the flow of funds across the Allora Appchain. We do not discuss the standard module accounts used in cosmos-sdk validator staking, as they are unmodified from the Cosmos SDK. Note that the actual string used for the module name is the name in (`monospace`) below:

- **Mint** (`mint`): The Allora mint module account is the only account allowed to create new tokens. It creates new tokens during its `BeginBlock` according to the [Allora inflation schedule](./params/chain#halving_interval) and then immediately sends those token to the Fee Collector account.
- **Fee Collector** (`fee_collector`): This module account collects all transaction fees on the network (this happens in the `auth` module during transaction execution).
- **Distribution** (`distribution`): The distribution module holds the tokens and does the balance accounting for cosmos validator staking. It takes funds from the fee collector account. Cosmos validators can withdraw their staked tokens and receive validator rewards from this module's RPC functions. The Allora codebase does not change this standard cosmos module, but we do frontrun it (described below).
- **Allora Rewards**(`allorarewards`): The Allora Rewards module account holds the tokens earned by reputers and workers for their services to the network. Reputers and workers share the collected transaction fees and inflationary rewards on the network with cosmos validators at a [percentage rate](./params/chain#percent_rewards_reputers_workers) set in the chain parameters. When rewards are paid out each block, the Allora Rewards module account pays the Allora Staking module, which then also increments the reputer or worker's stake appropriately. 
- **Allora Staking** (`allorastaking`): Separate from the standard cosmos validator staking modules and workflow, Allora supports staking for our Reputer and Workers actor roles. The Allora Staking module account is our analog to the distribution module. It holds the tokens that stakers send to network when they deposit stake, and it also holds the rewards that stakers receive from transaction fees, newly minted token inflation, and inference request fees. When reputers or workers go to withdraw their stake, the rewards are automatically combined with their stake and automatically claimed.
- **Allora Requests** (`allorarequests`):  The Allora Requests module account holds the tokens that are paid by Inference Consumers when they make an inference request. This module holds escrowed funds for subscriptions and only pays out when actual inferences are made upon that subscription. That means this module can hold funds for long periods of time before the transaction fees for a given subscription are actually paid out. It pays the Fee Collector account.

### Module Execution Order in a Block and the Impact on Payment Flows

In Cosmos SDK, before the transactions of a block are processed, modules are able to specify `BeginBlock` and `EndBlock` functions that run at the beginning and end of a block, respectively. Below you can see a snippet from Allora Chain's app.yaml file, which specifies the order that these functions should be run:

```yaml app.yaml
  begin_blockers: [emissions, distribution, staking, mint]
  end_blockers: [staking, emissions]
```

The Cosmos SDK distribution module works by implementing a `BeginBlock` function that takes the money deposited in the Fee Collector account from the _previous block_. After that, the Mint module mints new tokens to the Fee Collector account. In the middle, transactions run, and pay their transaction fees, as well as inference request fees to the Fee Collector account.

The app.yaml places the Allora emissions module in front of the distribution module. This is where the `percent_rewards_reputers_workers` [chain parameter](./params/chain#percent_rewards_reputers_workers) takes some percentage of the Fee Collector's token balance, and sends it to the Allora Rewards module account. So basically, the Allora emissions module frontruns the distribution module to steal funds that it otherwise would have gotten, in order to take the percentage cut of rewards that belong to reputers and workers.

```Text Chronological Order of Payments
New block starts. Call BeginBlock:
  BeginBlock(emissions): allorarewards takes a percentage from fee_collector
  BeginBlock(distribution): distribution takes all tokens left in fee_collector
  BeginBlock(mint): mints new tokens to fee_collector 
Block starts processing transactions
  Auth module transfers transaction fees to fee_collector for each tx
Block about to end. Call EndBlock:
  EndBlock(emissions): all inference requests are executed, their fees are paid to fee_collector

 New block starts. Call BeginBlock...
```

### Rewards Epochs

Cosmos Validators can use the distribution module and staking module standard cosmos functions to manipulate their validator stake and claim their validator rewards.

For Reputer and Worker rewards, the `epoch_length` [chain parameter](./params/chain#epoch_length) controls how often the reputer and staker rewards are paid out. Every `epoch_length` epochs, the rewards calculation will run in the emissions `EndBlock`, which will cause the Allora Rewards module account to pay the Allora Staking module account directly. The Allora Staking module will then increase the staking balances of all actors who are paid rewards as part of this procedure. In this way Allora is able to autocompound stake positions.

Finally when a Reputer or Worker wishes to withdraw their stake, they do so, and the rewards are returned together with the original balance staked by the reputer or worker in one lump sum.



================================================
FILE: pages/devs/reference/params/_meta.json
================================================
{
  "stake": "Stake Parameters",
  "consensus": "Consensus Parameters",
  "mint": "Mint Parameters",
  "chain": "Chain Parameters"
}


================================================
FILE: pages/devs/reference/params/chain.mdx
================================================
# Chain Parameters

> A glossary and description of chain-level parameters

## Mint Module and Token Inflation Parameters

#### inflation_rate_change

The maximum permitted annual change in inflation rate. The mint module will throw an error if the inflation rate exceeds this value.

Default Value: 357.3582624

#### inflation_max

The maximum inflation rate. The mint module will throw an error if the inflation rate exceeds this value.

Default Value: 357.3582624

#### inflation_min

The minimum permitted inflation rate. The mint module will throw an error if the inflation rate goes below this value

Default Value: 0

#### goal_bonded

The goal used to target the percentage of bonded staking cosmos validators.

Default Value: 0.67

#### blocks_per_year

The amount of blocks that the inflation schedule believes will happen each year.

Default Value: 6311520

#### max_supply

The capped amount of tokens that will ever be allowed to exist.

Default Value: 1 Billion Allo \* 1e18 (for base unit uAllo): 1e28 uAllo

#### halving_interval

The number of blocks at which to halve the inflation rate of newly minted tokens, like Bitcoin's emissions schedule:

Default Value: 25246080

#### current_block_provision

Number of tokens that will be minted every block during a halving interval. This chain parameter controls the first value set for the first block. Afterwards, each halvening this value will be divided by two.

Default Value: 2831000000000000000000

## Allora Specific Parameters

#### reward_cadence

The duration of a reward epoch in blocks. Every `reward_cadence` seconds, rewards are recomputed within `EndBlock`.

Default Value: 600 blocks

Shorter epochs can lead to more frequent reward updates and responsiveness. This is advantageous for rapidly reacting to changes in the network (eg new topics, models, incentives, etc) and make the rewards available earlier. However small values they also have an impact on network efficiency.

#### min_topic_unmet_demand

The minimum unmet demand on a topic to consider it active, and thus enter rounds of inference solicitation and weight adjustment.

Default Value: 100 allo

The value provides a minimum amount of demand in order to trigger inference and weight adjustment rounds, to protect the network against activity of little to no added value. It is also kept small enough to not represent a barrier of entry for participation.

#### max_topics_per_block

Maximum number of active topics to run inference and weight adjustment rounds for on each block.

Default Value: 2048 topics

This value is high enough to allow a reasonable number of active topics to coexist, while also protecting the network against too much activity per block, preventing congestion and ensuring a more predictable block processing time.

#### min_request_unmet_demand

Threshold under which the inference requests will be deleted or prevented from being created.

Default Value: 1 allo

The purpose is to allow to prevent unnecessary processing of requests with minimal impact, keeping the state of the chain tidy, while at the same time be conservative with partially exhausted inference requests.

#### max_missing_inference_percent

The percentage of inferences rounds missed by a worker, over which the worker gets penalized.

Default Value: 20 %

Penalizing workers for missing inferences encourages reliability and accountability in the AI inference process. However, setting this value too low may lead to frequent penalties, potentially discouraging worker participation. A value that strikes a balance between both has been set.

#### required_minimum_stake

Sets the minimum stake to be considered as a reputer in good standing. If a reputer has less than this stake, than their contribution to reputation scoring will be ignored, and they will not receive any rewards from the system.

Default Value: 100 allo

Setting a minimum stake helps ensure that participants have a vested interest in the network's success and are not simply sybils, enhancing security and commitment, while at the same time not being too high so that it may limit the accessibility of the network and discourage potential legitimate participants.

#### remove_stake_delay_window

Sets the duration, in seconds, during which a staker's tokens remain staked after initiating the unstaking process. This protects against flash-type attacks.

Default Value: 172800  (1 day)

A fair delay in unstaking, which can ensure stability in the network by preventing sudden fluctuations in staked tokens and discourage malicious actors, while keeping it low enough so it is not very inconvenient to users who want to unstake their tokens promptly.

#### min_request_cadence

Sets the minimum allowed time interval, in seconds, between consecutive AI calls from an inference request.

Default Value: 10 seconds

Imposing a minimum cadence ensures a reasonable pacing of inference requests, preventing potential abuse or unnecessary strain on the network. Adjusted based on the expected frequency of AI inference requests and the network's capacity, balanced between responsiveness and resource efficiency. 

#### min_weight_cadence

Sets the minimum allowed time interval, in seconds, between consecutive calls for topic weight adjustment.

Default Value: 3600 seconds (1 hour)

Imposing a minimum cadence ensures a reasonable pacing of loss-calculation, preventing potential abuse or unnecessary strain on the network. That being said, it need not occur too frequently, because weights accrue over many inferences anyway, and these calls are relatively expensive involving off-chain communication.

#### max_inference_request_validity

Sets the maximum allowable time, in seconds, for an AI inference request to remain valid before expiration.

Default Value: 29030400 seconds (1year)

Setting a maximum validity time ensures that AI inference requests are processed within a reasonable timeframe, preventing outdated requests, while at the same time allowing inference requests to be planned and executed at the designed cadence within a generous timeframe, especially where time-dependent effects (e.g. seasonal effects) can happen.

#### max_request_cadence

Sets the maximum allowable time, in seconds, for an AI inference request to remain valid before expiration.

Default Value: 29030400 seconds (1 year)

A shorter validity period ensures that AI inference requests are designed to be processed more quickly and with up-to-date information. However, because lowering this value may lead to the rejection of legitimate requests if they take longer to process, the maximum allowed equals the max inference request, which is a conservative and flexible decision to allow inference requests creators for maximal planning ahead.

#### percent_rewards_reputers_workers

Cosmos validators, Allora Reputers, and Allora Workers all deserve to be paid out rewards from token inflation as well as collected transaction fees for using the Allora network. In Allora, we have two [payment flows](/devs/reference/module-accounts) for paying out rewards. Cosmos validators use the standard cosmos-sdk staking flows to get their rewards, and reputers and workers separately get their rewards from the Allora specific algorithm and code. This parameter controls the ratio of rewards between cosmos validators on one side, and reputers and workers on the other.

Default Value: 50%

A higher percentage would pay more transaction fees to reputers and workers, at the expense of giving less rewards to cosmos validators. A lower percentage value would give more rewards to cosmos validators, but pay out less rewards to reputers and workers for their services to the network.

#### epsilon_safe_div

A small tolerance quantity used to cap division by zero.

Default Value: 0.0000001

#### max_string_length

The maximum length of a string to allow to store on the chain. For example, used in limiting metadata for the creation of new topics.

Default Value: 255


================================================
FILE: pages/devs/reference/params/consensus.mdx
================================================
# Consensus Parameters

> Parameters that uniquely affect validators of the Allora Chain

**block.max_bytes**

Sets the maximum size of a block in bytes.

Value: `22020096`

Standard value.  
This parameter limits the block size, preventing excessive network load. However, setting it too low may restrict the number of transactions in a block. The current value strikes a balance between controlling block size and allowing for sufficient transaction throughput.

**block.max_gas**

Sets the maximum amount of gas that can be used in a block.

Value: `-1`

Standard value.  
The current setting allows for flexibility by indicating no limit on the maximum gas usage in a block. While this offers freedom for transactions, careful monitoring is needed to prevent potential abuse. This approach acknowledges the need for adaptability in a dynamic network environment.

**evidence.max_age_num_blocks**

Sets the maximum age (in blocks) of evidence that can be included in a block.

Value: `100000`

Standard value.  
By limiting the age of evidence, this parameter maintains network security by preventing the inclusion of outdated evidence. The chosen value strikes a reasonable balance between retaining relevant evidence and ensuring integrity of the network.

**evidence.max_age_duration**

Sets the maximum age (in nanoseconds) of evidence that can be included in a block.

Value: `172800000000000`

Standard value.  
This parameter complements `max_age_num_blocks` by providing an additional measure to limit the inclusion of outdated evidence. The current setting aligns with the need for a comprehensive yet controlled approach to evidence inclusion.

**evidence.max_bytes**

Sets the maximum size of evidence in bytes.

Value: `1048576`

Standard value.  
Controlling the size of evidence prevents potential abuse and ensures efficient network operation. While too low a value may restrict the inclusion of legitimate evidence, the current setting finds a suitable compromise between limiting size and maintaining the effectiveness of the evidence mechanism.

**validator.pub_key_types**

Specifies the supported public key types for validators.

Value: `["ed25519"]`

Standard value.  
This parameter enhances security by explicitly specifying the supported public key type for validators.



================================================
FILE: pages/devs/reference/params/mint.mdx
================================================
# Mint Parameters

> Parameters from the minting module on Allora Network

**mint_denom**

The mint denomination for the blockchain is `uallo`.

**inflation_rate_change**

Determines the maximum annual rate at which the inflation rate can change.

Value: `"0.130000000000000000"`

Standard value.  
Balances the rate of change to adapt to economic conditions while preventing sudden shocks. It will be regularly evaluated and adjusted based on economic dynamics.

**inflation_max**

Inflation max sets the maximum allowable annual inflation rate.

Value: `"0.200000000000000000"`

Standard value.  
It may be adjusted based on the balance between controlling token supply growth and incentivizing network participants.

**inflation_min**

Inflation min sets the minimum allowable annual inflation rate.

Value: `"0.070000000000000000"`

Standard value.  
It provides adequate incentives while avoiding undue token supply inflation.

**goal_bonded**

Represents the target ratio of bonded (staked) tokens to the total token supply.

Current value: `"0.670000000000000000"`

Standard value.  
It provides adequate incentives while avoiding undue token supply inflation.

**max_supply**

Maximum total supply of `uallo` 

Current value: `"1000000000000000000000000000"`

**halving_interval **

The block interval for the halving of the block reward.

Current value: `25246080`

**current_block_provision**

The initial value of provisions per block. This value is recalculated and updated with each block.

Current value: `2831000000000000000000`

**blocks_per_year** 

Value: `6311520`  (a block every ~5 seconds)

In Cronos, Treasurenet, Celestia, and many other Cosmos-based networks, the blocks per second are 5. This is the most common value for Cosmos-based blockchains.  
In Bittensor this is 12 seconds. We are allowing a higher number of topics (suggested 2048) than there are max subnets (32), so it is expected to have a higher level of activity, thus higher number of transactions - which could benefit from faster blocks.  
The standard value of a block every ~5 seconds is kept, for consistency with other blockchains and for allowing quicker block times and faster transactions than in Bittensor for the aforementioned reasons.



================================================
FILE: pages/devs/reference/params/stake.mdx
================================================
# Stake Parameters

> Parameters that affect both kinds of staking featured by Allora

There are two types of staking in Allora Network which run through different staking mechanisms: Validation staking and Reputational staking.

**Validation staking** comes from the popular `staking` module on Cosmos SDK. It is used when staking into Validator nodes.

**Reputational staking** is specific to Allora Network, and it is used to stake into Worker and Reputer nodes.

The parameters for the two types are specified below.

## Reputational Staking

Parameters from the reputational-staking module on Allora Network. These are parameters for staking into reputers and workers.

These parameters are defined as "Chain Parameters" and can be found [here](./chain).

The parameters of concern to reputers in particular are:

- **required_minimum_stake**
- **remove_stake_delay_window**

## Validation Staking

Parameters from the validator-based staking module on Allora Network

**unbonding_time**

Sets the duration for which tokens remain bonded after initiating the unbonding process.

Value: `1814400s` (3 weeks)

Standard value.  
A longer unbonding time enhances security by discouraging malicious actors and stabilizes token supply dynamics, but too long a period may inconvenience users who want to unstake their tokens promptly. This setting achieves a reasonable trade-off.

**max_validators**

Sets the maximum number of validators allowed in the network.

Value: `100`

Standard value.  
It balances decentralization with network scalability. It will be regularly assessed and adjusted based on the network's growth and decentralization.

**max_entries**

Determines the maximum number of entries in the staking transaction pool.

Value: `7`

Standard value.  
It balances the transaction pool size based on expected network demand. It will be regularly assessed and adjusted as the network evolves.

**historical_entries**

Sets the maximum number of historical entries stored in the staking module.

Value: `10000`

Standard value.  
It balances historical data retention with storage efficiency. It will be regularly assessed and adjusted based on storage capabilities and network requirements.

**bond_denom**

Specifies the denomination of the bonded tokens.

Value: `10000`

**min_commission_rate**

Sets the minimum commission rate a validator can charge.

Value: `0.000000000000000000`

Standard value. No minimum commission rate needs to be set. This incentivizes stakers into the validators, offering them higher rewards, and also as part of a community-building strategy.



================================================
FILE: pages/devs/reputers/_meta.json
================================================
{
    "reputers": "Deploy a Simple Reputer using Docker",
    "coin-prediction-reputer": "Deploy a Pre-configured Reputer",
    "set-and-adjust-stake": "Set and Adjust Stake",
    "query-reputer-data": " How to Query Reputer Data using allorad",
    "query-ema-score": "Query EMA Score for a Reputer using allorad"
}


================================================
FILE: pages/devs/reputers/coin-prediction-reputer.mdx
================================================
import { Callout } from 'nextra/components'

# Deploy a Coin Prediction Reputer

This is an example of a setup for running an Allora Network reputer node for providing ground truth and reputation, where the Allora Network node defers the requests to another container which is responsible for providing the ground truth, which is run in a separate container. It also provides a means of updating the internal database of the ground truth provider.

## Components

- **Reputer**: The node that responds to reputer requests from the Allora Network.
- **Truth**: A container that performs reputation tasks, maintains the state of the model, and responds to reputation requests via a simple Flask application. It fetches data from CoinGecko.
- **Updater**: A cron-like container designed to periodically trigger the Truth node's data updates.

A full working example for a reputer node for ETH price prediction topics is provided in the [`docker-compose.yml` file](https://github.com/allora-network/coin-prediction-reputer/blob/main/docker-compose.yml) of our example repo. Simply run:

## Explainer Video

Please see the video below to get a full deep-dive on how to deploy a reputer:

<iframe width="560" height="315" src="https://www.youtube.com/embed/STNSWR2MNqU?si=8tZwcpHLF5wez-wA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Setup a Reputer Node with `docker-compose`

### Download the Repository

```bash
git clone https://github.com/allora-network/coin-prediction-reputer.git
cd coin-prediction-reputer
```

## Configure Your Environment

1. Copy `config.example.json` and name the copy `config.json`.
2. Open `config.json` and **update** the necessary fields inside the `wallet` sub-object and `worker` config with your specific values:

### `wallet` Sub-object

1. `nodeRpc`: The [RPC URL](/devs/get-started/setup-wallet#rpc-url-and-chain-id) for the corresponding network the node will be deployed on
2. `addressKeyName`: The name you gave your wallet key when [setting up your wallet](/devs/get-started/setup-wallet)
3. `addressRestoreMnemonic`: The mnemonic that was outputted when setting up a new key

{/* <Callout type="info">
`addressKeyName` and `addressRestoreMnemonic` are optional parameters. If you did not previously generate keys, keys will be generated for you when [running the node](/devs/workers/deploy-worker/using-docker#generate-keys-and-export-variables).

If you have existing keys that you wish to use, you will need to provide these variables.
</Callout> */}

### `reputer` Config

1. `topicId`: The specific topic ID you created the reputer for.
2. `SourceOfTruthEndpoint`: The endpoint exposed by your source of truth server to provide the truth data to the network.
3. `Token`: The token for the specific topic you are verifying truth data for. This token should be included in the source of truth endpoint for retrieval.
   - The `Token` variable is specific to the endpoint you expose in your `main.py` file. It is **not** related to any blockchain parameter and is only locally specific.
4. `minStake`: The minimum stake required to participate as a reputer. This stake will be deducted from the reputer's wallet balance.
5. `loopSeconds`: The amount of seconds to wait between attempts to get the next reputer [nonce](devs/topic-creators/topic-life-cycle#nonce)

<Callout>
When placing your minimum stake, the system will verify the amount of funds you have already staked in the topic. If your staked amount is insufficient, it will automatically pull the necessary funds from your wallet to meet the required minimum.
</Callout>

<Callout type="warning">
The `reputer` config is an array of sub-objects, each representing a different topic ID. This structure allows you to manage multiple topic IDs, each within its own sub-object.

To deploy a reputer that provides inferences for multiple topics, you can duplicate the existing sub-object and add it to the `reputer` array. Update the `topicId`, `SourceOfTruthEndpoint`, `minStake` and `Token` fields with the appropriate values for each new topic:
```json
"worker": [
      {
          "topicId": 1,
          "reputerEntrypointName": "api-worker-reputer",
          "loopSeconds": 30,
          "minStake": 100000,
          "parameters": {
              "SourceOfTruthEndpoint": "http://source:8888/truth/{Token}/{BlockHeight}",
              "Token": "ethereum"
          }
      },
      // reputer providing ground truth for topic ID 2
      {
          "topicId": 2,
          "reputerEntrypointName": "api-worker-reputer",
          "loopSeconds": 30,
          "minStake": 100000,
          "parameters": {
              "SourceOfTruthEndpoint": "http://source:8888/truth/{Token}/{BlockHeight}",
              "Token": "ethereum"
          }
      }
    ],
```
</Callout>

## Running the Node

Now that the node is configured, let's deploy and register it to the network. To run the node, follow these steps:

### Export Variables

Execute the following command from the root directory:

```sh
chmod +x init.config
./init.config 
```

This command will automatically export the necessary variables from the account created. These variables are used by the offchain node and are bundled with your provided `config.json`, then passed to the node as environment variables.

<Callout>
If you need to **make changes** to your `config.json` file after you ran the `init.config` command, rerun:

```sh
chmod +x init.config
./init.config 
```

before proceeding.

</Callout>

### Request from Faucet

Copy your Allora address and request some tokens from the [Allora Testnet Faucet](https://faucet.testnet.allora.network/) to register your reputer in the next step successfully.

### Deploy the Node

```
docker compose up --build
```

Both the offchain node and the source services will be started. They will communicate through endpoints attached to the internal DNS.

A **successful** response from your Reputer should display:

```bash
{"level":"debug","msg":"Send Reputer Data to chain","txHash":"<tx-hash>","time":"<timestamp>","message":"Success"}
```

Congratulations! You've successfully deployed and registered your node on Allora.

### Keep it updated

You can keep the state updated by hitting the url: 

```
http://localhost:8000/update/<token-name>/<token-from>/<token-to>
```
where:
- `token-name`: the name of the token on internal database, e.g. ETHUSD
- `token-from`: the name of the token on Coingecko naming, e.g. ethereum
- `token-to`: the name of the token on Coingecko naming, e.g. usd

It is expected that this endpoint is hit periodically, as this is crucial for maintaining the accuracy of the provided ground truth.

## Testing the Truth Service

Here we'll setup a reputer with only the "truth service", which fetches the ground truth.

To only test the truth service, you can simply follow these steps:

- Run `docker compose up --build truth` and wait for the initial data load.
- Requests can now be sent, e.g. ETH price ground truths can be fetched with: 
  ```
    $ curl http://localhost:8000/gt/ETHUSD/1719565747
    {"value":"3431.440268842158"}
  ```
  or you can trigger an update to the current ETH price:
  ```
    $ curl http://localhost:8000/update/ETHUSD/ethereum/usd
  ```


================================================
FILE: pages/devs/reputers/query-ema-score.mdx
================================================
import { Callout } from 'nextra/components'

# How to Query Reputer EMA Scores

## What is an EMA Score?

The EMA score (Exponential Moving Average) reflects a reputer's performance over time for a given topic, balancing recent and past achievements, and helps determine whether a participant stays in the **active set** (eligible for [rewards](/home/layers/consensus/reputers)) or remains 
in the **passive set**. 

Read about our v0.3.0 release on [Merit-Based Sortitioning](/home/release-notes#v030) for a deeper dive on what makes up the active and passive set.

Active participants have their EMA score updated based on their current performance, which influences their ongoing 
eligibility for [rewards](/home/layers/consensus/reputers). In contrast, inactive participants, who do not contribute during a given [epoch](/home/key-terms#epochs), receive an adjusted score using a 
"dummy" value, which determines whether they can re-enter the active set in future epochs and qualify for rewards. 

This process ensures fairness while allowing inactive participants the chance to rejoin the active set based on their historical performance.

## Query EMA Score for a Specific Reputer

To query the EMA score for a specific reputer (identified by the reputer's allo address), run:

```bash
allorad q emissions reputer-score-ema [topic_id] [reputer_address] --node https://allora-rpc.testnet.allora.network/
```

- Replace `[topic_id]` and `[reputer_address]` with your specific details.

## Query the Lowest Reputer in the Active Set's EMA Score 

To query the lowest EMA score for a reputer in the Active Set, run:

```bash
allorad q emissions current-lowest-reputer-score [topic_id] --node https://allora-rpc.testnet.allora.network/
```

- Replace `[topic_id]` with your specific details.

<Callout>
To determine if your reputer is in the active set and eligible for rewards, query your reputer's EMA score and the lowest reputer in the active set's EMA score and compare them. 
If your reputer's EMA score for a specific topic is higher than the EMA score of the lowest reputer in the active set, your reputer is in the active set for that topic.
</Callout>


================================================
FILE: pages/devs/reputers/query-reputer-data.mdx
================================================
# How to Query Reputer Data using `allorad`

Below is a list of commands to understand how to pull information about reputers via [`allorad`](/devs/get-started/cli#installing-allorad):

## Prerequisites

- [`allorad` CLI](/devs/get-started/cli)
- A basic understanding of the Allora Network

## Query Functions

These functions read from the appchain only and do not write. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad q emissions [Command] --node <RPC_URL>
```

## Check if Reputer is Registered in a Topic

- **RPC Method:** `IsReputerRegisteredInTopicId`
- **Command:** `is-reputer-registered [topic_id] [address]`
- **Description:** Checks whether a reputer is registered in a specific topic. Returns `true` if the reputer is registered in the given topic, and `false` otherwise.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic where you want to check the reputer’s registration status.
    - `address`: The address of the reputer you want to check.

### Use Case:
**Why use it?**  
- This command is essential for verifying whether a reputer is properly registered in a specific topic before submitting reputation-related data or participating in topic-related activities.

**Example Scenario:**  
- Before a reputer attempts to evaluate workers or participate in consensus, you can confirm if they are registered to the relevant topic, ensuring their eligibility for participation.

## Check Reputer Stake in a Topic

- **RPC Method:** `GetReputerStakeInTopic`
- **Command:** `stake-in-topic-reputer [address] [topic_id]`
- **Description:** Retrieves the stake a reputer has in a specific topic, including any stake that has been delegated to them.
- **Positional Arguments:**
    - `address`: The address of the reputer whose stake is being queried.
    - `topic_id`: The identifier of the topic.

### Use Case:
**Why use it?**  
- This command is essential for understanding the total stake a reputer holds in a specific topic, including delegated stake, which is important for determining their influence.

**Example Scenario:**  
- Before delegating more stake, you may want to check how much stake a reputer already has in a particular topic.

---

## Get Total Delegate Stake in a Reputer for a Topic

- **RPC Method:** `GetDelegateStakeInTopicInReputer`
- **Command:** `stake-total-delegated-in-topic-reputer [reputer_address] [topic_id]`
- **Description:** Retrieves the total amount of stake delegated to a reputer for a specific topic.
- **Positional Arguments:**
    - `reputer_address`: The address of the reputer.
    - `topic_id`: The identifier of the topic.

### Use Case:
**Why use it?**  
- This command provides insight into how much stake has been delegated to a reputer for a given topic, which can impact their role in network consensus.

**Example Scenario:**  
- As a delegator, you may want to see how much stake has already been delegated to a reputer before deciding to contribute more.

---

## Get Stake Delegated to a Reputer

- **RPC Method:** `GetDelegateStakePlacement`
- **Command:** `delegate-stake-placement [topic_id] [delegator] [target]`
- **Description:** Retrieves the amount of tokens delegated to a specific reputer by a given delegator for a topic.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `delegator`: The address of the delegator.
    - `target`: The address of the target reputer.

### Use Case:
**Why use it?**  
- Use this command to track how much stake a delegator has assigned to a particular reputer in a specific topic.

**Example Scenario:**  
- A delegator can check the exact amount of tokens they have staked on a specific reputer within a topic.

---

## Get Removed Delegated Stake from a Reputer

- **RPC Method:** `GetDelegateStakeRemoval`
- **Command:** `delegate-stake-removal [block_height] [topic_id] [delegator] [reputer]`
- **Description:** Retrieves the current state of a pending delegate stake removal for a delegator in a topic.
- **Positional Arguments:**
    - `block_height`: The block height at which the removal is pending.
    - `topic_id`: The identifier of the topic.
    - `delegator`: The address of the delegator.
    - `reputer`: The address of the target reputer.

#### Use Case:
**Why use it?**  
- This command helps track pending removals of delegated stake, ensuring visibility into the process of un-staking tokens from a reputer.

**Example Scenario:**  
- A delegator can check the status of their pending delegate stake removal request.

---

## Get Total Stake Delegated to a Reputer

- **RPC Method:** `GetDelegateStakeUponReputer`
- **Command:** `delegate-stake-on-reputer [topic_id] [target]`
- **Description:** Retrieves the total amount of tokens delegated to a reputer in a specific topic.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `target`: The address of the target reputer.

### Use Case:
**Why use it?**  
- This command provides insight into the total delegated stake a reputer has accumulated in a given topic, which impacts their standing in the network.

**Example Scenario:**  
- You may want to know how much stake has been assigned to a reputer before deciding to interact with them in the topic.

---

## Get Reputer's Latest Score in a Topic

- **RPC Method:** `GetReputerScoreEma`
- **Command:** `reputer-score-ema [topic_id] [reputer]`
- **Description:** Returns the latest Exponential Moving Average (EMA) score for a reputer in a specific topic.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer.

### Use Case:
**Why use it?**  
- This command allows you to track the latest performance score of a reputer, giving insight into their effectiveness within the network.

**Example Scenario:**  
- Before delegating stake, you may want to see how well a reputer is performing in terms of their most recent EMA score.

---

## Get Reputer's Stake Removal Information

- **RPC Method:** `GetStakeRemovalForReputerAndTopicId`
- **Command:** `stake-removal [reputer] [topic_id]`
- **Description:** Retrieves information about a pending stake removal request for a reputer in a specific topic.
- **Positional Arguments:**
    - `reputer`: The address of the reputer.
    - `topic_id`: The identifier of the topic.

### Use Case:
**Why use it?**  
- Use this command to check the details of any pending stake removal for a reputer in a topic.

**Example Scenario:**  
- You can track the status of a reputer’s pending stake removal request in the network.

---

## Get Total Stake Delegated to a Reputer

- **RPC Method:** `GetStakeReputerAuthority`
- **Command:** `reputer-authority [topic_id] [reputer]`
- **Description:** Retrieves the total stake a reputer holds in a topic, including both their own stake and delegated stake.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer.

### Use Case:
**Why use it?**  
- This command provides a complete view of a reputer's stake in a topic, combining both self-stake and delegated stake, which influences their standing in the network.

**Example Scenario:**  
- Before interacting with a reputer in a topic, you may want to see their total stake, including how much has been delegated to them.

---

## Get Listening Coefficient for a Reputer

- **RPC Method:** `GetListeningCoefficient`
- **Command:** `listening-coefficient [topic_id] [reputer]`
- **Description:** Returns the current [listening coefficient](/home/layers/consensus/reputers#solution-adjusted-stake) for a given reputer in a specific topic. The coefficient measures how much a reputer is "listening" or interacting with the network. If no coefficient exists, it defaults to `1`.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer whose listening coefficient is being queried.

### Use Case:
**Why use it?**  
- This command is useful to determine how actively a reputer is interacting with a topic. The listening coefficient reflects how engaged the reputer is in the network's consensus and decision-making process.

**Example Scenario:**  
- As a delegator, you may want to check the listening coefficient of a reputer before deciding to delegate stake to them, ensuring they are actively participating in the topic.

---

## Get Multiple Reputers' Stakes in a Topic

- **RPC Method:** `GetMultiReputerStakeInTopic`
- **Command:** `multi-reputer-stake [addresses] [topic_id]`
- **Description:** Retrieves the stakes for each reputer in a given list of addresses for a specific topic. The list can contain up to the `MaxPageLimit` number of addresses. If a reputer does not exist, their stake is defaulted to 0.
- **Positional Arguments:**
    - `addresses`: A list of reputer addresses whose stakes you want to retrieve.
    - `topic_id`: The identifier of the topic.

### Use Case:
**Why use it?**  
- This command allows you to query the stakes of multiple reputers in a specific topic in a single request, making it useful for bulk operations or analysis.

**Example Scenario:**  
- You want to check the stakes of a list of reputers for a specific topic to compare their authority and influence in the topic.



================================================
FILE: pages/devs/reputers/reputers.mdx
================================================
import { Callout } from 'nextra/components'

# Deploy a Reputer Node using Docker

Deploying a reputer in the Allora network involves configuring the `config.example.json` file to ensure your reputer can interact with the network and provide accurate truth data.

To build this setup, please follow these steps:

## Prerequisites

Ensure you have the following installed on your machine:

- Git
- Go (version 1.16 or later)
- Docker

## Clone the `allora-offchain-node` Repository

Download the `allora-offchain-node` git repo:

```bash
git clone https://github.com/allora-network/allora-offchain-node
cd allora-offchain-node
```

## Configure Your Environment

1. Copy `config.example.json` and name the copy `config.json`.
2. Open `config.json` and **update** the necessary fields inside the `wallet` sub-object and `worker` config with your specific values:

### `wallet` Sub-object

1. `nodeRpc`: The [RPC URL](/devs/get-started/setup-wallet#rpc-url-and-chain-id) for the corresponding network the node will be deployed on
2. `addressKeyName`: The name you gave your wallet key when [setting up your wallet](/devs/get-started/setup-wallet)
3. `addressRestoreMnemonic`: The mnemonic that was outputted when setting up a new key

{/* <Callout type="info">
`addressKeyName` and `addressRestoreMnemonic` are optional parameters. If you did not previously generate keys, keys will be generated for you when [running the node](/devs/workers/deploy-worker/using-docker#generate-keys-and-export-variables).

If you have existing keys that you wish to use, you will need to provide these variables.
</Callout> */}

### `reputer` Config

1. `topicId`: The specific topic ID you created the reputer for.
2. `SourceOfTruthEndpoint`: The endpoint exposed by your source of truth server to provide the truth data to the network.
3. `Token`: The token for the specific topic you are verifying truth data for. This token should be included in the source of truth endpoint for retrieval.
   - The `Token` variable is specific to the endpoint you expose in your `main.py` file. It is **not** related to any blockchain parameter and is only locally specific.
4. `minStake`: The minimum stake required to participate as a reputer. This stake will be deducted from the reputer's wallet balance.

<Callout>
When placing your minimum stake, the system will verify the amount of funds you have already staked in the topic. If your staked amount is insufficient, it will automatically pull the necessary funds from your wallet to meet the required minimum.
</Callout>

<Callout type="warning">
The `reputer` config is an array of sub-objects, each representing a different topic ID. This structure allows you to manage multiple topic IDs, each within its own sub-object.

To deploy a reputer that provides inferences for multiple topics, you can duplicate the existing sub-object and add it to the `reputer` array. Update the `topicId`, `SourceOfTruthEndpoint`, `minStake` and `Token` fields with the appropriate values for each new topic:
```json
"worker": [
      {
          "topicId": 1,
          "reputerEntrypointName": "api-worker-reputer",
          "loopSeconds": 30,
          "minStake": 100000,
          "parameters": {
              "SourceOfTruthEndpoint": "http://source:8888/truth/{Token}/{BlockHeight}",
              "Token": "ethereum"
          }
      },
      // reputer providing ground truth for topic ID 2
      {
          "topicId": 2,
          "reputerEntrypointName": "api-worker-reputer",
          "loopSeconds": 30,
          "minStake": 100000,
          "parameters": {
              "SourceOfTruthEndpoint": "http://source:8888/truth/{Token}/{BlockHeight}",
              "Token": "ethereum"
          }
      }
    ],
```
</Callout>

### Worker Config

The `config.example.json` file that was copied and edited in the previous steps also contains a JSON object for configuring and deploying a [worker](/devs/workers/using-docker). To ignore the worker and only deploy a reputer, delete the reputer sub-object from the `config.json` file.

## Create the Truth Server

### Prepare the API Gateway

Ensure you have an API gateway or server that can accept API requests to call your model.

### Server Responsibilities

- Accept API requests from `main.go`.
- Respond with the corresponding inference obtained from the model.

### Truth Relay

Below is a sample structure of what your `main.go`, `main.py` and Dockerfile will look like. You can also find a working example [here](https://github.com/allora-network/basic-coin-prediction-node).

#### `main.go`

`allora-offchain-node` comes preconfigured with a `main.go` file inside the [`adapter/api-worker-reputer` folder](https://github.com/allora-network/allora-offchain-node/blob/dev/adapter/api-worker-reputer/main.go).

The `main.go` file fetches the responses outputted from the Source of Truth Endpoint based on the `SourceOfTruthEndpoint` and `Token` provided in the section above.

#### `main.py`

`allora-offchain-node` comes preconfigured with a Flask application that uses a `main.py` file to expose the Source of Truth Endpoint. 

The Flask application serves the request from `main.go`, which is routed to the `get_truth` function using the required arguments (`Token`, `blockHeight`). Before proceeding, ensure that all necessary packages are listed in the `requirements.txt` file.

```python
from flask import Flask
from model import get_inference  # Importing the hypothetical model

app = Flask(__name__)

@app.route('/truth/<token>/<blockheight>', methods=['GET'])
def get_truth(token, blockheight):
    random_float = str(random.uniform(0.0, 100.0))
    return random_float

if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

<Callout type="warning">
  The source of truth in `allora-offchain-node` is barebones and outputs a random integer. Follow the source of truth built in [`coin-prediction-reputer`](/devs/reputers/coin-prediction-reputer) as an example for a reputer that uses CoinGecko to fetch price data.
</Callout>

#### `Dockerfile`

A sample Dockerfile has been created in `allora-offchain-node` that can be used to deploy your model on port 8000.

```dockerfile
FROM python:3.9-slim

RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["python", "main.py"]
```

## Running the Node

Now that the node is configured, let's deploy and register it to the network. To run the node, follow these steps:

### Export Variables

Execute the following command from the root directory:

```sh
chmod +x init.config
./init.config 
```

This command will automatically export the necessary variables from the account created. These variables are used by the offchain node and are bundled with your provided `config.json`, then passed to the node as environment variables.

<Callout>
If you need to **make changes** to your `config.json` file after you ran the `init.config` command, rerun:

```sh
chmod +x init.config
./init.config 
```

before proceeding.

</Callout>

### Request from Faucet

Copy your Allora address and request some tokens from the [Allora Testnet Faucet](https://faucet.testnet.allora.network/) to register your worker in the next step successfully.

### Deploy the Node

```
docker compose up --build
```

Both the offchain node and the source services will be started. They will communicate through endpoints attached to the internal DNS.

A **successful** response from your Reputer should display:

```bash
{"level":"debug","msg":"Send Reputer Data to chain","txHash":"<tx-hash>","time":"<timestamp>","message":"Success"}
```

Congratulations! You've successfully deployed and registered your node on Allora.

## Learn More

Learn more by directly checking out the code and README for [the example ETH price reputer](https://github.com/allora-network/coin-prediction-reputer).


================================================
FILE: pages/devs/reputers/set-and-adjust-stake.mdx
================================================
# Set and Adjust Stake

> We define stake, motivate its use, and demonstrate how it can be adjusted

## How Stake works for Reputers

[Stake](/home/key-terms#stake) is used to signal confidence. A reputer earns more rewards based on their accuracy comparative to consensus (the other reputers providing data for a topic) and stake.

Stake also protects Allora from malicious behavior, such as sybil attacks. We require all types of nodes to register on the chain before they can earn any rewards. Registering requires staking at least a minimum amount of ALLO. As a result, creating an army of malicious nodes would quickly become prohibitively expensive.

## Prerequisites

- [`allorad` CLI](/devs/get-started/cli)

## Tx Functions

These functions read from the appchain only and do not write. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad tx emissions [Command] --node <RPC_URL>
```

## Add Stake to Self 

- **RPC Method:** `AddStake`
- **Command:** `add-stake [sender] [topic_id] [amount]`
- **Description:** Adds stake to the sender for a specific topic.
- **Positional Arguments:**
    - `sender`: The address of the sender adding stake.
    - `topic_id`: The identifier of the topic.
    - `amount`: The amount of stake to be added.

### Use Case:
**Why use it?**  
- This command is used when a reputer or worker wants to increase their stake in a specific topic, increasing their influence or authority.

**Example Scenario:**  
- As a reputer, you want to increase your stake in a specific topic to gain more influence and improve your reputation scores.

---

## Remove Stake from Self

- **RPC Method:** `RemoveStake`
- **Command:** `remove-stake [sender] [topic_id] [amount]`
- **Description:** Removes stake from the sender (a reputer) in a specific topic.
- **Positional Arguments:**
    - `sender`: The address of the sender removing stake (reputer).
    - `topic_id`: The identifier of the topic.
    - `amount`: The amount of stake to be removed.

### Use Case:
**Why use it?**  
- This command is used by reputers to reduce their stake in a topic, either for liquidity purposes or when their role in the topic has changed.

**Example Scenario:**  
- A reputer wants to reduce their stake in a topic after completing their contributions and being satisfied with the rewards.

---

## Cancel Pending Stake Removal (Reputer)

- **RPC Method:** `CancelRemoveStake`
- **Command:** `cancel-remove-stake [sender] [topic_id]`
- **Description:** Cancels the removal of stake that is pending for the sender (a reputer) in a topic.
- **Positional Arguments:**
    - `sender`: The address of the sender canceling the stake removal (reputer).
    - `topic_id`: The identifier of the topic.

### Use Case:
**Why use it?**  
- This command allows reputers to cancel a stake removal request if they change their mind and wish to keep their stake in the topic.

**Example Scenario:**  
- A reputer wants to cancel their stake removal request because they decide to maintain their position in the topic for an additional epoch.

---

## Delegate Stake to a Reputer

- **RPC Method:** `DelegateStake`
- **Command:** `delegate-stake [sender] [topic_id] [reputer] [amount]`
- **Description:** Delegates stake from the sender to a specific reputer for a topic.
- **Positional Arguments:**
    - `sender`: The address of the sender (delegator).
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer receiving the delegated stake.
    - `amount`: The amount of stake to be delegated.

### Use Case:
**Why use it?**  
- This command is used by delegators to delegate their stake to a reputer, giving the reputer more authority and influence within a specific topic.

**Example Scenario:**  
- As a delegator, you want to support a reputer you trust by delegating your tokens to them for a particular topic.

---

## Remove Delegated Stake from a Reputer

- **RPC Method:** `RemoveDelegateStake`
- **Command:** `remove-delegate-stake [sender] [topic_id] [reputer] [amount]`
- **Description:** Removes delegated stake from a reputer for a topic.
- **Positional Arguments:**
    - `sender`: The address of the sender (delegator).
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer whose delegated stake is being removed.
    - `amount`: The amount of stake to be removed.

### Use Case:
**Why use it?**  
- This command is used when a delegator wants to withdraw or reduce the stake they have delegated to a reputer in a topic.

**Example Scenario:**  
- A delegator wants to reduce their stake delegated to a reputer after reassessing the reputer's performance in a topic.

---

## Cancel Pending Delegated Stake Removal

- **RPC Method:** `CancelRemoveDelegateStake`
- **Command:** `cancel-remove-delegate-stake [sender] [topic_id] [reputer]`
- **Description:** Cancels the removal of delegated stake for a delegator staking on a reputer in a topic.
- **Positional Arguments:**
    - `sender`: The address of the sender (delegator).
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer whose delegated stake removal is being canceled.

### Use Case:
**Why use it?**  
- This command allows delegators to cancel a delegated stake removal request if they change their mind and want to keep their stake with the reputer.

**Example Scenario:**  
- A delegator decides to cancel their pending stake removal and continue supporting the reputer in the topic.

---

## Claim Rewards for Delegated Stake

- **RPC Method:** `RewardDelegateStake`
- **Command:** `reward-delegate-stake [sender] [topic_id] [reputer]`
- **Description:** Claims the rewards for a delegator who has delegated stake to a reputer in a specific topic.
- **Positional Arguments:**
    - `sender`: The address of the sender (delegator).
    - `topic_id`: The identifier of the topic.
    - `reputer`: The address of the reputer to whom the stake was delegated.

### Use Case:
**Why use it?**  
- This command is used by delegators to claim their rewards based on the performance of the reputer they delegated stake to.

**Example Scenario:**  
- A delegator wants to claim their rewards for a topic after their reputer has successfully contributed to the topic's outcomes.



================================================
FILE: pages/devs/sdk/_meta.json
================================================
{
    "overview": "Overview",
    "allora-sdk-ts": "TypeScript SDK",
    "allora-sdk-py": "Python SDK"
}



================================================
FILE: pages/devs/sdk/allora-sdk-py.mdx
================================================
import { Callout } from 'nextra/components'

# Allora Python SDK

The Allora Python SDK provides a convenient way to interact with the Allora API from Python applications.

## Installation

You can install the Allora Python SDK using pip:

```bash
pip install allora_sdk
```

## Basic Usage

Here's how to use the Allora Python SDK:

```python
from allora_sdk import AlloraClient

# Initialize the client
client = AlloraClient(
    chain="testnet",  # Use "mainnet" for mainnet
    api_key="YOUR_API_KEY"  # Optional, but recommended for production use
)

# Fetch all available topics
def fetch_topics():
    try:
        topics = client.get_all_topics()
        print(f"Available topics: {topics}")
    except Exception as e:
        print(f"Error fetching topics: {e}")

# Fetch inference for a specific topic
def fetch_inference(topic_id):
    try:
        inference = client.get_inference_by_topic_id(topic_id)
        print(f"Inference data: {inference}")
    except Exception as e:
        print(f"Error fetching inference: {e}")

# Fetch price inference for a specific asset and timeframe
def fetch_price_inference():
    try:
        inference = client.get_price_inference(
            asset="BTC",
            timeframe="8h"
        )
        print(f"Price inference data: {inference}")
    except Exception as e:
        print(f"Error fetching price inference: {e}")
```

## API Reference

### `AlloraClient`

The main class for interacting with the Allora API.

#### Constructor

```python
def __init__(self, chain="testnet", api_key=None, base_api_url=None):
    """
    Initialize the Allora API client.
    
    Args:
        chain (str): The chain to use. Can be "testnet" or "mainnet".
        api_key (str, optional): Your API key. Recommended for production use.
        base_api_url (str, optional): The base URL for the API.
    """
```

#### Methods

##### `get_all_topics()`

Fetches all available topics from the Allora API.

```python
def get_all_topics(self):
    """
    Fetch all available topics from the Allora API.
    
    Returns:
        list: A list of all available topics.
    
    Raises:
        Exception: If the API request fails.
    """
```

##### `get_inference_by_topic_id(topic_id, signature_format="ethereum-11155111")`

Fetches an inference for a specific topic from the Allora API.

```python
def get_inference_by_topic_id(self, topic_id, signature_format="ethereum-11155111"):
    """
    Fetch an inference for a specific topic from the Allora API.
    
    Args:
        topic_id (int): The unique identifier of the topic to get inference for.
        signature_format (str, optional): The format of the signature.
            Defaults to "ethereum-11155111".
    
    Returns:
        dict: The inference data.
    
    Raises:
        Exception: If the API request fails.
    """
```

##### `get_price_inference(asset, timeframe, signature_format="ethereum-11155111")`

Fetches a price inference for a specific asset and timeframe from the Allora API.

```python
def get_price_inference(self, asset, timeframe, signature_format="ethereum-11155111"):
    """
    Fetch a price inference for a specific asset and timeframe from the Allora API.
    
    Args:
        asset (str): The asset to get price inference for. Can be "BTC" or "ETH".
        timeframe (str): The timeframe to get price inference for. Can be "5m" or "8h".
        signature_format (str, optional): The format of the signature.
            Defaults to "ethereum-11155111".
    
    Returns:
        dict: The inference data.
    
    Raises:
        Exception: If the API request fails.
    """
```

## Examples

### Fetching and Using Price Inference

```python
import os
from allora_sdk import AlloraClient

# Initialize the client
client = AlloraClient(
    chain="testnet",
    api_key=os.environ.get("ALLORA_API_KEY")
)

try:
    # Fetch BTC price inference for 8-hour timeframe
    inference = client.get_price_inference(
        asset="BTC",
        timeframe="8h"
    )
    
    # Extract the network inference value
    network_inference = inference["inference_data"]["network_inference"]
    print(f"BTC 8-hour price inference: {network_inference}")
    
    # Extract confidence interval values
    confidence_intervals = inference["inference_data"]["confidence_interval_values"]
    print("Confidence intervals:", confidence_intervals)
    
    # Use the inference data in your application
    # ...
except Exception as e:
    print(f"Error fetching BTC price inference: {e}")
```

### Fetching All Topics and Displaying Them

```python
import os
from allora_sdk import AlloraClient

# Initialize the client
client = AlloraClient(
    chain="testnet",
    api_key=os.environ.get("ALLORA_API_KEY")
)

try:
    # Fetch all topics
    topics = client.get_all_topics()
    
    # Display topics
    print(f"Found {len(topics)} topics:")
    for topic in topics:
        print(f"- Topic ID: {topic['topic_id']}")
        print(f"  Name: {topic['topic_name']}")
        print(f"  Description: {topic.get('description', 'No description')}")
        print(f"  Active: {'Yes' if topic.get('is_active') else 'No'}")
        print(f"  Worker count: {topic['worker_count']}")
        print(f"  Updated at: {topic['updated_at']}")
        print("---")
except Exception as e:
    print(f"Error fetching topics: {e}")
```

### Using Inference Data in a Web Application

```python
from flask import Flask, jsonify
import os
from allora_sdk import AlloraClient

app = Flask(__name__)

# Initialize the client
client = AlloraClient(
    chain="testnet",
    api_key=os.environ.get("ALLORA_API_KEY")
)

@app.route('/api/price/btc')
def get_btc_price():
    try:
        # Fetch BTC price inference
        inference = client.get_price_inference(
            asset="BTC",
            timeframe="8h"
        )
        
        # Extract the network inference value and confidence intervals
        return jsonify({
            'price': inference["inference_data"]["network_inference"],
            'confidence_intervals': inference["inference_data"]["confidence_interval_values"],
            'timestamp': inference["inference_data"]["timestamp"]
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
```



================================================
FILE: pages/devs/sdk/allora-sdk-ts.mdx
================================================
import { Callout } from 'nextra/components'

# Allora TypeScript SDK

The Allora TypeScript SDK provides a convenient way to interact with the Allora API from JavaScript and TypeScript applications.

## Installation

You can install the Allora TypeScript SDK using npm or yarn:

```bash
# Using npm
npm install @alloralabs/allora-sdk

# Using yarn
yarn add @alloralabs/allora-sdk
```

## Basic Usage

Here's a simple example of how to use the Allora TypeScript SDK:

```typescript
import { AlloraAPIClient, ChainSlug } from '@alloralabs/allora-sdk/v2'

// Initialize the client
const alloraClient = new AlloraAPIClient({
  chainSlug: ChainSlug.TESTNET, // Use ChainSlug.MAINNET for mainnet
  apiKey: process.env.ALLORA_API_KEY, // Optional, but recommended for production use
});

// Fetch all available topics
async function fetchTopics() {
  try {
    const topics = await alloraClient.getAllTopics();
    console.log('Available topics:', topics);
  } catch (error) {
    console.error('Error fetching topics:', error);
  }
}

// Fetch inference for a specific topic
async function fetchInference(topicId: number) {
  try {
    const inference = await alloraClient.getInferenceByTopicID(topicId);
    console.log('Inference data:', inference);
  } catch (error) {
    console.error('Error fetching inference:', error);
  }
}

// Fetch price inference for a specific asset and timeframe
async function fetchPriceInference() {
  try {
    const inference = await alloraClient.getPriceInference(
      PriceInferenceToken.BTC,
      PriceInferenceTimeframe.EIGHT_HOURS
    );
    console.log('Price inference data:', inference);
  } catch (error) {
    console.error('Error fetching price inference:', error);
  }
}
```

<Callout type="info">
The API key is optional but recommended for production use. If not provided, a default API key will be used, which may be subject to rate limiting.
</Callout>

## API Reference

### `AlloraAPIClient`

The main class for interacting with the Allora API.

#### Constructor

```typescript
constructor(config: AlloraAPIClientConfig)
```

Parameters:
- `config`: An object with the following properties:
  - `chainSlug`: The chain to use. Can be `ChainSlug.TESTNET` or `ChainSlug.MAINNET`.
  - `apiKey`: Your API key. Optional, but recommended for production use.
  - `baseAPIUrl`: The base URL for the API. Optional, defaults to `https://api.allora.network/v2`.

#### Methods

##### `getAllTopics()`

Fetches all available topics from the Allora API.

```typescript
async getAllTopics(): Promise<AlloraTopic[]>
```

Returns: A promise that resolves to an array of all available topics.

##### `getInferenceByTopicID(topicID, signatureFormat)`

Fetches an inference for a specific topic from the Allora API.

```typescript
async getInferenceByTopicID(
  topicID: number,
  signatureFormat: SignatureFormat = SignatureFormat.ETHEREUM_SEPOLIA
): Promise<AlloraInference>
```

Parameters:
- `topicID`: The unique identifier of the topic to get inference for.
- `signatureFormat`: The format of the signature. Optional, defaults to `SignatureFormat.ETHEREUM_SEPOLIA`.

Returns: A promise that resolves to the inference data.

##### `getPriceInference(asset, timeframe, signatureFormat)`

Fetches a price inference for a specific asset and timeframe from the Allora API.

```typescript
async getPriceInference(
  asset: PriceInferenceToken,
  timeframe: PriceInferenceTimeframe,
  signatureFormat: SignatureFormat = SignatureFormat.ETHEREUM_SEPOLIA
): Promise<AlloraInference>
```

Parameters:
- `asset`: The asset to get price inference for. Can be `PriceInferenceToken.BTC` or `PriceInferenceToken.ETH`.
- `timeframe`: The timeframe to get price inference for. Can be `PriceInferenceTimeframe.FIVE_MIN` or `PriceInferenceTimeframe.EIGHT_HOURS`.
- `signatureFormat`: The format of the signature. Optional, defaults to `SignatureFormat.ETHEREUM_SEPOLIA`.

Returns: A promise that resolves to the inference data.

### Enums

#### `ChainSlug`

```typescript
enum ChainSlug {
  TESTNET = "testnet",
  MAINNET = "mainnet",
}
```

#### `PriceInferenceToken`

```typescript
enum PriceInferenceToken {
  BTC = "BTC",
  ETH = "ETH",
}
```

#### `PriceInferenceTimeframe`

```typescript
enum PriceInferenceTimeframe {
  FIVE_MIN = "5m",
  EIGHT_HOURS = "8h",
}
```

#### `SignatureFormat`

```typescript
enum SignatureFormat {
  ETHEREUM_SEPOLIA = "ethereum-11155111",
}
```

### Interfaces

#### `AlloraAPIClientConfig`

```typescript
interface AlloraAPIClientConfig {
  chainSlug?: ChainSlug;
  apiKey?: string;
  baseAPIUrl?: string;
}
```

#### `AlloraTopic`

```typescript
interface AlloraTopic {
  topic_id: number;
  topic_name: string;
  description?: string | null;
  epoch_length: number;
  ground_truth_lag: number;
  loss_method: string;
  worker_submission_window: number;
  worker_count: number;
  reputer_count: number;
  total_staked_allo: number;
  total_emissions_allo: number;
  is_active: boolean | null;
  updated_at: string;
}
```

#### `AlloraInferenceData`

```typescript
interface AlloraInferenceData {
  network_inference: string;
  network_inference_normalized: string;
  confidence_interval_percentiles: string[];
  confidence_interval_percentiles_normalized: string[];
  confidence_interval_values: string[];
  confidence_interval_values_normalized: string[];
  topic_id: string;
  timestamp: number;
  extra_data: string;
}
```

#### `AlloraInference`

```typescript
interface AlloraInference {
  signature: string;
  inference_data: AlloraInferenceData;
}
```

## Examples

### Fetching and Using Price Inference

```typescript
import { AlloraAPIClient, ChainSlug, PriceInferenceToken, PriceInferenceTimeframe } from '@alloralabs/allora-sdk/v2'

async function fetchAndUseBTCPriceInference() {
  // Initialize the client
  const alloraClient = new AlloraAPIClient({
    chainSlug: ChainSlug.TESTNET,
    apiKey: process.env.ALLORA_API_KEY,
  });

  try {
    // Fetch BTC price inference for 8-hour timeframe
    const inference = await alloraClient.getPriceInference(
      PriceInferenceToken.BTC,
      PriceInferenceTimeframe.EIGHT_HOURS
    );

    // Extract the network inference value
    const networkInference = inference.inference_data.network_inference;
    console.log(`BTC 8-hour price inference: ${networkInference}`);

    // Extract confidence interval values
    const confidenceIntervals = inference.inference_data.confidence_interval_values;
    console.log('Confidence intervals:', confidenceIntervals);

    // Use the inference data in your application
    // ...
  } catch (error) {
    console.error('Error fetching BTC price inference:', error);
  }
}
```

### Fetching All Topics and Displaying Them

```typescript
import { AlloraAPIClient, ChainSlug } from '@alloralabs/allora-sdk/v2'

async function displayAllTopics() {
  // Initialize the client
  const alloraClient = new AlloraAPIClient({
    chainSlug: ChainSlug.TESTNET,
    apiKey: process.env.ALLORA_API_KEY,
  });

  try {
    // Fetch all topics
    const topics = await alloraClient.getAllTopics();

    // Display topics
    console.log(`Found ${topics.length} topics:`);
    topics.forEach(topic => {
      console.log(`- Topic ID: ${topic.topic_id}`);
      console.log(`  Name: ${topic.topic_name}`);
      console.log(`  Description: ${topic.description || 'No description'}`);
      console.log(`  Active: ${topic.is_active ? 'Yes' : 'No'}`);
      console.log(`  Worker count: ${topic.worker_count}`);
      console.log(`  Updated at: ${topic.updated_at}`);
      console.log('---');
    });
  } catch (error) {
    console.error('Error fetching topics:', error);
  }
}
```

### Using Inference Data with React

```typescript
import React, { useState, useEffect } from 'react';
import { AlloraAPIClient, ChainSlug, PriceInferenceToken, PriceInferenceTimeframe } from '@alloralabs/allora-sdk/v2';

function PriceDisplay() {
  const [price, setPrice] = useState<string | null>(null);
  const [loading, setLoading] = useState<boolean>(true);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    const fetchPrice = async () => {
      try {
        setLoading(true);
        
        // Initialize the client
        const alloraClient = new AlloraAPIClient({
          chainSlug: ChainSlug.TESTNET,
          apiKey: process.env.REACT_APP_ALLORA_API_KEY,
        });

        // Fetch ETH price inference
        const inference = await alloraClient.getPriceInference(
          PriceInferenceToken.ETH,
          PriceInferenceTimeframe.FIVE_MIN
        );

        // Set the price
        setPrice(inference.inference_data.network_inference);
        setError(null);
      } catch (err) {
        setError('Failed to fetch price data');
        console.error(err);
      } finally {
        setLoading(false);
      }
    };

    fetchPrice();
    
    // Refresh price every 5 minutes
    const intervalId = setInterval(fetchPrice, 5 * 60 * 1000);
    
    // Clean up interval on component unmount
    return () => clearInterval(intervalId);
  }, []);

  if (loading) return <div>Loading price data...</div>;
  if (error) return <div>Error: {error}</div>;

  return (
    <div>
      <h2>Current ETH Price</h2>
      <p className="price">{price}</p>
    </div>
  );
}

export default PriceDisplay;
```



================================================
FILE: pages/devs/sdk/overview.mdx
================================================
# Allora SDKs

Allora provides Software Development Kits (SDKs) in multiple programming languages to make it easier to integrate with the Allora network. These SDKs provide a convenient way to interact with the Allora API and consume inferences from the network.

Currently, the following SDKs are available:

- [TypeScript SDK](/devs/sdk/allora-sdk-ts) - For JavaScript and TypeScript applications
- [Python SDK](/devs/sdk/allora-sdk-py) - For Python applications

Choose the SDK that best fits your development environment and follow the getting started guide to begin integrating with Allora.



================================================
FILE: pages/devs/topic-creators/_meta.json
================================================
{
    "topic-life-cycle": "Topic Life Cycle",
    "how-to-create-topic": "How to Create/Fund a Topic using allorad",
    "query-topic-data": "How to Query Topic Data using allorad"
}


================================================
FILE: pages/devs/topic-creators/how-to-create-topic.mdx
================================================
# How to Create a Topic

> Inferences for the same domain are aggregated into the same topic

## What is a Topic?

Topics are [Schelling points](https://en.wikipedia.org/wiki/Focal_point_(game_theory)) where disparate-but-alike data scientists and domain experts aggregate their predictions. For example, we might create a topic for predicting the future price of ETH. There, all experts with any talent in predicting the future price of ETH will submit their inferences. Topics vary by domain and parameterization, defining how these inferences are collected and valued.

Developers can make topics for arbitrary categories of inferences so long as they complete these steps:

## Prerequisites:

1. A wallet with sufficient funds to at least cover gas. Use [the faucet](/devs/get-started/setup-wallet) to get funds. 
2. [Allorad CLI tool](/devs/get-started/cli#installing-allorad)

## Explainer Video

Please see the video below to get a full deep-dive on the different parameters that make up a topic:

<iframe width="560" height="315" src="https://www.youtube.com/embed/WfM3Nrkgh6Y?si=Syap6TMjU7usLwVH" title="How to create a Topic" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Tx Functions

These functions write to the appchain. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad tx emissions [Command]
```

## Creating Your First Topic

The transaction for creating a topic has the following structure:

```go
type MsgCreateNewTopic struct {
  // Address of the wallet that will own the topic
  Creator          string   `json:"creator,omitempty"`
  // Information about the topic
  Metadata         string   `json:"metadata,omitempty"`
  // The method used for loss calculations 
  LossMethod       string   `json:"loss_method,omitempty"`
  // The frequency (in blocks) of inference calculations (Must be greater than 0)
  EpochLength      int64    `json:"epoch_length,omitempty"`
  // The time it takes for the ground truth to become available (Cannot be negative)
  GroundTruthLag   int64    `json:"ground_truth_lag,omitempty"`
  // the time window within a given epoch that worker nodes can submit an inference
  WorkerSubmissionWindow int64 `json:"worker_submission_window"`
  // Raising this parameter raises how much high-quality inferences are favored over lower-quality inferences (Must be between 2.5 and 4.5)
  PNorm            github_com_allora_network_allora_chain_math.Dec `json:"p_norm"`
  // Raising this parameter lowers how much workers historical performances influence their current reward distribution (Must be between 0 and 1)
  AlphaRegret      github_com_allora_network_allora_chain_math.Dec  `json:"alpha_regret"`
  // Indicates if the loss function's output can be negative. If false, the reputer submits logs of losses; if true, the reputer submits raw losses.
  AllowNegative    bool     `json:"allow_negative,omitempty"`
  // the numerical precision at which the network should distinguish differences in the logarithm of the loss
  Epsilon          github_com_allora_network_allora_chain_math.Dec  `json:"epsilon"`
  MeritSortitionAlpha github_com_allora_network_allora_chain_math.Dec  `json:"merit_sortition_alpha"`
  ActiveInfererQuantile github_com_allora_network_allora_chain_math.Dec `json:"active_inferer_quantile"`
  ActiveForecasterQuantile github_com_allora_network_allora_chain_math.Dec `json:"active_forecaster_quantile"`
  ActiveReputerQuantile github_com_allora_network_allora_chain_math.Dec `json:"active_reputer_quantile"`
}
```

Using the [`allorad` CLI](/devs/get-started/cli#installing-allorad) to create a topic:

```shell bash
allorad tx emissions create-topic \
  allo13tr5nx74zjdh7ya8kgyuu0hweppnnx8d4ux7pj \    # Creator address
  "ETH prediction in 24h" \                                         # Metadata
  "mse" \                                         # LossMethod
  3600 \                                          # EpochLength
  0 \                                             # GroundTruthLag
  3 \                                             # WorkerSubmissionWindow
  3 \                                             # PNorm 
  1 \                                             # AlphaRegret 
  true \                                          # AllowNegative
  0.001 \                                         # Epsilon
  0.1 \                                           # MeritSortitionAlpha
  0.25 \                                           # ActiveInfererQuantile
  0.25 \                                           # ActiveForecasterQuantile
  0.25 \                                           # ActiveReputerQuantile
  --node <RPC_URL>
  --chain-id <CHAIN_ID>
```

Be sure to swap out [`RPC_URL`](/devs/get-started/setup-wallet#rpc-url-and-chain-id), `YOUR_ADDRESS`, [`CHAIN_ID`](/devs/get-started/setup-wallet#rpc-url-and-chain-id) and all other arguments as appropriate with the desired values.

### Notes

An explanation in more detail of some of these fields. 

- `Metadata` is a descriptive field to let users know what this topic is about and/or any specific indication about how it is expected to work.
- `allowNegative` determines whether the loss function output can be negative.
  - If **true**, the reputer submits raw losses.
  - If **false**, the reputer submits logs of losses.

---

## Fund a Topic

- **RPC Method:** `FundTopic`
- **Command:** `fund-topic [sender] [topic_id] [amount] [extra_data]`
- **Description:** Sends funds to a specific topic to be used for paying for inferences or other topic-related activities.
- **Positional Arguments:**
    - `sender`: The address of the sender providing the funds.
    - `topic_id`: The identifier of the topic to receive the funds.
    - `amount`: The amount of funds being sent to the topic.
    - `extra_data`: Additional data or metadata associated with the funding transaction.

### Use Case:
**Why use it?**  
- This command is used to fund a topic, ensuring there are sufficient funds available to reward workers, forecasters, or other participants submitting inferences or engaging with the topic.

**Example Scenario:**  
- As a network administrator or topic creator, you want to add funds to a topic to ensure that workers and forecasters are compensated for their contributions.



================================================
FILE: pages/devs/topic-creators/query-topic-data.mdx
================================================
# How to Query Topic Data using `allorad`


To query network-level data on the Allora chain using the `allorad` CLI, you need to interact with various RPC methods designed to return aggregate or holistic information about specific topics.

## Prerequisites

- [`allorad` CLI](/devs/get-started/cli)

## Query Functions

These functions read from the appchain only and do not write. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad q emissions [Command] --node <RPC_URL>
```

### Get Topic by Topic ID

- **RPC Method:** `GetTopic`
- **Command:** `topic [topic_id]`
- **Description:** Retrieves information about a specific topic by its ID.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to query details about a particular topic.

**Example Scenario:**  
- You want to check the metadata and settings for a specific topic in the network.

---

### Check if Topic Exists

- **RPC Method:** `TopicExists`
- **Command:** `topic-exists [topic_id]`
- **Description:** Checks if a topic exists at the given ID. Returns `true` if the topic exists, `false` otherwise.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to verify whether a topic has been created or is active in the network.

**Example Scenario:**  
- Before interacting with a topic, you want to confirm that it exists in the system.

---

### Check if Topic is Active

- **RPC Method:** `IsTopicActive`
- **Command:** `is-topic-active [topic_id]`
- **Description:** Checks whether a specific topic is currently active. Returns `true` if the topic is active, `false` otherwise.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- This command helps determine if a topic is active and available for participation.

**Example Scenario:**  
- Before submitting any data, you want to confirm that the topic is active and accepting inputs.

---

### Get Next Topic ID

- **RPC Method:** `GetNextTopicId`
- **Command:** `next-topic-id`
- **Description:** Returns the ID of the next available topic that can be created.

#### Use Case:
**Why use it?**  
- Use this command to determine the next available topic ID when creating a new topic.

**Example Scenario:**  
- Before creating a new topic, you may want to check what the next topic ID will be.

---

### Get Reputer Stake in Topic

- **RPC Method:** `GetReputerStakeInTopic`
- **Command:** `stake-in-topic-reputer [address] [topic_id]`
- **Description:** Retrieves the stake a reputer has in a specific topic, including any stake that has been delegated to them.
- **Positional Arguments:**
    - `address`: The address of the reputer.
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to check the total stake a reputer holds in a specific topic.

**Example Scenario:**  
- You want to verify how much stake a particular reputer has in a specific topic.

---

### Get Total Stake Delegated to Reputer in a Topic

- **RPC Method:** `GetDelegateStakeInTopicInReputer`
- **Command:** `stake-total-delegated-in-topic-reputer [reputer_address] [topic_id]`
- **Description:** Retrieves the total stake that has been delegated to a reputer in a specific topic.
- **Positional Arguments:**
    - `reputer_address`: The address of the reputer.
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to see how much stake has been delegated to a reputer in a topic.

**Example Scenario:**  
- You want to check the total delegated stake assigned to a specific reputer.

---

### Get Delegate Stake Placement in Topic

- **RPC Method:** `GetDelegateStakePlacement`
- **Command:** `delegate-stake-placement [topic_id] [delegator] [target]`
- **Description:** Retrieves the amount of tokens delegated to a specific reputer by a given delegator for a topic.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `delegator`: The address of the delegator.
    - `target`: The address of the target reputer.

#### Use Case:
**Why use it?**  
- This command allows delegators to track how much stake they have assigned to a reputer for a topic.

**Example Scenario:**  
- You want to know how much stake you have delegated to a particular reputer in a specific topic.

---

### Get Delegate Stake Removal in a Topic

- **RPC Method:** `GetDelegateStakeRemoval`
- **Command:** `delegate-stake-removal [block_height] [topic_id] [delegator] [reputer]`
- **Description:** Retrieves the current state of a pending delegate stake removal in a topic.
- **Positional Arguments:**
    - `block_height`: The block height at which the removal is pending.
    - `topic_id`: The identifier of the topic.
    - `delegator`: The address of the delegator.
    - `reputer`: The address of the reputer.

#### Use Case:
**Why use it?**  
- Use this command to check the status of pending delegated stake removals in a topic.

**Example Scenario:**  
- You want to know whether your request to remove delegated stake is still pending.

---

### Get Total Stake in Topic

- **RPC Method:** `GetTopicStake`
- **Command:** `topic-stake [topic_id]`
- **Description:** Retrieves the total amount of stake, including delegate stake, in a specific topic.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to check the total stake in a topic, including both direct and delegated stakes.

**Example Scenario:**  
- You want to know the overall stake in a topic before participating or delegating more tokens.

---

### Get Latest Available Network Inferences for a Topic

- **RPC Method:** `GetLatestAvailableNetworkInferences`
- **Command:** `latest-available-network-inferences [topic_id]`
- **Description:** Retrieves the latest network inference for a given topic, but only if all necessary information to compute the inference is present.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- This command is useful for getting the most recent network-wide inference for a topic when all necessary data has been collected.

**Example Scenario:**  
- You want to retrieve the latest ETH price prediction, but only if all the data from workers and forecasters has been collected.

---

### Get Topic Reward Nonce

- **RPC Method:** `GetTopicRewardNonce`
- **Command:** `topic-reward-nonce [topic_id]`
- **Description:** Retrieves the reward nonce used to calculate rewards for a specific topic.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to understand the reward cycle for a particular topic, as it provides the nonce used to calculate rewards.

**Example Scenario:**  
- You want to check the reward nonce for a topic before submitting contributions.

---

### Get Topic Fee Revenue

- **RPC Method:** `GetTopicFeeRevenue`
- **Command:** `topic-fee-revenue [topic_id]`
- **Description:** Retrieves the effective fee revenue for a topic, which represents the total fees collected by the topic less an exponential decay of the fees over time.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- This command provides insights into the fee revenue for a topic and how that impacts its overall weight and performance.

**Example Scenario:**  
- You want to check the total fee revenue generated by a topic before adjusting its parameters or interacting further.

---

### Get Previous Topic Weight

- **RPC Method:** `GetPreviousTopicWeight`
- **Command:** `previous-topic-weight [topic_id]`
- **Description:** Retrieves the previous weight of a topic, which can be used to estimate future or past topic performance.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.

#### Use Case:
**Why use it?**  
- Use this command to analyze the historical weight of a topic, which can help predict its future influence.

**Example Scenario:**  
- You want to assess the past performance of a topic before participating in it again.

---

### Get Active Topics at Block

- **RPC Method:** `GetActiveTopicsAtBlock`
- **Command:** `active-topics-at-block [block_height]`
- **Description:** Retrieves all active topics at a specific block height.
- **Positional Arguments:**
    - `block_height`: The block height at which to retrieve the active topics.

#### Use Case:
**Why use it?**  
- Use this command to identify all topics that are active at a given block.

**Example Scenario:**  
- You want to see which topics were active during a specific block height to compare performance or contributions.

---

### Get Topic Inferences at Block

- **RPC Method:** `GetInferencesAtBlock`
- **Command:** `inferences-at-block [topic_id] [block_height]`
- **Description:** Retrieves all inferences produced for a topic at a given block height.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `block_height`: The block height for which to retrieve the inferences.

#### Use Case:
**Why use it?**  
- Use this command to get all inferences made for a topic at a specific block height.

**Example Scenario:**  
- You want to analyze the inferences produced at a specific block for performance review or reward calculation.

---

### Get Topic Forecast Scores Until Block

- **RPC Method:** `GetForecastScoresUntilBlock`
- **Command:** `forecast-scores-until-block [topic_id] [block_height]`
- **Description:** Retrieves all forecast scores for a topic until a specific block height, limited by `MaxSamplesToScaleScores`.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `block_height`: The block height for which to retrieve the forecast scores.

#### Use Case:
**Why use it?**  
- Use this command to track forecaster performance over time in a topic by looking at forecast scores until a specific block height.

**Example Scenario:**  
- You want to evaluate the forecast scores for a topic until a particular block to assess forecaster accuracy.

---

### Get Reputer Scores at Block

- **RPC Method:** `GetReputersScoresAtBlock`
- **Command:** `reputer-scores-at-block [topic_id] [block_height]`
- **Description:** Retrieves all reputer scores for a topic at a specific block height.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic.
    - `block_height`: The block height for which to retrieve the reputer scores.

#### Use Case:
**Why use it?**  
- Use this command to evaluate how reputers performed at a specific block height.

**Example Scenario:**  
- You want to analyze reputer performance at a particular block to understand how their contributions impacted the topic.

---




================================================
FILE: pages/devs/topic-creators/topic-life-cycle.mdx
================================================
# Topic Life Cycle

The Topic Life Cycle in the Allora Network is a dynamic process that determines the stages a topic goes through from creation to conclusion. These stages are influenced by various factors such as funding, popularity, and performance metrics. Understanding the life cycle of a topic is crucial for engaging with the network.

## Key Terms and Concepts

### Epoch Length

How often inferences are sampled and scored in the topic. Defined when [creating a topic](/devs/topic-creators/how-to-create-topic) as `EpochLength`.

### Epoch Last Ended

The timestamp indicating when the last epoch ended, important for tracking topic activity.

### Ground Truth Lag

The amount of time into the future a specific inference is calculating for. Defined when [creating a topic](/devs/topic-creators/how-to-create-topic) as `GroundTruthLag`.

E.g. "Every 15 minutes, provide BTC prediction for 1 day in the future":
- 10 min - EpochLength
- 1 day - GroundTruthLag

### Nonce

The block height at which a given outbound request from network validators is made. Nonces ensure that responses are correctly paired with their requests to facilitate accurate reward distribution and loss calculation.

Every topic will inevitably generate multiple worker and reputer requests, each needing to be matched with rewards for participants. The blockchain must differentiate between responses still pending rewards and those already rewarded, and reputers must identify which worker payloads to use for loss calculations. This requires uniquely identifying each outbound request.

The same nonce value will be used to fulfill a complete work and reputation cycle:

1. A request for inferences and forecasts using a particular nonce is issued first.
2. Once the workers have submitted their work, the worker nonce is fulfilled and a reputer nonce is created using the same value.
3. This reputer nonce will be processed when appropriate, triggering a reputation request.
4. When the reputers respond by submitting their work, the reputer nonce is also fulfilled, ending its cycle.

### Topic Competitiveness

Competitiveness in the Allora Network refers to a topic's ability to attract and retain funding, stakes, and participation relative to other topics. A competitive topic has the following characteristics:

- **High Effective Revenue**: A greater accumulation of revenue indicates strong interest.
- **Significant Stake**: Large amounts of reputer and delegated stakes signify confidence in the topic's value.

Both of these metrics are a function of [weight](/devs/topic-creators/topic-life-cycle#weight), which proxies overall participation and ultimately topic competitiveness.

### Effective Revenue

Effective Revenue is the measure of the impact that the total accrued revenue has on a topic's weight. It determines how much influence the revenue has on making a topic active and competitive.

- Initially, Effective Revenue equals the total amount of money a topic accrues before the first epoch.
- Once a topic becomes active, funds from Effective Revenue are used, impacting the ecosystem bucket.

The Effective Revenue drips over time, reflecting the topic's diminishing competitiveness relative to other topics.

### Ecosystem Bucket

The Ecosystem Bucket is a mechanism that distributes a portion of the total funds at a rate (approximately 10%) that decreases exponentially over time. This bucket serves as a comparative baseline for topic competitiveness. The effective revenue of a topic needs to be balanced with the ecosystem bucket to ensure the topic's competitiveness.

- The bucket holds the money and drips at a certain rate.
- This rate is uncoupled from the effective revenue drip to avoid complex calculations to determine how much effective revenue the topic actually has remaining.
- It provides an estimation but doesn’t have a bearing on the total amount of money dripped from the ecosystem, ensuring financial safety.

### Weight

Weight is a measure of a topic's competitiveness within the blockchain network. It is a function of the combined stake of reputers (including delegated stakes) and the topic's Effective Revenue. The weight of a topic determines its likelihood of becoming active and indirectly influences the distribution of rewards and resources within the network.

- Higher weight signifies greater competitiveness.
- Driven by the total stake and the impact of effective revenue.

## Topic States

### Inactive

A topic is inactive after it is created but before it becomes sufficiently funded.

### Active

A topic becomes active once it is sufficiently funded. A topic is sufficiently funded once it has more than a threshold amount of weight, which is a function of the amount of:
- [Reputer stake](/devs/reputers/set-and-adjust-stake) placed in the topic
- Delegated stake
- Effective revenue garnered by the topic

Different actors can permissionlessly [fund a topic](/devs/reference/allorad#send-funds-to-a-topic-to-pay-for-inferences) using the `allorad` CLI tool.

### Churnable

A topic becomes churnable once it is:
- Active
- One of the top topics by weight (descending)
- The topic's `EpochLength` has passed since its inception or last epoch

Once a topic is churnable, the chain can emit worker (and eventually reputer) requests to topic workers and reputers, respectively.

Reputer requests start after a topic's `GroundTruthLag` amount of blocks have passed. Once worker and reputer responses are fulfilled, the topic becomes _churned_.

### Rewardable

A topic is rewardable once:
- It has been churned
- It has fulfilled worker and reputer requests
- It is ready to have its rewards calculated



================================================
FILE: pages/devs/validators/_meta.json
================================================
{
  "nop-requirements": "System Requirements",
  "deploy-chain": "Deploy Allora Chain",
  "run-full-node": "Run a Full Node",
  "stake-a-validator": "Stake a Validator",
  "validator-operations": "Validator Operations",
  "software-upgrades": "Software Upgrades"
}



================================================
FILE: pages/devs/validators/deploy-chain.mdx
================================================
# Deploy Allora Appchain

> We discuss the settlement layer for the Allora Network and how to deploy it

## What is the Appchain?

The Allora Appchain is a Cosmos SDK appchain that serves as the settlement layer for the Allora Network. It serves to coordinate all incentives for all actors:

- The weights between reputers and workers, as well as a reference to the logic used to update those weights, are stored on-chain.
- Rewards payable from inflation are calculated based on those weights at a global cadence on-chain.
- Consumers pay for inferences to be collected and for all the above calculations to run. These funds get allocated to workers and reputers, respectively.

The appchain also coordinates actions between protocol actors.

- The appchain triggers requests to workers and reputers to collect inferences and run loss-calculation logic, respectively, as per each topic's respective inference and loss-calculation cadence.
- The appchain collects a recent history of inferences in batches to later be scored by loss-calculation.

## Why and How might one interact with the Allora Appchain?

Different actors interact with the Allora Appchain for different reasons. They do so via a standard client connection (such as [CosmJS](https://tutorials.cosmos.network/tutorials/7-cosmjs/1-cosmjs-intro.html)) or the [Appchain CLI](/devs/get-started/cli#installing-allorad).

- Data scientists interact with the Appchain to [register their worker nodes](/devs/reference/allorad#register-network-actor) and to [withdraw rewards](/devs/reference/allorad#remove-stake-from-a-topic) accrued for their inferences. These rewards are paid by both consumers and inflation based on their relative weight.
- Developers interact with the Appchain to [create topics](/devs/reference/allorad#create-new-topic), fund topics, and perhaps also to [read recent inferences](/devs/reference/allorad#get-the-latest-network-inferences-and-weights-for-a-topic).
- Validators run the Appchain and receive standard inflationary rewards for running Cosmos SDK appchains and a cut of the funds from consumers. They will also [register themselves](/devs/validators/stake-a-validator) on the Appchain so that they can be eligible for rewards.

## Dependencies

- Create a set of keys and initialize genesis. See example in `scripts/init.sh`. 
- The script `scripts/l1_node.sh` is provided too, to facilitate configuration and maintenance of the node when connecting it to a network, downloading genesis, 

## Deploy with docker-compose

There is a `docker-compose.yml` provided that sets up a validator node.

### Run

Once this is set up, run `docker compose up`.

## Deploy in k8s with helm chart

Upshot team uses a [universal-helm](https://upshot-tech.github.io/helm-charts/) chart to deploy applications into kubernetes clusters.  
There is a `index-provider/values.yaml` provided that sets up one head node and one worker node.

### Dependencies

- You need to have configured `kubeconfig` file on the computer to connect to the cluster and deploy the node.

### Deploy with the Helm Chart

1. Add upshot Helm chart repo:

```bash
helm repo add upshot https://upshot-tech.github.io/helm-charts

```

2. Install helm chart with the given values file:

```bash
helm install \
  index-provider \
  upshot/universal-helm \
  -f appchain/values.yaml
```

### Edit Chain Parameters

The public mainnet uses standard cosmos governance modules to vote on global network parameters (such as reward epoch time in blocks, for example). For testnets and devnets, however, you can use the following allorad CLI command to set the global parameters of the blockchain if you are whitelisted to do so. The parameters below are just example values:

```Text bash
allorad tx emissions update-params  "$VALIDATOR_KEY_FOR_TX_SEND" '{"version":["v0.0.4"], "min_topic_weight":["5"], "max_topics_per_block":[50]}'
```



================================================
FILE: pages/devs/validators/nop-requirements.mdx
================================================
# System Requirements

You can use any modern Linux distribution to run an Allora validator.  
Internally we use **Debian 12** x86_64.

## MAINNET and TESTNET validators' requirements:

- CPU: ≥6cores, ≥12 threads
- Memory: ≥64GB
- Disk: SSD or NVMe ≥1.92 TB total
- Bandwidth: ≥1Gbit/s guaranteed

## Note

Participating as a validator is temporarily allowed only for whitelisted accounts. The Upshot Team currently has access to whitelisted addresses. We plan to make this action permissionless soon. In the meantime, those interested in becoming validators should reach out [here](https://docs.google.com/forms/d/e/1FAIpQLScj2rGAjFAAPZANrr2vZr_WAmLhniHn2x_l8K7EQcJ1i8XqHw/viewform).

## Responsibilities

Validators are responsible for operating most of the infrastructure associated with instantiating the Allora Network. They do this in three ways:

1. Staking in worker nodes (data scientists) based on their confidence in said workers' abilities to produce accurate inferences.
2. Operating the appchain as Cosmos validators.

## Executing Loss-Calculation Logic Off-Chain

The topic-specific logic ran by validators is compiled to WASM and stored on IPFS. Our appchain calls upon validators to execute this logic in every `topic.loss_cadence`-length epoch. Running this WASM involves querying for the following values:

- Current set of losses between reputers and workers
- Inferences from the past epoch
- The revealed ground truth consisting of up to `topic.inference_cadence/topic.loss_cadence`-many values. In other words, it entails one ground truth value for each inference cadence within the preceding loss-calculation epoch.

The WASM also involves committing the new losses to the appchain in a transaction. This is ultimately done by one b7s node.

Computing loss-calculation logic off-chain saves the network validators operating expenses (because less is run on-chain), allows topic creators to write loss-calculation logic in any language (that compiles to WASM), and lessens the need for frequent node software upgrades (because the module source code remains unchanged even as new topics are added, each with their specific loss-calculation schemes).



================================================
FILE: pages/devs/validators/run-full-node.mdx
================================================
import { Callout } from 'nextra/components'

# Running a full node

> How to become a Validator on Allora

This guide provides instructions on how to run a full node for the Allora network. There are two primary methods for running an Allora node: using systemd with cosmosvisor for easier upgrade management (recommended) or using docker compose. It's important to choose the method that best suits your environment and needs.

***

## Prerequisites

- Git
- Go (version 1.21 or later)
- Basic command-line knowledge
- Linux/Unix environment with systemd
- curl and jq utilities

***

## Method 1: Using systemd with cosmosvisor (Recommended)

Running the Allora node with systemd and cosmosvisor provides production-grade reliability and easier binary upgrade management. This is the recommended approach for validators and production environments.

### Step 1: Install cosmosvisor

First, install cosmosvisor, which will manage binary upgrades:

```shell
go install cosmossdk.io/tools/cosmovisor/cmd/cosmovisor@latest
```

Verify the installation:

```shell
cosmovisor version
```

### Step 2: Install allorad Binary

Download the latest `allorad` binary from the releases page:

1. Navigate to the [Allora Chain Releases page](https://github.com/allora-network/allora-chain/releases/latest).
2. Download the `allorad` binary appropriate for your operating system (e.g., `allorad-linux-amd64`, `allorad-darwin-amd64`).
3. Rename and move the binary to a standard location:

```shell
# Rename the downloaded binary
mv ./allorad-linux-amd64 ./allorad  # Adjust filename as needed

# Move to system path
sudo mv ./allorad /usr/local/bin/allorad

# Make executable
sudo chmod +x /usr/local/bin/allorad
```

### Step 3: Initialize the Node

Initialize your node (replace `<your-moniker>` with your desired node name):

```shell
allorad init <your-moniker> --chain-id allora-testnet-1
```

### Step 4: Download Network Configuration

Download the testnet configuration files:

```shell
# Download genesis.json
curl -s https://raw.githubusercontent.com/allora-network/networks/main/allora-testnet-1/genesis.json > $HOME/.allorad/config/genesis.json

# Download config.toml
curl -s https://raw.githubusercontent.com/allora-network/networks/main/allora-testnet-1/config.toml > $HOME/.allorad/config/config.toml

# Download app.toml
curl -s https://raw.githubusercontent.com/allora-network/networks/main/allora-testnet-1/app.toml > $HOME/.allorad/config/app.toml
```

### Step 5: Configure Seeds and Peers

Configure seeds and persistent peers for network connectivity:

```shell
# Fetch and set seeds
SEEDS=$(curl -s https://raw.githubusercontent.com/allora-network/networks/main/allora-testnet-1/seeds.txt)
sed -i.bak -e "s/^seeds *=.*/seeds = \"$SEEDS\"/" $HOME/.allorad/config/config.toml

# Optionally set persistent peers
PEERS=$(curl -s https://raw.githubusercontent.com/allora-network/networks/main/allora-testnet-1/peers.txt)
sed -i.bak -e "s/^persistent_peers *=.*/persistent_peers = \"$PEERS\"/" $HOME/.allorad/config/config.toml
```

### Step 6: Configure cosmosvisor

Set up the cosmosvisor directory structure and environment:

```shell
# Set environment variables
export DAEMON_NAME=allorad
export DAEMON_HOME=$HOME/.allorad
export DAEMON_RESTART_AFTER_UPGRADE=true

# Create cosmosvisor directories
mkdir -p $DAEMON_HOME/cosmovisor/genesis/bin
mkdir -p $DAEMON_HOME/cosmovisor/upgrades

# Copy the current binary to genesis
cp /usr/local/bin/allorad $DAEMON_HOME/cosmovisor/genesis/bin/
```

### Step 7: Configure State Sync (Optional but Recommended)

State sync allows your node to quickly catch up with the network. Create and run this state sync script:

```shell
cat > state_sync.sh << 'EOF'
#!/bin/bash

set -e

# Choose your preferred RPC endpoint
SNAP_RPC="https://allora-rpc.testnet.allora.network"
CONFIG_TOML_PATH="$HOME/.allorad/config/config.toml"

echo "Using RPC Endpoint: $SNAP_RPC"
echo "Fetching latest block height..."

LATEST_HEIGHT=$(curl -s $SNAP_RPC/block | jq -r .result.block.header.height)
if [ -z "$LATEST_HEIGHT" ] || [ "$LATEST_HEIGHT" == "null" ]; then
    echo "Error: Could not fetch latest height"
    exit 1
fi

BLOCK_HEIGHT_OFFSET=2000
BLOCK_HEIGHT=$((LATEST_HEIGHT - BLOCK_HEIGHT_OFFSET))

echo "Fetching trust hash for block $BLOCK_HEIGHT..."
TRUST_HASH=$(curl -s "$SNAP_RPC/block?height=$BLOCK_HEIGHT" | jq -r .result.block_id.hash)
if [ -z "$TRUST_HASH" ] || [ "$TRUST_HASH" == "null" ]; then
    echo "Error: Could not fetch trust hash"
    exit 1
fi

echo "Updating config for state sync..."
RPC_SERVERS="$SNAP_RPC,$SNAP_RPC"

sed -i.bak -E \
    -e "s|^(enable[[:space:]]*=[[:space:]]*).*$|\\1true|" \
    -e "s|^(rpc_servers[[:space:]]*=[[:space:]]*).*$|\\1\"$RPC_SERVERS\"|" \
    -e "s|^(trust_height[[:space:]]*=[[:space:]]*).*$|\\1$BLOCK_HEIGHT|" \
    -e "s|^(trust_hash[[:space:]]*=[[:space:]]*).*$|\\1\"$TRUST_HASH\"|" \
    "$CONFIG_TOML_PATH"

echo "State sync configuration updated successfully"
EOF

chmod +x state_sync.sh
./state_sync.sh
```

### Step 8: Reset Node Data

Reset existing data while keeping the address book:

```shell
allorad tendermint unsafe-reset-all --home $HOME/.allorad --keep-addr-book
```

<Callout type="warning">
**Warning**: This command deletes blockchain data. Only run this on a fresh node or when you intend to resync from scratch.
</Callout>

### Step 9: Create systemd Service

Create a systemd service file for cosmosvisor:

```shell
sudo tee /etc/systemd/system/allorad.service > /dev/null <<EOF
[Unit]
Description=Allora Node (allorad via Cosmovisor)
After=network-online.target

[Service]
User=$USER
ExecStart=$(which cosmovisor) run start
Restart=always
RestartSec=3
LimitNOFILE=65535
Environment="DAEMON_HOME=$HOME/.allorad"
Environment="DAEMON_NAME=allorad"
Environment="DAEMON_ALLOW_DOWNLOAD_BINARIES=false"
Environment="DAEMON_RESTART_AFTER_UPGRADE=true"
Environment="DAEMON_POLL_INTERVAL=300ms"
Environment="UNSAFE_SKIP_BACKUP=true"

[Install]
WantedBy=multi-user.target
EOF
```

<Callout type="info">
**Security Note**: `DAEMON_ALLOW_DOWNLOAD_BINARIES` is set to `false` for security. Validators should manually place upgrade binaries in the appropriate directories.
</Callout>

### Step 10: Start the Service

Enable and start the systemd service:

```shell
sudo systemctl daemon-reload
sudo systemctl enable allorad
sudo systemctl start allorad
```

### Monitoring and Management

Monitor your node logs:

```shell
sudo journalctl -u allorad -f
```

Check service status:

```shell
sudo systemctl status allorad
```

Check sync status:

```shell
curl -s http://localhost:26657/status | jq .result.sync_info.catching_up
```

Once this returns `false`, your node is fully synced.

### Managing Upgrades with cosmosvisor

When a governance upgrade is approved, prepare for it by placing the new binary:

```shell
# For an upgrade named "v1.0.0", create the upgrade directory
mkdir -p $DAEMON_HOME/cosmovisor/upgrades/v1.0.0/bin

# Download and place the new binary (replace with actual URL)
# wget NEW_BINARY_URL -O $DAEMON_HOME/cosmovisor/upgrades/v1.0.0/bin/allorad
# chmod +x $DAEMON_HOME/cosmovisor/upgrades/v1.0.0/bin/allorad
```

<Callout type="info">
**Info**: cosmosvisor will automatically switch to the new binary at the upgrade height specified in the governance proposal. Monitor governance proposals and prepare upgrade binaries in advance.
</Callout>

***

## Method 2: Using `docker compose`

Running the Allora node with `docker compose` simplifies the setup and ensures consistency across different environments, but requires manual upgrade management.

### Step 1: Clone the Allora Chain Repository

If you haven't already, clone the latest release of the [allora-chain repository](https://github.com/allora-network/allora-chain):

```shell
git clone https://github.com/allora-network/allora-chain.git
```

### Step 2: Run the Node with Docker Compose

Navigate to the root directory of the cloned repository and start the node using `docker compose`:

```shell
cd allora-chain
docker compose pull
docker compose up
```

> run `docker compose up -d` to run the container in detached mode, allowing it to run in the background.

<Callout type="info">
**Info**: Don't forget to pull the images first, to ensure that you're using the latest images. 
</Callout>

<Callout type="warning">
Make sure that any previous containers you launched are killed, before launching a new container that uses the same port.

You can run the following command to kill any containers running on the same port:
```bash
docker container ls
docker rm -f <container-name>
```
</Callout> 

#### Run Only a Node with Docker Compose
In this case, you will use Allora's heads.
##### Run

```
docker compose pull
docker compose up node
```
To run only a head: `docker compose up head`

<Callout type="info">
**NOTE:** You also can comment the head service in the Dockerfile.
</Callout>

### Monitoring Logs

To view the node's logs, use the following command:

```shell
docker compose logs -f
```

### Executing RPC Calls

You can interact with the running node through RPC calls. For example, to check the node's status:

```shell
curl -s http://localhost:26657/status | jq .
```

This command uses `curl` to send a request to the node's RPC interface and `jq` to format the JSON response. 

Once your node has finished syncing and is caught up with the network, this command will return `false`:

```shell
curl -so- http\://localhost:26657/status | jq .result.sync_info.catching_up
```

<Callout type="info">
**Info**: The time required to sync depends on the chain's size and height. 

    - For newly launched chains, syncing will take **minutes**.
    - Established chains like Ethereum can take around **a day** to sync using Nethermind or similar clients.
    - Some chains may take **several days** to sync.
    - Syncing an archival node will take significantly more time.
</Callout> 

<Callout type="warning">
**Warning**: Network participants will not be able to connect to your node until it is finished syncing and the command above returns `false`.
</Callout> 

### Syncing from Snapshot

Users can also opt to sync their nodes from our [latest snapshot script](https://github.com/allora-network/allora-chain/blob/main/scripts/restore_snapshot.sh) following the instructions below:

1. Install [`rclone`](https://rclone.org/), a command-line program to manage files on cloud storage

```bash
brew install rclone
```

2. Follow the instructions to configure `rclone` after running `rclone config` in the command line

3. Uncomment the [following lines](https://github.com/allora-network/allora-chain/blob/ccad6d27e55b27a7ec3b2aebd7e55f1bc26798ed/scripts/l1_node.sh#L15) from your Allora Chain repository:

```go
# uncomment this block if you want to restore from a snapshot
# SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
# "${SCRIPT_DIR}/restore_snapshot.sh"
```

4. Run the node using Docker:

```bash
docker compose pull
docker compose up -d
```


================================================
FILE: pages/devs/validators/software-upgrades.mdx
================================================
# Software Upgrades

> How to upgrade the Allora software version during hard forks.

The Allora network relies on multiple different pieces of software to do different tasks.
For example the `allora-chain` repository handles the blockchain software that runs the chain, while
the `offchain-node` repository performs off-chain tasks. Each piece of software may need to
be upgraded separately.

## Allora-Chain Upgrades

The `allora-chain` software is a cosmos-sdk based blockchain that runs the Allora network. New 
software releases are published on the 
Allora Chain [Github](https://github.com/allora-network/allora-chain/releases) page and are tagged
with a version number. Upgrading to non-breaking versions is as simple as downloading the
pre-built binaries or compiling the software from source and running the new version.

### Upgrading to a Breaking Version

For breaking versions such as hard forks, or software upgrades requiring changes to the underlying
state machine of the allora-chain, the upgrade process is more involved. These upgrades require 
using the `gov` and `upgrade` cosmos-sdk modules to first propose and vote on a software upgrade,
and then to execute the upgrade at a specific block height.

#### For Allora Chain Developers

For writing an upgrade the steps are roughly the following:

1. In the `app/` [folder](https://github.com/allora-network/allora-chain/tree/main/app/upgrades), 
create a new folder for your upgrade. 
2. In that folder create a file that contains an `UpgradeName`, and a function `CreateUpgradeHandler` which 
returns a `"cosmossdk.io/x/upgrade/types".UpgradeHandler`.Optionally include a `UpgradeInfo` that is a json string telling the client software
where to download the upgrade binary version e.g.

```golang
const UpgradeInfo = `'{"binaries":{"linux/amd64":"https://github.com/allora-network/allora-chain/releases/download/v9.9.9/allorad_amd64.tar.gz"}}'`
```

3. Wire up the new upgrade handler to the chain by editing the `setupUpgradeHandlers` function in [app/upgrades.go](https://github.com/allora-network/allora-chain/blob/main/app/upgrades.go). You can see a reference for how to do this in the upgrade integration test [here](https://github.com/allora-network/allora-chain/blob/main/test/integration/upgrade.patch)
4. If you're upgrading standard cosmos-sdk module versions you may have to tweak the `module.VersionMap` that the `CreateUpgradeHandler` returns/processes.
5. If you're upgrading one of the Allora forked/created modules, you'll need to bump the `ConsensusVersion` for the module.
6. In the module, have the `module.Configurator` do a `cfg.RegisterMigration` with the module name, the previous consensus 
version that is being upgraded from, and the function to run to do the migration as a parameter.
7. Write a function that process the kv store or does whatever other migrations are necessary. Examples [here](https://github.com/allora-network/allora-chain/blob/main/x/emissions/migrations/v2/migrations.go) and [here](https://github.com/evmos/evmos/blob/main/x/evm/migrations/v7/migrate.go).
8. Merge the PR, tag it appropriately and post it to the releases page.
9. Create a Software Upgrade Proposal for validators to vote on. You can see a reference where this is done in the [proposeUpgrade](https://github.com/allora-network/allora-chain/blob/main/test/integration/upgrade_test.go) function in the integration tests.
10. Convince all the validators to vote yes on the Software Upgrade Proposal, and run cosmovisor so that the upgrade will actually go through at the proposed block.

#### For Allora Chain Validator Operators

For those running the chain software, you will have to have to perform an upgrade as follows:

1. Make sure you're running the `allorad` software with [Cosmovisor](https://docs.cosmos.network/main/build/tooling/cosmovisor)) managing the process, `DAEMON_NAME=allorad` and `DAEMON_HOME=/path/to/allorad/data/folder`. Hopefully you've already run `cosmovisor init /path/to/allorad-binary` and have the `/allorad/data/folder/cosmovisor` set.
2. At some point the blockchain developers will provide you with a binary to put in that  `/allorad/data/folder/cosmovisor` folder to upgrade to. This may be optional if the `UpgradeInfo` is set correctly by the developers, but if you're the paranoid type you can always download the binary yourself ahead of the upgrade and put it in the right folder by hand.
3. When the developers put up the upgrade proposal to governance, be helpful and vote to make it pass. You can do this via the CLI with `allorad tx gov vote $proposal_id yes --from $validator` or an example of doing this programmatically can be found in the integration test [voteOnProposal](https://github.com/allora-network/allora-chain/blob/main/test/integration/upgrade_test.go) function.
4. At the block height of the upgrade, the old software will panic - cosmovisor will catch the panic and restart the process using the new binary for the upgrade instead. Monitor your logs appropriately to see the restart.

## Further References

This is probably the most helpful document to understand the full workflow of a cosmos-sdk chain
upgrade: [Medium Blog Post Cosmos Dev Series: Cosmos Blockchain Upgrade](https://medium.com/web3-surfers/cosmos-dev-series-cosmos-sdk-based-blockchain-upgrade-b5e99181554c)

Cosmos SDK Upgrade Module: [Documentation](https://docs.cosmos.network/main/build/modules/upgrade)

Cosmovisor Process Manager Software [Documentation](https://docs.cosmos.network/main/build/tooling/cosmovisor)

Cosmos SDK Gov Module: [Documentation](https://docs.cosmos.network/main/build/modules/gov)


================================================
FILE: pages/devs/validators/stake-a-validator.mdx
================================================
# Stake a Validator

Follow these steps to stake on a node in the Allora network. This process includes running and syncing a full node, funding your account, and setting up your Validator for staking.

## Prerequisites

- Successfully run and synced a full `allorad` node. Refer to [Running a Full Node](./run-full-node) for detailed instructions.
- Basic command-line and Docker knowledge.
- Access to the node's terminal or command line.

## 1\. Verify Node Sync

Ensure your node is fully synced with the network by executing the following command:

```shell
curl -s http://localhost:26657/status | jq .result.sync_info.catching_up
```

Wait until the output returns `false`, indicating your node has caught up with the network.

## 2\. Fund Your Account

After initializing your node, `scripts/l1_node.sh` generates key and account information, found in `data/*.account_info`. Locate your account address within this file to fund it. 

```shell
cat data/validator0.account_info

- address: allo1xxxxx
  name: validator0
  pubkey: xxx
  type: local
[...]
```

For testnet environments, use the appropriate [faucet](/devs/get-started/setup-wallet#add-faucet-funds).

## 3\. Stake as a Validator

To become a validator, perform the following inside the validator's Docker container environment. You can choose your validator's name by setting a custom moniker (with `--moniker=...`). We will take the example of `validator0` with `10000000 uallo`. 

### Access the Validator's Shell

Use `docker compose` to access the validator's shell environment:

```shell
docker compose exec validator0 bash
```

**Note**: You can list all available keys with: 

```shell
allorad --home=$APP_HOME keys --keyring-backend=test list
```

### Prepare Stake Information

Within the validator's shell, create a JSON file named `stake-validator.json` with your validator's stake information. Replace values with your actual data:

```shell
cat > stake-validator.json << EOF
{
    "pubkey": $(allorad --home=$APP_HOME comet show-validator),
    "amount": "1000000uallo",
    "moniker": "$(echo $MONIKER)",
    "commission-rate": "0.1",
    "commission-max-rate": "0.2",
    "commission-max-change-rate": "0.01",
    "min-self-delegation": "1"
}
EOF
```

### Execute the Stake Command

With your stake information file ready, execute the following command to stake as a Validator:

```shell
allorad tx staking create-validator ./stake-validator.json \
    --chain-id=allora-testnet-1 \
    --home="$APP_HOME" \
    --keyring-backend=test \
    --from="$MONIKER"
```

This command outputs a transaction hash, which can be checked on the network's explorer: `https://explorer.testnet.allora.network/allora-testnet-1/tx/$TX_HASH`.

## 4\. Verify Validator Setup

Ensure your validator is properly registered and staked with the network by executing the following commands:

### Check Registration and Stake

Retrieve and verify your validator's information by running these 2 commands:

```shell
VAL_PUBKEY=$(allorad --home=$APP_HOME comet show-validator | jq -r .key)
```

```shell
allorad --home=$APP_HOME q staking validators -o=json | \
    jq '.validators[] | select(.consensus_pubkey.value=="'$VAL_PUBKEY'")'
```

This command outputs detailed information about your validator. If it's correctly set up, it will look like this:

```json
{
  "operator_address": "allovaloper1n8t4ffvwstysveuf3ccx9jqf3c6y7kte48qcxm",
  "consensus_pubkey": {
    "type": "tendermint/PubKeyEd25519",
    "value": "gOl6fwPc19BtkmiOGjjharfe6eyniaxdkfyqiko3/cQ="
  },
  "status": 3,
  "tokens": "1000000",
  "delegator_shares": "1000000000000000000000000",
  "description": {
    "moniker": "val2"
  },
  "unbonding_time": "1970-01-01T00:00:00Z",
  "commission": {
    "commission_rates": {
      "rate": "100000000000000000",
      "max_rate": "200000000000000000",
      "max_change_rate": "10000000000000000"
    },
    "update_time": "2024-02-26T22:50:31.187119394Z"
  },
  "min_self_delegation": "1"
}
```

### Check Voting Power

Verify that your Validator's voting power is greater than 0, indicating active participation in the Network:

```shell
allorad --home=$APP_HOME status | jq -r '.validator_info.voting_power'
```

**Note**: Please allow 30-60 seconds for the information to update. A voting power greater than 0 signifies a successful stake setup. Congratulations!



================================================
FILE: pages/devs/validators/validator-operations.mdx
================================================
# Validator Operations

## Unjailing a validator

To unjail a validator execute the following command from the validator

```Text bash
allorad --home="$APP_HOME" \
  tx slashing unjail --from $VALIDATOR_ADDRESS
```

## Unstaking/unbounding a validator

If you need to delete a validator from the chain, you just need to unbound the stake with your custom parameters:

```bash
allorad --home="$APP_HOME" \
  tx staking unbond ${VALIDATOR_OPERATOR_ADDRESS} \
  ${STAKE_AMOUNT}uallo --from "$MONIKER" \
   --keyring-backend=test --chain-id ${NETWORK}
```



================================================
FILE: pages/devs/workers/_meta.json
================================================
{

  "requirements": "System Requirements",
  "deploy-worker": "Build/Deploy an Inference Worker",
  "walkthroughs": "Walkthroughs",
  "deploy-forecaster": "Build and Deploy a Forecaster",
  "query-worker-data": "How To Query Worker Data using allorad",
  "query-ema-score": "Query EMA Score of a Worker using allorad"
}


================================================
FILE: pages/devs/workers/deploy-forecaster.mdx
================================================
# Build and Deploy a Forecaster

The Allora Forecaster is designed to run a model that predicts how accurate inferers are at arbitrary tasks.
Any forecaster can be augmented using proprietary data sources, which likely overlap with the data used by inference models.
A [boilerplate forecaster](https://github.com/allora-network/allora-forecaster) has been provided that has demonstrated ability for arbitrary topics.

## Forecaster Components Overview

| **Component**           | **Purpose**                                                                 | **Key Functions**                                                                                                 |
|-------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Data Indexing**        | Retrieves necessary data from the blockchain using the Postgres indexer.    | Utilizes the `extract` folder for querying data from Postgres and making it accessible to the forecaster.         |
| **Modeling**            | Core functionality for model selection and training.                        | Supports different machine learning algorithms like LightGBM and XGBoost.               |
| **Prediction Engine**    | Runs selected models on historical data to generate future predictions.     | Ingests time-series data and outputs forecast values based on the chosen model.                                   |
| **Model Plots**         | Visualizes model performance and forecast accuracy.                         | Generates plots such as Prediction vs Actual, Residuals, and Forecast Horizon for intuitive evaluation.           |
| **Performance Metrics**  | Measures the accuracy and effectiveness of model predictions.               | Key metrics include MAE (Mean Absolute Error), RMSE (Root Mean Squared Error), R2 Score, Mean Absolute Percentage Error, Median Absolute Percentage Error |             |
| **Scoring Mechanism**    | Assigns scores based on model performance compared to other participants.   | Determines which forecasts contribute to the Allora Network’s final consensus based on accuracy and uniqueness.    |

## Setup




================================================
FILE: pages/devs/workers/query-ema-score.mdx
================================================
import { Callout } from 'nextra/components'

# How to Query Worker EMA Scores

## What is an EMA Score?

The EMA score (Exponential Moving Average) reflects a worker's performance over time for a given topic, balancing recent and past achievements, and helps determine whether a participant stays in the **active set** (eligible for [rewards](/home/layers/consensus/workers)) or remains 
in the **passive set**. 

Read about our v0.3.0 release on [Merit-Based Sortitioning](/home/release-notes#v030) for a deeper dive on what makes up the active and passive set.

Active participants have their EMA score updated based on their current performance, which influences their ongoing 
eligibility for [rewards](/home/layers/consensus/workers). In contrast, inactive participants, who do not contribute during a given [epoch](/home/key-terms#epochs), receive an adjusted score using a 
"dummy" value, which determines whether they can re-enter the active set in future epochs and qualify for rewards. 

This process ensures fairness while allowing inactive participants the chance to rejoin the active set based on their historical performance.

## Query EMA Score for a Specific Worker

To query the EMA score for a specific worker (identified by the worker's allo address), run:

```bash
allorad q emissions inferer-score-ema [topic_id] [worker_address] --node https://allora-rpc.testnet.allora.network/
```

- Replace `[topic_id]` and `[worker_address]` with your specific details.

## Query the Lowest Worker in the Active Set's EMA Score 

To query the lowest EMA score for a worker in the Active Set, run:

```bash
allorad q emissions current-lowest-inferer-score [topic_id] --node https://allora-rpc.testnet.allora.network/
```

- Replace `[topic_id]` with your specific details.

<Callout>
To determine if your worker is in the active set and eligible for rewards, query your worker's EMA score and the lowest worker in the active set's EMA score and compare them. 
If your worker's EMA score for a specific topic is higher than the EMA score of the lowest worker in the active set, your worker is in the active set for that topic.
</Callout>


================================================
FILE: pages/devs/workers/query-worker-data.mdx
================================================
# How to Query Worker Data using `allorad`

Below is a list of commands to understand how to pull information about workers via [`allorad`](/devs/get-started/cli#installing-allorad):

## Prerequisites

- [`allorad` CLI](/devs/get-started/cli)
- A basic understanding of the Allora Network

## Query Functions

These functions read from the appchain only and do not write. Add the **Command** value into your query to retrieve the expected data.

```bash
allorad q emissions [Command] --node <RPC_URL>
```

## Check if Worker is Registered in a Topic

- **RPC Method:** `IsWorkerRegisteredInTopicId`
- **Command:** `is-worker-registered [topic_id] [address]`
- **Description:** Checks whether a worker is registered in a specific topic. It returns `true` if the worker is registered in the given topic, and `false` otherwise.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic where you want to check the worker’s registration status.
    - `address`: The address of the worker you want to check.

### Use Case:
**Why use it?**  
- This command is essential if you want to verify whether a worker is properly registered in a specific topic before submitting inferences or participating in a topic's operations.

**Example Scenario:**  
- Before deploying a worker to submit inferences on a particular topic, you can confirm that the worker is registered to that topic to ensure proper functionality and avoid errors.

---

## Get Worker Inferences Scores at Block

- **RPC Method:** `GetWorkerInferenceScoresAtBlock`
- **Command:** `inference-scores [topic_id] [block_height]`
- **Description:** Return scores for a worker at a block height.
    - Scores determine how [worker rewards](/home/layers/consensus/workers) are paid out.

- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned.
  - `block_height` Block height to query.

### Use Case
**Why use it?**  
- You may want to verify if a worker has received a high score at a specific block, particularly if you're troubleshooting worker rewards or performance discrepancies.
   
**Example Scenario:**  
- If you believe your worker's reward for a particular topic is inaccurate, use this command to view how it was scored at a specific block.

---

## Get Latest Worker Inference By Topic ID

- **RPC Method:** `GetWorkerLatestInferenceByTopicId`
- **Command:** `worker-latest-inference [topic_id] [worker_address]`
- **Description:** Gets the latest inference for a given worker and topic.

- **Positional Arguments:**
  - `topic_id` Identifier of the topic whose information will be returned
  - `worker_address` Given worker to query on

### Use Case
**Why use it?**
- This command is useful if you want to check whether a worker is actively submitting inferences for a topic and how recent those inferences are.

**Example Scenario:**
- A worker has missed rewards, and you want to verify if their latest inference was successfully submitted on time for a given topic.

---

## Get Worker Node Info

- **RPC Method:** `GetWorkerNodeInfo`
- **Command:** `worker-info [address]`
- **Description:** Get node info for a specified worker node.
  - Returns the **owner address** of the worker node.
  - Returns the **worker node address** being queried.

- **Positional Arguments:**
  - `address` The address of the worker node whose information will be retrieved.

### Use Case
**Why use it?**
- This command is helpful for checking the current status of a worker node, especially if you are managing multiple nodes and want to verify the ownership or troubleshoot node configuration.

**Example Scenario:**
- You want to ensure the node you’ve set up is operating under the correct owner and is correctly registered on the network.

---

## Get Naive Inferer Network Regret

- **RPC Method:** `GetNaiveInfererNetworkRegret`
- **Command:** `naive-inferer-network-regret [topic_id] [inferer]`
- **Description:** Returns the network regret associated with including an inferer's naive inference in a batch for a given topic. If no specific regret is calculated, the command defaults to the topic's `InitialRegret` value.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic for which the regret will be calculated.
    - `inferer`: The address of the inferer whose naive inference is being evaluated.

### Use Case:
**Why use it?**  
- Use this command to assess the [regret](/home/key-terms#regrets) associated with incorporating an inferer’s naive inference into a batch. Useful for analyzing how poorly an inference may perform within the context of the network’s aggregate inference for a topic.

**Example Scenario:**  
- If you want to understand how an inferer's baseline performance impacts the network outcome, this command helps quantify that penalty.

---

## Get One-Out Inferer-Inferer Network Regret

- **RPC Method:** `GetOneOutInfererInfererNetworkRegret`
- **Command:** `one-out-inferer-inferer-network-regret [topic_id] [one_out_inferer] [inferer]`
- **Description:** Returns the network regret when the implied outcome of the `one_out_inferer` is included in a batch alongside the `inferer`. If no specific regret value exists, it defaults to the topic’s `InitialRegret`.
- **Positional Arguments:**
    - `topic_id`: The identifier of the topic for which the regret will be calculated.
    - `one_out_inferer`: The address of the inferer whose implied inference is being evaluated.
    - `inferer`: The address of the inferer to compare against.

### Use Case:
**Why use it?**  
- This command is useful when comparing how two inferers impact the network when their inferences are processed together. It helps identify the potential penalty on network performance when adding a specific inferer to a batch.

**Example Scenario:**
- You might want to compare the impact of two inferers to see how their joint performance influences the overall network regret. This is particularly useful for optimizing inference strategies.




================================================
FILE: pages/devs/workers/requirements.mdx
================================================
# System Requirements

To participate in the Allora Network, ensure your system meets the following requirements:

**Operating System**: Any modern operating system including Windows, macOS, or Linux  
**CPU**: Minimum of 1/2 core.  
**Memory**: 2 to 4 GB.  
**Storage**: SSD or NVMe with at least 5GB of space.

## Technical Requirement

Certain technical tools and platforms are required to develop and deploy your predictive models as workers within the Allora Network.

### Development Environment

**Docker**: Essential for creating and managing containers.

### Production Environment

**Kubernetes**: A container orchestration system for automating software deployment, scaling, and management  
**Helm**: A package manager for Kubernetes. _We advise the use of the Upshot Universal Helm Chart for deployment_  
**Preferred Cloud Service**: Depending on your preference, you can choose a cloud environment where your Node  will be running



================================================
FILE: pages/devs/workers/deploy-worker/_meta.json
================================================
{
  "using-docker": "Using Docker",
  "build-and-deploy-worker-with-node-runners": "Using AWS Node Runners",
  "build-and-deploy-worker-with-alibaba-cloud": "Using Alibaba Cloud",
  "allora-mdk": "Using the Allora Model Development Kit"
}


================================================
FILE: pages/devs/workers/deploy-worker/allora-mdk.mdx
================================================
import { Callout } from 'nextra/components'

# Build and Deploy a Worker using the Allora Model Development Kit (MDK)

The Allora MDK is an open-source github repository that allows users to spin up an inference model for over 7,000 cryptocurrencies and stocks.
The MDK leverages the [Tiingo API](https://www.tiingo.com) as a data feed for these cryptocurrencies and stocks, although custom datasets can be 
integrated as well. 

Let's walk through the steps needed to download, train, and evaluate a given model on a custom dataset, and then deploy this trained model onto the 
network.

## Regression Techniques

Each of these regression techniques is implemented at a basic level and is available out of the box in the Model Development Kit (MDK). These models provide a foundation that you can build upon to create more advanced solutions.


| **Model**                     | **Description**                                                                                                                                                     |
|-------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **ARIMA**                      | Auto-Regressive Integrated Moving Average model used for time series forecasting by modeling the dependencies between data points.                                   |
| **LSTM**                       | Long Short-Term Memory neural network, a type of recurrent neural network (RNN) that excels in capturing long-term dependencies in sequential data, like time series. |
| **Prophet**                    | A forecasting model developed by Facebook, designed to handle seasonality and make predictions over long time horizons.                                              |
| **Random Forest**              | An ensemble learning method for regression tasks that builds multiple decision trees and outputs the average prediction from individual trees.                       |
| **Random Forest (Time Series)**| A time series variant of Random Forest, optimized for predicting time-dependent variables.                                                                            |
| **Regression**                 | A simple linear regression model for predicting continuous values based on input features.                                                                           |
| **Regression (Time Series)**   | A time series version of basic regression models, optimized for forecasting trends over time.                                                                        |
| **XGBoost**                    | Extreme Gradient Boosting, a highly efficient and scalable implementation of gradient boosting machines for regression tasks, often used for time series forecasting. |
| **XGBoost (Time Series)**      | A time series-specific adaptation of XGBoost, tuned for forecasting with sequential data.                                                                            |

<Callout type="info">
Although these models are already integrated into the MDK, you can add more models as well as modify existing ones to create a better inference model tailored to your specific needs.
</Callout>

## Installation

### Clone the MDK Repository

Run the following commands in a new terminal window:

```bash
git clone https://github.com/allora-network/allora-mdk.git
cd allora-mdk
```

#### Conda not Installed?
<Callout type="warning"> 
On Mac, simply use brew to install Miniconda:

```bash
brew install miniconda
```
</Callout>

### Create Conda Environment

```bash
conda env create -f environment.yml
```

<Callout> 
If you want to set it up manually:
```bash
conda create --name modelmaker python=3.9 && conda activate modelmaker
pip install setuptools==72.1.0 Cython==3.0.11 numpy==1.24.3
```
</Callout>

### Install Dependencies

```bash
pip install -r requirements.txt
```

### Add Tiingo API Key

Go to [tiingo.com](https://www.tiingo.com) and set up an API Key after creating an account, which you will input into your `.env` file:

```
# .env
TIINGO_API_KEY=your_tiingo_api_key
```

## Usage

### Model Training

```bash
make train
```

Running the above command will guide you through a series of sub-prompts that you can use to curate a unique training set for the given cryptocurrency or stock you choose as a target variable.

#### Select the Data Source

After running `make train`, the command line will prompt you to select your dataset:

```bash
Select the data source:
1. Tiingo Stock Data
2. Tiingo Crypto Data
3. Load data from CSV file
Enter your choice (1/2/3): 
```

- Although the MDK is natively integrated with Tiingo, a model maker can effectively configure any data set to train on from a CSV file as well.

#### Select the Target Variable

After selecting your data source, you will be prompted to pick a target variable for your model to provide inferences on.

```bash
Enter the crypto symbol (default: btcusd):
```

#### Select the Time Interval

Next, you'll have to select the time interval. The time interval determines how frequently the data points are sampled or aggregated over a given period of time.

- If you're dealing with smaller [epoch lengths](/devs/topic-creators/topic-life-cycle#epoch-length), shorter intervals like minutes or seconds might be necessary to capture rapid changes in the market.
- For longer epoch lengths, you may choose daily, weekly, or monthly intervals.

```bash
Enter the frequency (1min/5min/4hour/1day, default: 1day): 
```

<Callout type="warning">
Using shorter time intervals increases CPU power requirements because the dataset grows significantly. More data points lead to larger memory consumption, longer data processing times, and more complex computations. The CPU has to handle more input/output operations, and models take longer to train due to the higher volume of data needed to capture patterns effectively.
</Callout>

#### Start and End Date

When selecting the start and end dates for your training data, keep in mind that larger time periods result in more data, requiring increased CPU power and memory. Longer timeframes capture more trends but also demand greater computational resources, especially during model training.

```bash
Enter the start date (YYYY-MM-DD, default: 2021-01-01): 
Enter the end date (YYYY-MM-DD, default: 2024-10-20): 
```

#### Selecting Models to Train

Now that we've set up our data source, target variable, and time interval, it's time to select the models to train on. In the prompt, you can either choose to train on all available models or make a custom selection.

```bash
Select the models to train:
1. All models
2. Custom selection
Enter your choice (1/2): 
```

If you opt for Custom selection, you will be prompted to choose from the regression techniques listed earlier, such as ARIMA, LSTM, Random Forest, or XGBoost. 

#### Model Selection

Now that we've set up our data source, target variable, and time interval, it's time to select the models to train on. In the prompt, you can either choose to train on all available models or make a custom selection.

```bash
Select the models to train:
1. All models
2. Custom selection
Enter your choice (1/2): 
```

If you opt for Custom selection, you will be prompted to choose from the regression techniques listed earlier, such as ARIMA, LSTM, Random Forest, or XGBoost. You can select the models that are best suited for your specific problem or dataset.

#### Model Evaluation

After selecting and training the models, the next step is to evaluate them. The MDK provides built-in tools to assess the performance of your model using standard metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Simply run:

```bash
make eval
```

This will generate performance reports, helping you identify the best model to deploy.

## Deployment

Deploying a model requires packaging your trained model from the MDK and integrating it with a worker node repository before exposing the worker as an endpoint.

### Package your Trained Model

Run the following command to package your model for the Allora worker:

```bash
make package-arima
```

<Callout type="warning">
Replace arima with the name of the model you’d like to package (e.g., lstm, xgboost, etc.).
</Callout>

<Callout>
This will:

- Copy the model’s files and dependencies into the `package folder`.
- Run test's for inference and training to validate functionality in a worker
- Generate a configuration file, `config.py`, that contains the active model information.
</Callout>

### Deploy your Worker

#### Expose the Endpoint

Run:

```bash
MODEL=ARIMA make run
cd src && uvicorn main:app --reload --port 8000
```

<Callout type="warning">
Replace ARIMA with the name of the model you’d like to package (e.g., LSTM, XGBOOST, etc.).
</Callout>

This will expose your endpoint, which will be called when a [worker nonce](/devs/topic-creators/topic-life-cycle#nonce) is available. If your endpoint is exposed successfully, you should see the following output on your command line:

```bash
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

You can query your endpoint in the CLI by running:

```bash
curl http://127.0.0.1:8000/inference
```

#### Deploy to the Network

Now that you have a specific endpoint that can be queried for an inference output, you can paste the endpoint into your `config.json` file of your prediction node repository.

##### Configure Your Environment

1. Copy `example.config.json` and name the copy `config.json`.
2. Open `config.json` and **update** the necessary fields inside the `wallet` sub-object and `worker` config with your specific values:

###### `wallet` Sub-object

1. `nodeRpc`: The [RPC URL](/devs/get-started/setup-wallet#rpc-url-and-chain-id) for the corresponding network the node will be deployed on
2. `addressKeyName`: The name you gave your wallet key when [setting up your wallet](/devs/get-started/setup-wallet)
3. `addressRestoreMnemonic`: The mnemonic that was outputted when setting up a new key

{/* <Callout type="info">
`addressKeyName` and `addressRestoreMnemonic` are optional parameters. If you did not previously generate keys, keys will be generated for you when [running the node](/devs/workers/deploy-worker/using-docker#generate-keys-and-export-variables).

If you have existing keys that you wish to use, you will need to provide these variables.
</Callout> */}

###### `worker` Config

1. `topicId`: The specific topic ID you created the worker for. 
2. `InferenceEndpoint`: The endpoint exposed by your worker node to provide inferences to the network.
3. `Token`: The token for the specific topic you are providing inferences for. The token needs to be exposed in the inference server endpoint for retrieval.
  - The `Token` variable is specific to the endpoint you expose in your `main.py` file. It is not related to any topic parameter.

<Callout type="warning">
The `worker` config is an array of sub-objects, each representing a different topic ID. This structure allows you to manage multiple topic IDs, each within its own sub-object.

To deploy a worker that provides inferences for multiple topics, you can duplicate the existing sub-object and add it to the `worker` array. Update the `topicId`, `InferenceEndpoint` and `Token` fields with the appropriate values for each new topic:
```json
"worker": [
      {
        "topicId": 1,
        "inferenceEntrypointName": "apiAdapter",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}",
          "Token": "ETH"
        }
      },
      // worker providing inferences for topic ID 2
      {
        "topicId": 2, 
        "inferenceEntrypointName": "apiAdapter",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}", // the specific endpoint providing inferences
          "Token": "ETH" // The token specified in the endpoint
        }
      }
    ],
```
</Callout>

Then run:

```bash
make node-env
make compose
```

- This will load your config into your environment and spin up your docker node, which will check for open worker nonces and submit inferences to the network.

If your node is working correctly, you should see it actively checking for the active worker nonce:

```bash
offchain_node    | {"level":"debug","topicId":1,"time":1723043600,"message":"Checking for latest open worker nonce on topic"}
```

A **successful** response from your Worker should display:

```bash
{"level":"debug","msg":"Send Worker Data to chain","txHash":<tx-hash>,"time":<timestamp>,"message":"Success"}
```


================================================
FILE: pages/devs/workers/deploy-worker/build-and-deploy-worker-with-alibaba-cloud.mdx
================================================
# Build and Deploy a Worker Node With Alibaba Cloud

This guide provides detailed instructions on how to deploy an Allora Worker Node on Alibaba Cloud infrastructure. 

## Overview

Deploying Allora Worker Nodes on Alibaba Cloud enables you to participate in the Allora Network by leveraging reliable and scalable cloud infrastructure. This guide walks you through the complete setup process, from server configuration to running your first worker node.

## Prerequisites

Before you begin, ensure you have:
- An Alibaba Cloud account
- Access to [Allora Forge](https://forge.allora.network) (you must apply before running a worker)
- An Allora wallet with testnet tokens

## 1. Purchase an Alibaba Cloud Server

For running an Allora Worker Node, we recommend the following configuration:

- **CPU**: 2 Core
- **RAM**: 4 GB
- **Storage**: 40 GB ESSD
- **Bandwidth**: 5 Mbps
- **Operating System**: Ubuntu 24.04

![Alibaba Cloud Server Configuration](/alibaba/picture1.jpg)

## 2. Install Docker

Follow these steps to install Docker on your Alibaba Cloud server:

### Update System Packages

```bash
sudo apt update && sudo apt upgrade -y
```

### Install Golang

```bash
sudo apt install golang-go -y
```

### Install Required Packages

```bash
apt-get install ca-certificates curl gnupg lsb-release -y
```

### Add Docker GPG Key

Using Alibaba Cloud mirror for faster downloads in China regions:

```bash
curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
```

### Add Docker Repository

```bash
add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
```

### Install Docker Components

```bash
apt-get install docker-ce docker-ce-cli containerd.io -y
```

### Start Docker

```bash
systemctl start docker
```

### Enable Docker to Start on Boot

```bash
systemctl enable docker
```

### Verify Docker Installation

```bash
docker version
```

If you see output similar to the image below, Docker has been installed successfully:

![Docker Version Output](/alibaba/picture2.jpg)

## 3. Download and Configure allorad

Allora Network provides convenient precompiled binaries. At the time of writing, the current version is v0.12.1.

### Download allorad

```bash
wget https://github.com/allora-network/allora-chain/releases/download/v0.12.1/allora-chain_0.12.1_linux_amd64
```

For the latest version, check the [allora-chain releases page](https://github.com/allora-network/allora-chain/releases).

### Install allorad

Rename the binary, move it to an executable path, and grant permissions:

```bash
mv allora-chain_0.12.1_linux_amd64 allorad
mv allorad /usr/local/bin
chmod a+x /usr/local/bin/allorad
```

### Verify Installation

```bash
allorad version
```

If it shows `0.12.1`, the installation is complete, and a `.allorad` folder will be created in the root directory.

![allorad Version Output](/alibaba/picture3.jpg)

## 4. Configure the Allora Worker Node

This guide uses the [basic-coin-prediction-node](https://github.com/allora-network/basic-coin-prediction-node) repository as an example.

### Clone the Repository

```bash
git clone https://github.com/allora-network/basic-coin-prediction-node.git
cd basic-coin-prediction-node
```

![Cloning the Repository](/alibaba/picture4.jpg)

### Configure Environment Variables

Rename `.env.example` to `.env`:

```bash
cp .env.example .env
```

This file contains sensitive environment variables. If you're fetching data directly in your Python file (e.g., using ccxt), you may not need to modify this file.

### Configure Worker Settings

Rename `config.example.json` to `config.json`:

```bash
cp config.example.json config.json
```

Edit `config.json` with your wallet information and worker configuration. Here's a complete example:

```json
{
    "wallet": {
        "chainId": "allora-testnet-1",
        "addressKeyName": "<Your Wallet Name>",
        "addressRestoreMnemonic": "<Your Wallet Mnemonic>",
        "alloraHomeDir": "",
        "gasAdjustment": 2,
        "gasPrices": "100",
        "gasPriceUpdateInterval": 5,
        "simulateGasFromStart": true,
        "GasPerByte": 1,
        "BaseGas": 200000,
        "maxFees": 300000000,
        "nodeRpcs": [
            "https://allora-rpc.monallo.ai", 
            "https://allora-rpc.testnet.allora.network/"
        ],
        "nodegRpcs": [
            "allora-grpc.monallo.ai:443",
            "allora-grpc.testnet.allora.network:443",
            "testnet-allora.lavenderfive.com:443"
        ],
        "maxRetries": 8,
        "retryDelay": 3,
        "accountSequenceRetryDelay": 5,
        "launchRoutineDelay": 5,
        "submitTx": true,
        "blockDurationEstimated": 5,
        "windowCorrectionFactor": 0.8,
        "timeoutRPCSecondsQuery": 60,
        "timeoutRPCSecondsTx": 300,
        "timeoutRPCSecondsRegistration": 300,
        "timeoutHTTPConnection": 10
    },
    "worker": [
        {
            "topicId": 62,
            "inferenceEntrypointName": "apiAdapter",
            "loopSeconds": 5,
            "parameters": {
                "InferenceEndpoint": "http://inference:8000/inference/{Token}",
                "Token": "SOL-USDT"
            }
        }
    ]
}
```

**Important Configuration Notes:**
- Replace `<Your Wallet Name>` with your wallet name
- Replace `<Your Wallet Mnemonic>` with your wallet's recovery phrase
- Adjust `topicId` to match the topic you're participating in
- Modify the `Token` parameter to match your target trading pair

### Update Docker Image Version

In `docker-compose.yml`, update the offchain node image version:

Change from:
```yml
image: alloranetwork/allora-offchain-node:v0.6.0
```

To:
```yml
image: alloranetwork/allora-offchain-node:v0.12.0
```

**Note:** It's acceptable if the minor version differs slightly from your allorad version.

### Customize the Inference Model

The `app.py` file is the core of the worker node project. This example provides a basic machine learning model template that you can customize for your predictions.

<details>
<summary>Click to view the example app.py code</summary>

```python
# -*- coding: utf-8 -*-

import time                    # Built-in library, used for periodic loops and waiting (sleep)
import math                    # Built-in library, used for logarithmic calculations
import threading               # Built-in library, uses threads for scheduled tasks
import requests                # HTTP request library, used to call OKX REST API
import pickle                  # Used for model serialization (save/load)
import json                    # Used to handle JSON data format
from flask import Flask, Response, request  # Flask: used to provide a web interface
import numpy as np             # Numerical computation library
import statsmodels.api as sm   # OLS regression model library (for least squares training)

# Global configuration (modifiable):
TOKEN = "SOL-USDT"             # Product code, e.g., "SOL-USDT" (trading pair), can be changed
API_BASE = "https://www.okx.com"  # Base address of OKX API
MODEL_PATH = "ols_model.pkl"   # Local path to save the trained model file
UPDATE_INTERVAL = 300          # Interval for scheduled updates (seconds), here 5 minutes = 300 seconds
DATA_POINTS = 180              # Number of past days of daily data to fetch

app = Flask(__name__)          # Create Flask application instance

def getData():
    """
    Fetch past DATA_POINTS days of 1-day candlestick data (including closing prices).
    Uses OKX `/api/v5/market/candles` endpoint (allows up to 300 entries at a time).
    Returns a list of tuples (timestamp, closing price) sorted in ascending order (UTC).
    """
    # Calculate limit
    limit = DATA_POINTS if DATA_POINTS <= 300 else 300
    # Request parameters
    params = {
        "instId": TOKEN,
        "bar": "1D",
        "limit": str(limit)
    }
    # Send GET request
    resp = requests.get(f"{API_BASE}/api/v5/market/candles", params=params, timeout=10)
    resp.raise_for_status()  # Raise exception on error
    resp_json = resp.json()
    if resp_json.get("code") != "0":
        raise RuntimeError(f"OKX returned error: {resp_json.get('msg')}")
    # data = [ [ts, o, h, l, c, ...], ... ], where ts is the open time in milliseconds
    data = resp_json["data"]
    # Convert to ascending order and extract [ts, closing price]
    result = sorted([(int(item[0]) // 1000, float(item[4])) for item in data], key=lambda x: x[0])
    return result

def compute_log_returns(series):
    """
    Accepts a list of tuples (ts, price) sorted in ascending order by ts.
    Calculates daily log returns ln(p_{t+1} / p_t).
    Returns a numpy array X (features) and a target vector y:
      X = [[price_t, ts_t], …] excluding the last day;
      y = [ln(price_{t+1}/price_t), …] corresponding to each X[i].
    """
    n = len(series)
    if n < 2:
        raise ValueError("Not enough data points to compute log-return")
    prices = np.array([p for (_, p) in series], dtype=float)
    timestamps = np.array([t for (t, _) in series], dtype=float)
    log_returns = np.log(prices[1:] / prices[:-1])
    # Features can include only price, or also timestamp to capture trends
    X = np.column_stack((prices[:-1], timestamps[:-1]))
    y = log_returns
    return X, y

def update_loop():
    """
    Background thread function: every UPDATE_INTERVAL seconds,
    automatically calls update_task().
    """
    while True:
        try:
            update_task()
        except Exception as e:
            # Do not stop thread on error, just print it
            print("update_task error:", e)
        time.sleep(UPDATE_INTERVAL)

def update_task():
    """
    Main update logic:
    1. Fetch past DATA_POINTS daily data;
    2. Compute log-returns;
    3. Fit an OLS least squares model;
    4. Save the model to a local file.
    """
    series = getData()
    X, y = compute_log_returns(series)
    # Add intercept term
    X_with_const = sm.add_constant(X)
    model = sm.OLS(y, X_with_const).fit()
    with open(MODEL_PATH, "wb") as f:
        pickle.dump(model, f)
    # No return after training, model saved successfully
    print("Model trained and saved.")

@app.route("/inference/<string:token>")
def generate_inference(token):
    """
    Route `/inference/<token>`:
    Checks if the token matches the global TOKEN (case-insensitive).
    Then fetches the latest two days' prices using getData,
    uses the saved model to predict the next log-return,
    and computes the predicted price as c_t * exp(pred_return).
    Responds with JSON containing the predicted log-return and predicted price.
    """
    if token.upper() != TOKEN.upper():
        # Token mismatch, return 400
        return Response(json.dumps({"error": "Token not supported"}), status=400, mimetype="application/json")
    # Load the trained model
    try:
        with open(MODEL_PATH, "rb") as f:
            model = pickle.load(f)
    except FileNotFoundError:
        return Response(json.dumps({"error": "Model not trained yet"}), status=500, mimetype="application/json")
    # Get the latest two daily data points
    series = getData()
    if len(series) < 2:
        # Not enough data for prediction
        return Response(json.dumps({"error": "Not enough data for inference"}), status=500, mimetype="application/json")
    # Latest ts, price
    ts_current, price_current = series[-2]  # Second last entry is the "current" parameters
    ts_next, price_next = series[-1]       # Last entry is the "actual next day" price
    # Prepare feature vector
    X_pred = np.array([[price_current, ts_current]])
    X_pred_with_const = sm.add_constant(X_pred, has_constant="add")
    pred_log_return = float(model.predict(X_pred_with_const)[0])

    return Response(str(pred_log_return), status=200, mimetype="application/json")

@app.route("/update")
def http_update():
    """
    Another endpoint `/update`: manually/externally triggers an update task (train model) once.
    Returns "0" for success, "1" for failure.
    """
    try:
        update_task()
        return "0"
    except Exception as e:
        print("Manual update error:", e)
        return "1"

if __name__ == "__main__":
    # When the script is run directly, train the model once first
    update_task()
    # Then start a background thread to continuously update
    thread = threading.Thread(target=update_loop, daemon=True)
    thread.start()
    # Start Flask web service
    app.run(host="0.0.0.0", port=8000)
```

</details>

This example demonstrates:
- Fetching historical price data from OKX API
- Training an OLS regression model on log returns
- Providing predictions via a Flask API endpoint
- Automatic model retraining at regular intervals

You can customize this code to implement your own prediction strategies and data sources.

## 5. Deploy the Worker Node

### Pull Docker Images

```bash
docker compose pull
```

![Pulling Docker Images](/alibaba/picture5.jpg)

### Initialize Configuration

```bash
chmod +x init.config
./init.config
```

### Start the Worker Node

```bash
docker compose up --build
```

Wait for the topic registration to complete, and then the worker will begin submitting predictions to the Allora chain.

![Worker Node Running](/alibaba/picture6.jpg)

## Next Steps

Once your worker node is running successfully:

1. Monitor your node's performance in the [Allora Forge dashboard](https://forge.allora.network)
2. Check your inference submissions on the Allora Network explorer
3. Optimize your model based on performance metrics
4. Consider deploying multiple workers for different topics

## Troubleshooting

### Common Issues

- **Docker permission errors**: Ensure your user is in the docker group: `sudo usermod -aG docker $USER`
- **Network connectivity**: Verify that your Alibaba Cloud security group allows outbound HTTPS traffic
- **RPC connection issues**: Try alternative RPC endpoints from the config.json example above
- **Gas estimation errors**: Adjust `gasAdjustment` and `maxFees` parameters if transactions fail

## Additional Resources

- [Allora Network Documentation](https://docs.allora.network)
- [Allora Forge Platform](https://forge.allora.network)
- [Basic Coin Prediction Node Repository](https://github.com/allora-network/basic-coin-prediction-node)
- [Allora Chain Releases](https://github.com/allora-network/allora-chain/releases)




================================================
FILE: pages/devs/workers/deploy-worker/build-and-deploy-worker-with-node-runners.mdx
================================================
# Build and Deploy a Worker Node With AWS Node Runners

Welcome to the AWS Node Runners documentation! This page provides detailed instructions on how to leverage Node Runners on AWS, including benefits, setup instructions, and useful links.

## Overview

Node Runners on AWS enables you to deploy and manage blockchain nodes efficiently using AWS infrastructure. Whether you're deploying Ethereum nodes or other blockchain networks, Node Runners simplifies the process, offering scalability, reliability, and cost-effectiveness.

For more detailed information and step-by-step guides, please refer to the [AWS Node Runners Documentation](https://aws-samples.github.io/aws-blockchain-node-runners/docs/Blueprints/Ethereum).

### Allora Network's AWS Infrastructure

This diagram illustrates the architecture of the integration between the Allora Network (built on a Cosmos AppChain) and an AWS-based infrastructure for handling inference requests. 

![node-runners](/aws-node-runners.png)

#### Key Components

1. **Allora Network (Cosmos AppChain)**
   - **Public Head Node**: Acts as the entry point for the Allora Network, handling requests and responses.

2. **AWS Account Setup**
   - **Region**: The geographical location within AWS where the resources are deployed.
   - **Virtual Private Cloud (VPC)**: Provides an isolated network environment within the AWS region.
     - **Public Subnet**: A subnet within the VPC that has access to the internet through the VPC Internet Gateway.
     - **VPC Internet Gateway**: Allows communication between the instances in the VPC and the internet.

3. **EC2 Instance (Allora Worker Node)**
   - **Offchain Node**: This component handles network communication, receiving requests from the Allora Network and sending responses back.
   - **Node Function**: Processes requests by interfacing with the private model server. It acts as an intermediary, ensuring the requests are correctly formatted and the responses are appropriately handled.
   - **Model Server**: Hosts the proprietary model. It executes the main inference script (`Main.py`) to generate inferences based on the received requests.

#### Process Flow

1. **Request Flow**:
   - The Allora Network's Public Head Node sends a request for inferences to the EC2 instance within the AWS environment.
   - The request passes through the VPC Internet Gateway and reaches the Offchain node in the public subnet.
   - The Offchain node forwards the request to the Node Function.
   - The Node Function calls `Main.py` on the Model Server to generate the required inferences.

2. **Response Flow**:
   - The Model Server processes the request and returns the inferences to the Node Function.
   - The Node Function sends the inferences back to the Offchain node.
   - The Offchain node communicates the inferences back to the Allora Network via the VPC Internet Gateway.

## AWS Activate

Before proceeding, please note that eligibility for AWS Activate credits and terams are governed by AWS. This documentation may become outdated, so ensure you refer to the [AWS Activate program page](https://aws.amazon.com/startups/credits#hero) for the latest eligibility requirements and instructions.

## AWS Activate Stepwise Process

To receive up to $5,000 in AWS Activate credits, follow these steps:

1. **Fill out our [Typeform](https://vk4z45e3hne.typeform.com/to/TVwcjiL1)**: Provide your details to receive our Activate Provider Organizational ID.
   - Name (required)
   - Contact Information (optional): Email, Telegram, Discord handle, Linkedin
   - Official Company Website (required)

2. **AWS Activate High-Level Instructions**: After obtaining our Organizational ID,
   - Visit [AWS Activate Credit Packages](https://aws.amazon.com/startups/credits#packages).
   - Apply through the Activate Portfolio




================================================
FILE: pages/devs/workers/deploy-worker/using-docker.mdx
================================================
import { Callout } from 'nextra/components'

# Build and Deploy a Worker Node using Docker

This document outlines a setup where the worker node is supported by an inference server. Communication occurs through an endpoint, allowing the worker to request inferences from the server.

To build this setup, please follow these steps:

## Prerequisites

Ensure you have the following installed on your machine:

- Git
- Go (version 1.16 or later)
- Docker

## Clone the `allora-offchain-node` Repository

Download the `allora-offchain-node` git repo:

```bash
git clone https://github.com/allora-network/allora-offchain-node
cd allora-offchain-node
```

## Configure Your Environment

1. Copy `config.example.json` and name the copy `config.json`.
2. Open `config.json` and **update** the necessary fields inside the `wallet` sub-object and `worker` config with your specific values:

### `wallet` Sub-object

1. `nodeRpc`: The [RPC URL](/devs/get-started/setup-wallet#rpc-url-and-chain-id) for the corresponding network the node will be deployed on
2. `addressKeyName`: The name you gave your wallet key when [setting up your wallet](/devs/get-started/setup-wallet)
3. `addressRestoreMnemonic`: The mnemonic that was outputted when setting up a new key

{/* <Callout type="info">
`addressKeyName` and `addressRestoreMnemonic` are optional parameters. If you did not previously generate keys, keys will be generated for you when [running the node](/devs/workers/deploy-worker/using-docker#generate-keys-and-export-variables).

If you have existing keys that you wish to use, you will need to provide these variables.
</Callout> */}

### `worker` Config

1. `topicId`: The specific topic ID you created the worker for. 
2. `InferenceEndpoint`: The endpoint exposed by your worker node to provide inferences to the network.
3. `Token`: The token for the specific topic you are providing inferences for. The token needs to be exposed in the inference server endpoint for retrieval.
  - The `Token` variable is specific to the endpoint you expose in your `main.py` file. It is not related to any topic parameter.

<Callout type="warning">
The `worker` config is an array of sub-objects, each representing a different topic ID. This structure allows you to manage multiple topic IDs, each within its own sub-object.

To deploy a worker that provides inferences for multiple topics, you can duplicate the existing sub-object and add it to the `worker` array. Update the `topicId`, `InferenceEndpoint` and `Token` fields with the appropriate values for each new topic:
```json
"worker": [
      {
        "topicId": 1,
        "inferenceEntrypointName": "api-worker-reputer",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}",
          "Token": "ETH"
        }
      },
      // worker providing inferences for topic ID 2
      {
        "topicId": 2, 
        "inferenceEntrypointName": "api-worker-reputer",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}", // the specific endpoint providing inferences
          "Token": "ETH" // The token specified in the endpoint
        }
      }
    ],
```
</Callout>

### Reputer Config

The `config.example.json` file that was copied and edited in the previous steps also contains a JSON object for configuring and deploying a [reputer](/devs/reputers/reputers). To ignore the reputer and only deploy a worker, delete the reputer sub-object from the `config.json` file.

## Create the Inference Server

### Prepare the API Gateway

Ensure you have an API gateway or server that can accept API requests to call your model. 

<Callout>
The model in `allora-offchain-node` is barebones and outputs a random integer. Follow the model built in [`basic-coin-prediction-node`](https://github.com/allora-network/basic-coin-prediction-node) as an example for a full model that uses linear regression to provide an inference.

A full breakdown of the components needed to build the model is available [here](/devs/workers/walkthroughs/walkthrough-price-prediction-worker).
</Callout>

### Server Responsibilities

- Accept API requests from `main.go`.
- Respond with the corresponding inference obtained from the model.

### Inference Relay

Below is a sample structure of what your `main.go`, `main.py` and Dockerfile will look like.

#### `main.go`

`allora-offchain-node` comes preconfigured with a `main.go` file inside the [`adapter/api-worker-reputer` folder](https://github.com/allora-network/allora-offchain-node/blob/dev/adapter/api-worker-reputer/main.go).

The `main.go` file fetches the responses outputted from the Inference Endpoint based on the `InferenceEndpoint` and `Token` provided in the section above.

#### `main.py`

`allora-offchain-node` comes preconfigured with a Flask application that uses a `main.py` file to expose the Inference Endpoint. 

The Flask application serves the request from `main.go`, which is routed to the `get_inference`  function using the required argument (`Token`). Before proceeding, ensure that all necessary packages are listed in the `requirements.txt` file.

```python
from flask import Flask
from model import get_inference  # Importing the hypothetical model

app = Flask(__name__)

@app.route('inference/<argument>')
def get_inference(param):
    random_float = str(random.uniform(0.0, 100.0))
    return random_float

if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

<Callout type="warning">
  The model in `allora-offchain-node` is barebones and outputs a random integer. Follow the model built in [`basic-coin-prediction-node`](https://github.com/allora-network/basic-coin-prediction-node) as an example for a full model that uses linear regression to provide an inference.

  A full breakdown of the components needed to build the model is available [here](/devs/workers/walkthroughs/walkthrough-price-prediction-worker).
</Callout>



#### `Dockerfile`

A sample Dockerfile has been created in `allora-offchain-node` that can be used to deploy your model on port 8000.

```dockerfile
FROM python:3.9-slim

RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["python", "main.py"]
```

## Running the Node

Now that the node is configured, let's deploy and register it to the network. To run the node, follow these steps:

### Export Variables

Execute the following command from the root directory:

```sh
chmod +x init.config
./init.config 
```

This command will automatically export the necessary variables from the account created. These variables are used by the offchain node and are bundled with your provided `config.json`, then passed to the node as environment variables.

<Callout>
If you need to **make changes** to your `config.json` file after you ran the `init.config` command, rerun: 

```sh
chmod +x init.config
./init.config 
```

before proceeding.

</Callout>

### Request from Faucet

Copy your Allora address and request some tokens from the [Allora Testnet Faucet](https://faucet.testnet.allora.network/) to register your worker in the next step successfully.

### Deploy the Node

```
docker compose up --build
```

Both the offchain node and the source services will be started. They will communicate through endpoints attached to the internal DNS.

If your node is working correctly, you should see it actively checking for the active worker nonce:

```bash
offchain_node    | {"level":"debug","topicId":1,"time":1723043600,"message":"Checking for latest open worker nonce on topic"}
```

A **successful** response from your Worker should display:

```bash
{"level":"debug","msg":"Send Worker Data to chain","txHash":<tx-hash>,"time":<timestamp>,"message":"Success"}
```

Congratulations! You've successfully deployed and registered your node on Allora.

### Testing

You can test your local inference server by performing a `GET` request on `http://localhost:8000/inference/<token>`.

```bash
curl http://localhost:8000/inference/<token>
```



================================================
FILE: pages/devs/workers/walkthroughs/_meta.json
================================================
{
    "walkthrough-hugging-face-worker": "Hugging Face Worker",
    "walkthrough-price-prediction-worker": "Price Prediction Worker"
}


================================================
FILE: pages/devs/workers/walkthroughs/walkthrough-hugging-face-worker.mdx
================================================
import { Callout } from 'nextra/components'

# Walkthrough: Deploying a Hugging Face Model as a Worker Node on the Allora Network

> This guide provides a step-by-step process to deploy a Hugging Face model as a Worker Node within the Allora Network. By following these instructions, you will be able to integrate and run models from Hugging Face, contributing to the Allora decentralized machine intelligence ecosystem.


## Prerequisites

Before you start, ensure you have the following:

- A Docker environment with `docker compose` installed.
- Basic knowledge of machine learning and the [Hugging Face](https://huggingface.co/) ecosystem.
- Familiarity with Allora Network documentation on [building and deploying a worker node using Docker](/devs/workers/deploy-worker/using-docker).

## Overview

During this walkthrough, we will build a worker node from an existing Hugging Face model to deploy and participate on the Allora Network. We will use this model to predict the price of BTC in 24h. 

You can find all the files in [this Git repository](https://github.com/allora-network/allora-huggingface-walkthrough).

In this example, we will use the Chronos model: [amazon/chronos-t5-tiny](https://huggingface.co/amazon/chronos-t5-tiny). Chronos is a family of pretrained time series forecasting models based on language model architectures. In essence:
  - A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. 
  - Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. 

Chronos models have been trained on a large corpus of publicly available time series data, as well as synthetic data generated using Gaussian processes.

<Callout type="info">
For simplicity, we will use Zero-shot forecasting, which refers to the ability of models to generate forecasts from unseen datasets.
</Callout>

Our worker will provide inferences on the BTC 24h Prediction, which is Topic `4` on Allora Testnet.

> Note:
> To deploy on the Allora Network, you will need to [pick the topic ID](/devs/get-started/existing-topics) you wish to generate inference for, or [create a new topic](/devs/topic-creators/how-to-create-topic).

We will use [Coingecko](https://www.coingecko.com/en/api) to fetch the data. You will need to [create an API key](https://www.coingecko.com/en/api/pricing).

## Clone the repo

Clone the [basic-coin-prediction-node](https://github.com/allora-network/basic-coin-prediction-node) repository. It will serve as the base sample for your quick setup.

```bash
git clone https://github.com/allora-network/basic-coin-prediction-node
cd basic-coin-prediction-node
```

## Configure Your Environment

1. Copy `config.example.json` and name the copy `config.json`.
2. Open `config.json` and **update** the necessary fields inside the `wallet` sub-object and `worker` config with your specific values:

### `wallet` Sub-object

1. `nodeRpc`: The [RPC URL](/devs/get-started/setup-wallet#rpc-url-and-chain-id) for the corresponding network the node will be deployed on
2. `addressKeyName`: The name you gave your wallet key when [setting up your wallet](/devs/get-started/setup-wallet)
3. `addressRestoreMnemonic`: The mnemonic that was outputted when setting up a new key

### `worker` Config

1. `topicId`: The specific topic ID you created the worker for. 
2. `InferenceEndpoint`: The endpoint exposed by your worker node to provide inferences to the network.
3. `Token`: The token for the specific topic you are providing inferences for. The token needs to be exposed in the inference server endpoint for retrieval.
  - The `Token` variable is specific to the endpoint you expose in your `main.py` file. It is not related to any topic parameter.

<Callout type="warning">
The `worker` config is an array of sub-objects, each representing a different topic ID. This structure allows you to manage multiple topic IDs, each within its own sub-object.

To deploy a worker that provides inferences for multiple topics, you can duplicate the existing sub-object and add it to the `worker` array. Update the `topicId`, `InferenceEndpoint` and `Token` fields with the appropriate values for each new topic:
```json
"worker": [
      {
        "topicId": 1,
        "inferenceEntrypointName": "api-worker-reputer",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}",
          "Token": "ETH"
        }
      },
      // worker providing inferences for topic ID 2
      {
        "topicId": 2, 
        "inferenceEntrypointName": "api-worker-reputer",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}", // the specific endpoint providing inferences
          "Token": "ETH" // The token specified in the endpoint
        }
      }
    ],
```
</Callout>

## Creating the inference server

We will create a very simple Flask application to serve inferences from the Hugging Face model.

Here is an example of our newly created `app.py`:

```python
from flask import Flask, Response
import requests
import json
import pandas as pd
import torch
from chronos import ChronosPipeline

# create our Flask app
app = Flask(__name__)

# define the Hugging Face model we will use
model_name = "amazon/chronos-t5-tiny"

def get_coingecko_url(token):
    base_url = "https://api.coingecko.com/api/v3/coins/"
    token_map = {
        'ETH': 'ethereum',
        'SOL': 'solana',
        'BTC': 'bitcoin',
        'BNB': 'binancecoin',
        'ARB': 'arbitrum'
    }
    
    token = token.upper()
    if token in token_map:
        url = f"{base_url}{token_map[token]}/market_chart?vs_currency=usd&days=30&interval=daily"
        return url
    else:
        raise ValueError("Unsupported token")

# define our endpoint
@app.route("/inference/<string:token>")
def get_inference(token):
    """Generate inference for given token."""
    try:
        # use a pipeline as a high-level helper
        pipeline = ChronosPipeline.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
        )
    except Exception as e:
        return Response(json.dumps({"pipeline error": str(e)}), status=500, mimetype='application/json')

    try:
        # get the data from Coingecko
        url = get_coingecko_url(token)
    except ValueError as e:
        return Response(json.dumps({"error": str(e)}), status=400, mimetype='application/json')

    headers = {
        "accept": "application/json",
        "x-cg-demo-api-key": "<Your Coingecko API key>" # replace with your API key
    }

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        df = pd.DataFrame(data["prices"])
        df.columns = ["date", "price"]
        df["date"] = pd.to_datetime(df["date"], unit='ms')
        df = df[:-1] # removing today's price
        print(df.tail(5))
    else:
        return Response(json.dumps({"Failed to retrieve data from the API": str(response.text)}), 
                        status=response.status_code, 
                        mimetype='application/json')

    # define the context and the prediction length
    context = torch.tensor(df["price"])
    prediction_length = 1

    try:
        forecast = pipeline.predict(context, prediction_length)  # shape [num_series, num_samples, prediction_length]
        print(forecast[0].mean().item()) # taking the mean of the forecasted prediction
        return Response(str(forecast[0].mean().item()), status=200)
    except Exception as e:
        return Response(json.dumps({"error": str(e)}), status=500, mimetype='application/json')

# run our Flask app
if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000, debug=True)
```

## Modifying requirements.txt

Update the `requirements.txt` to include the necessary packages for the inference server:

```
flask[async]
gunicorn[gthread]
transformers[torch]
pandas
git+https://github.com/amazon-science/chronos-forecasting.git
```

## Deployment

Now that the node is configured, let's deploy and register it to the network. To run the node, follow these steps:

### Export Variables

Execute the following command from the root directory:

```sh
chmod +x init.config
./init.config 
```

This command will automatically export the necessary variables from the account created. These variables are used by the offchain node and are bundled with your provided `config.json`, then passed to the node as environment variables.

<Callout>
If you need to **make changes** to your `config.json` file after you ran the `init.config` command, rerun:

```sh
chmod +x init.config
./init.config 
```

before proceeding.

</Callout>

### Request from Faucet

Copy your Allora address and request some tokens from the [Allora Testnet Faucet](https://faucet.testnet.allora.network/) to register your worker in the next step successfully.

### Deploy the Node

```
docker compose up --build
```

Both the offchain node and the source services will be started. They will communicate through endpoints attached to the internal DNS.

If your node is working correctly, you should see it actively checking for the active worker nonce:

```bash
offchain_node    | {"level":"debug","topicId":1,"time":1723043600,"message":"Checking for latest open worker nonce on topic"}
```

A **successful** response from your Worker should display:

```bash
{"level":"debug","msg":"Send Worker Data to chain","txHash":<tx-hash>,"time":<timestamp>,"message":"Success"}
```

Congratulations! You've successfully deployed and registered your node on Allora.

### Testing

You can test your local inference server by performing a `GET` request on `http://localhost:8000/inference/<token>`.

```bash
curl http://localhost:8000/inference/<token>
```




================================================
FILE: pages/devs/workers/walkthroughs/walkthrough-price-prediction-worker.mdx
================================================
import { Callout } from 'nextra/components'

# Walkthrough: Build and Deploy Price Prediction Worker Node

> How to build a node that predicts the future price of Ether

## Prerequisites 

1. Make sure you have checked the documentation on how to [build and deploy a worker node using Docker](/devs/workers/deploy-worker/using-docker).
2. Clone the [basic-coin-prediction-node](https://github.com/allora-network/basic-coin-prediction-node) repository. It will serve as the base sample for your quick setup.

```bash
git clone https://github.com/allora-network/basic-coin-prediction-node
cd basic-coin-prediction-node
```

## Explainer Video

Please see the video below to get a full deep-dive on how to deploy a price-prediction worker:

<iframe 
  width="560" 
  height="315" 
  src="https://www.youtube.com/embed/CAGU_zFz708?si=oBxhk6M5lNKV0Vl6" 
  title="How to build a Unique Model" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
  allowfullscreen>
</iframe>

## Configure Your Environment

### `.env` File Configuration

When setting up your environment, please follow the guidelines below for configuring your `.env` file:

- **`TOKEN`**: Specifies the cryptocurrency token to use. Must be one of the following:
  - `'ETH'` (Ethereum)
  - `'SOL'` (Solana)
  - `'BTC'` (Bitcoin)
  - `'BNB'` (Binance Coin)
  - `'ARB'` (Arbitrum)
  
  > **Note**: If you are using Binance as the data provider, any token can be used. However, if you are using Coingecko, you should add its `coin_id` in the [token map](https://github.com/allora-network/basic-coin-prediction-node/blob/70cf49d0a2317769d883ae882c146efbb915f5c0/updater.py#L107). Find more information [here](https://docs.coingecko.com/reference/simple-price) and the list [here](https://docs.google.com/spreadsheets/d/1wTTuxXt8n9q7C4NDXqQpI3wpKu1_5bGVmP9Xz0XGSyU/edit?gid=0#gid=0).

- **`TRAINING_DAYS`**: Represents the number of days of historical data to use for training. Must be an integer greater than or equal to 1.

- **`TIMEFRAME`**: Defines the timeframe of the data used in the format like `10min` (minutes), `1h` (hours), `1d` (days), etc.
  
  - For Coingecko, the data granularity (candle's body) is automatic. To avoid downsampling when using Coingecko:
    - Use a **`TIMEFRAME`** of `>= 30min` if **`TRAINING_DAYS`** is `<= 2`.
    - Use a **`TIMEFRAME`** of `>= 4h` if **`TRAINING_DAYS`** is `<= 30`.
    - Use a **`TIMEFRAME`** of `>= 4d` if **`TRAINING_DAYS`** is `>= 31`.

- **`MODEL`**: Specifies the machine learning model to use. Must be one of the following:
  - `'LinearRegression'`
  - `'SVR'` (Support Vector Regression)
  - `'KernelRidge'`
  - `'BayesianRidge'`
  
  > You can easily add support for other models by adding them to the configuration [here](https://github.com/allora-network/basic-coin-prediction-node/blob/main/model.py#L133).

- **`REGION`**: Defines the region for the Binance API. Must be `'EU'` or `'US'`.

- **`DATA_PROVIDER`**: Specifies the data provider to use. Must be either `'Binance'` or `'Coingecko'`.
  
  - Feel free to add support for other data providers to personalize your model!

- **`CG_API_KEY`**: Your Coingecko API key, required if you've set **`DATA_PROVIDER`** to `'coingecko'`.

#### Sample Configuration (.env file)

Below is an example configuration for your `.env` file:

```bash
TOKEN=ETH
TRAINING_DAYS=30
TIMEFRAME=4h
MODEL=SVR
REGION=US
DATA_PROVIDER=binance
CG_API_KEY=
```

### `config.json` Configuration

1. Copy `config.example.json` and name the copy `config.json`.
2. Open `config.json` and **update** the necessary fields inside the `wallet` sub-object and `worker` config with your specific values:

#### `wallet` Sub-object

1. `nodeRpc`: The [RPC URL](/devs/get-started/setup-wallet#rpc-url-and-chain-id) for the corresponding network the node will be deployed on
2. `addressKeyName`: The name you gave your wallet key when [setting up your wallet](/devs/get-started/setup-wallet)
3. `addressRestoreMnemonic`: The mnemonic that was outputted when setting up a new key

{/* <Callout type="info">
`addressKeyName` and `addressRestoreMnemonic` are optional parameters. If you did not previously generate keys, keys will be generated for you when [running the node](/devs/workers/deploy-worker/using-docker#generate-keys-and-export-variables).

If you have existing keys that you wish to use, you will need to provide these variables.
</Callout> */}

#### `worker` Config

1. `topicId`: The specific topic ID you created the worker for. 
2. `InferenceEndpoint`: The endpoint exposed by your worker node to provide inferences to the network.
3. `Token`: The token for the specific topic you are providing inferences for. The token needs to be exposed in the inference server endpoint for retrieval.
  - The `Token` variable is specific to the endpoint you expose in your `main.py` file. It is not related to any topic parameter.

<Callout type="warning">
The `worker` config is an array of sub-objects, each representing a different topic ID. This structure allows you to manage multiple topic IDs, each within its own sub-object.

To deploy a worker that provides inferences for multiple topics, you can duplicate the existing sub-object and add it to the `worker` array. Update the `topicId`, `InferenceEndpoint` and `Token` fields with the appropriate values for each new topic:
```json
"worker": [
      {
        "topicId": 1,
        "inferenceEntrypointName": "api-worker-reputer",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}",
          "Token": "ETH"
        }
      },
      // worker providing inferences for topic ID 2
      {
        "topicId": 2, 
        "inferenceEntrypointName": "api-worker-reputer",
        "loopSeconds": 5,
        "parameters": {
          "InferenceEndpoint": "http://localhost:8000/inference/{Token}", // the specific endpoint providing inferences
          "Token": "ETH" // The token specified in the endpoint
        }
      }
    ],
```
</Callout>

## Building a Custom Model

`basic-coin-prediction-node` comes preconfigured with a model that uses regression to predict the price of Ethereum, and contribute an inference to topic 1 on Allora. Learn more about how this model is built from the ground up and how you can customize your model to give a unique inference to the network in the [next section](/devs/workers/walkthroughs/walkthrough-price-prediction-worker/modelpy).

## Deployment

Now that the node is configured, let's deploy and register it to the network. To run the node, follow these steps:

### Export Variables

Execute the following command from the root directory:

```sh
chmod +x init.config
./init.config 
```

This command will automatically export the necessary variables from the account created. These variables are used by the offchain node and are bundled with your provided `config.json`, then passed to the node as environment variables.

<Callout>
If you need to **make changes** to your `config.json` file after you ran the `init.config` command, rerun:

```sh
chmod +x init.config
./init.config 
```

before proceeding.

</Callout>

### Request from Faucet

Copy your Allora address and request some tokens from the [Allora Testnet Faucet](https://faucet.testnet.allora.network/) to register your worker in the next step successfully.

### Deploy the Node

```
docker compose up --build
```

Both the offchain node and the source services will be started. They will communicate through endpoints attached to the internal DNS.

If your node is working correctly, you should see it actively checking for the active worker nonce:

```bash
offchain_node    | {"level":"debug","topicId":1,"time":1723043600,"message":"Checking for latest open worker nonce on topic"}
```

A **successful** response from your Worker should display:

```bash
{"level":"debug","msg":"Send Worker Data to chain","txHash":<tx-hash>,"time":<timestamp>,"message":"Success"}
```

Congratulations! You've successfully deployed and registered your node on Allora.

### Testing

You can test your local inference server by performing a `GET` request on `http://localhost:8000/inference/<token>`.

```bash
curl http://localhost:8000/inference/<token>
```




================================================
FILE: pages/devs/workers/walkthroughs/walkthrough-price-prediction-worker/_meta.json
================================================
{
    "modelpy": "Model.py"
}


================================================
FILE: pages/devs/workers/walkthroughs/walkthrough-price-prediction-worker/modelpy.mdx
================================================
import { Callout } from 'nextra/components'

# Model.py

## Introduction

The [`model.py` file](https://github.com/allora-network/basic-coin-prediction-node/blob/main/model.py) in `basic-coin-prediction-node` consists of several key components:

- **Imports and Configuration:** Sets up necessary libraries and configuration variables.
- **Paths Configuration:** Generates paths for storing data dynamically based on coin symbols.
- **Downloading Data:** Downloads historical price data for the specified symbols, intervals, years, and months.
- **Formatting Data:** Reads, formats, and saves the downloaded data as CSV files.
- **Training the Model:** Trains a linear regression model on the formatted price data and saves the trained model.

While the import and path configuration processes are straightforward, downloading and formatting the data, as well as training the model, require specific steps. 

This documentation will guide you through creating models for different coins, making it easy to extend the script for general-purpose use.

## Downloading the Data

The [`download_data`](https://github.com/allora-network/basic-coin-prediction-node/blob/5d70e9feee7d1e7725c7602427b6856e7ffbe479/model.py#L16) function is designed to automate the process of downloading historical market data from Binance, a popular cryptocurrency exchange. 
This function focuses on fetching data for a specified set of symbols (in this case, the trading pair `"ETHUSDT"`) across various time intervals and storing them in a defined directory. 

### How to Use for Downloading Data of Any Coin

#### Update the Symbols List

Replace `["ETHUSDT"]` with the desired trading pair(s), e.g., `["BTCUSDT", "LTCUSDT"]`.

#### Adjust Time Intervals

Modify the intervals list if you need different time intervals. Binance supports various intervals like `["1m", "5m", "1h", "1d", "1w", "1M"]`.

#### Extend Date Ranges

Update the years and months lists to match the historical range you need.

#### Define the Download Path 

Ensure `binance_data_path` is set to the directory where you want the data to be saved.

Here’s a quick **example** of how to adjust the script for downloading data for multiple trading pairs:

```python
def download_data():
    cm_or_um = "um"
    symbols = ["BTCUSDT", "LTCUSDT"]  # Updated symbols
    intervals = ["1d"]
    years = ["2020", "2021", "2022", "2023", "2024"]
    months = ["01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"]
    download_path = binance_data_path
    download_binance_monthly_data(
        cm_or_um, symbols, intervals, years, months, download_path
    )
    print(f"Downloaded monthly data to {download_path}.")
    current_datetime = datetime.now()
    current_year = current_datetime.year
    current_month = current_datetime.month
    download_binance_daily_data(
        cm_or_um, symbols, intervals, current_year, current_month, download_path
    )
    print(f"Downloaded daily data to {download_path}.")
```

### Formatting the Data

The [`format_data`](https://github.com/allora-network/basic-coin-prediction-node/blob/5d70e9feee7d1e7725c7602427b6856e7ffbe479/model.py#L36) function processes raw data files downloaded from Binance, transforming them into a consistent format for analysis. Here are the key steps:

1. **File Handling:**
   - Lists and sorts all files in the `binance_data_path` directory.
   - Exits if no files are found.

2. **Initialize DataFrame:**
   - An empty DataFrame `price_df` is created to store the combined data.

3. **Process Each File:**
   - Filters for `.zip` files and reads the contained CSV file.
   - Retains the first 11 columns and renames them to: `["start_time", "open", "high", "low", "close", "volume", "end_time", "volume_usd", "n_trades", "taker_volume", "taker_volume_usd"]`.
   - Sets the DataFrame index to the `end_time` column, converted to a timestamp.

4. **Concatenate Data:**
   - Combines data from each file into the `price_df` DataFrame.

5. **Sort and Save:**
   - Sorts the final DataFrame by date and saves it to `training_price_data_path`.

### Column Descriptions

- **start_time**: The start of the trading period.
- **open**: Opening price.
- **high**: Highest price during the period.
- **low**: Lowest price during the period.
- **close**: Closing price.
- **volume**: Trading volume.
- **end_time**: End of the trading period.
- **volume_usd**: Trading volume in USD.
- **n_trades**: Number of trades.
- **taker_volume**: Taker buy volume.
- **taker_volume_usd**: Taker buy volume in USD.

This function consolidates and formats the historical price data, making it ready for analysis or machine learning tasks.

### Training the Model

The [`train_model`](https://github.com/allora-network/basic-coin-prediction-node/blob/5d70e9feee7d1e7725c7602427b6856e7ffbe479/model.py#L75) function trains a **linear regression model** using historical price data and saves the trained model to a file. Here's a breakdown of the process:

1. **Load the Data:**
   - Reads the price data from a CSV file specified by `training_price_data_path`.

2. **Prepare the DataFrame:**
   - Converts the `date` column to a timestamp and stores it as a numerical value.
   - Computes the average price using the `open`, `close`, `high`, and `low` columns.

3. **Reshape Data for Regression:**
   - Extracts the `date` column as the feature (`x`) and the computed average price as the target (`y`).
   - Reshapes these arrays to the format expected by `scikit-learn`.

4. **Split the Data:**
   - Splits the data into a training set and a test set using an 80/20 split. However, the test set is not used further in this function.

5. **Train the Model:**
   - Initializes and trains a `LinearRegression` model using the training data.

6. **Save the Model:**
   - Creates the directory for the model file if it doesn't exist.
   - Saves the trained model to a file specified by `model_file_path` using `pickle`.

7. **Print Confirmation:**
   - Prints a message indicating that the trained model has been saved.

### Modifying the Function for Different Models

<Callout>
#### Understanding the Target Variable in Regression Models

The **target variable (y)** in regression models is a critical component that determines the type of analysis and predictions that can be performed. In this context, the target variable represents continuous data—in this case, the average of financial metrics such as the open, close, high, and low prices over time. The y-axis points on the regression graph correspond to these continuous values, which can take on any numerical value within a defined range.
</Callout>

To change the model used for training, replace the `LinearRegression` model with another machine learning algorithm. Here is an example:

#### Using Polynomial Regression

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
import os
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split

def train_model():
    # Load the eth price data
    price_data = pd.read_csv(training_price_data_path)
    df = pd.DataFrame()

    # Convert 'date' to a numerical value (timestamp) we can use for regression
    df["date"] = pd.to_datetime(price_data["date"])
    df["date"] = df["date"].map(pd.Timestamp.timestamp)

    # Calculate the mean price as the target variable
    df["price"] = price_data[["open", "close", "high", "low"]].mean(axis=1)

    # Reshape the data to the shape expected by sklearn
    x = df["date"].values.reshape(-1, 1)
    y = df["price"].values.reshape(-1, 1)

    # Split the data into training set and test set
    x_train, _, y_train, _ = train_test_split(x, y, test_size=0.2, random_state=0)

    # Create a pipeline that first transforms the input data to polynomial features and then fits a linear model
    model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
    model.fit(x_train, y_train)

    # Create the model's parent directory if it doesn't exist
    os.makedirs(os.path.dirname(model_file_path), exist_ok=True)

    # Save the trained model to a file
    with open(model_file_path, "wb") as f:
        pickle.dump(model, f)

    print(f"Trained polynomial regression model saved to {model_file_path}")
```

